{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# coding=utf-8\n\n\"\"\"\nGoal: Implement a trading simulator to simulate and compare trading strategies.\nAuthors: Thibaut Théate and Damien Ernst\nInstitution: University of Liège\n\"\"\"\n\n###############################################################################\n################################### Imports ###################################\n###############################################################################\n\nimport os\nimport sys\nimport importlib\nimport pickle\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nfrom tabulate import tabulate\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\nimport copy\nimport numpy as np\n\nshiftRange = [0]\nstretchRange = [1]\nfilterRange = [5]\nnoiseRange = [0]\n\n\nimport pandas as pd\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom matplotlib import pyplot as plt\n\n\nimport math\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom abc import ABC, abstractmethod\n\n\n###############################################################################\n########################### Class tradingStrategy #############################\n###############################################################################\n\nclass tradingStrategy(ABC):\n    \"\"\"\n    GOAL: Define the abstract class representing a classical trading strategy.\n\n    VARIABLES: /\n\n    METHODS: - chooseAction: Make a decision regarding the next trading\n                             position (long=1 and short=0).\n             - training: Train the trading strategy on a known trading\n                         environment (called training set) in order to\n                         tune the trading strategy parameters.\n             - testing: Test the trading strategy on another unknown trading\n                        environment (called testing set) in order to evaluate\n                        the trading strategy performance.\n    \"\"\"\n\n    @abstractmethod\n    def chooseAction(self, state):\n        \"\"\"\n        GOAL: Make a decision regarding the next trading position\n              (long=1 and short=0).\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - action: Trading position decision (long=1 and short=0).\n        \"\"\"\n\n        pass\n\n\n    @abstractmethod\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        \"\"\"\n        GOAL: Train the trading strategy on a known trading environment\n              (called training set) in order to tune the trading strategy\n              parameters.\n\n        INPUTS: - trainingEnv: Known trading environment (training set).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase.\n                - verbose: Enable the printing of a training feedback.\n                - rendering: Enable the trading environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - trainingEnv: Trading environment associated with the best\n                                trading strategy parameters backtested.\n        \"\"\"\n\n        pass\n\n\n    @abstractmethod\n    def testing(self, testingEnv, trainingEnv, rendering=False, showPerformance=False):\n        \"\"\"\n        GOAL: Test the trading strategy on another unknown trading\n              environment (called testing set) in order to evaluate\n              the trading strategy performance.\n\n        INPUTS: - testingEnv: Unknown trading environment (testing set).\n                - trainingEnv: Known trading environment (training set).\n                - rendering: Enable the trading environment rendering.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - testingEnv: Trading environment backtested.\n        \"\"\"\n\n        pass\n\n\n\n###############################################################################\n############################## Class BuyAndHold ###############################\n###############################################################################\n\nclass BuyAndHold(tradingStrategy):\n    \"\"\"\n    GOAL: Implement a simple \"Buy and Hold\" trading strategy.\n\n    VARIABLES: /\n\n    METHODS: - chooseAction: Always make the long trading position decision\n                             (return 1).\n             - training: Train the trading strategy on a known trading\n                         environment (called training set) in order to\n                         tune the trading strategy parameters. However,\n                         there is no training required for a simple\n                         Buy and Hold strategy because the strategy does not\n                         involve any tunable parameter.\n             - testing: Test the trading strategy on another unknown trading\n                        environment (called testing set) in order to evaluate\n                        the trading strategy performance.\n    \"\"\"\n\n    def chooseAction(self, state):\n        \"\"\"\n        GOAL: Always make the long trading position decision.\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - action: Trading position decision (always long -> 1).\n        \"\"\"\n\n        # Trading position decision -> always long\n        return 1\n\n\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        \"\"\"\n        GOAL: Train the trading strategy on a known trading environment\n              (called training set) in order to tune the trading strategy\n              parameters. However, there is no training required for a\n              simple Buy and Hold strategy because the strategy does not\n              involve any tunable parameter.\n\n        INPUTS: - trainingEnv: Known trading environment (training set).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase. None for\n                                      the Buy and Hold strategy.\n                - verbose: Enable the printing of a training feedback. None\n                           for the Buy and Hold strategy.\n                - rendering: Enable the trading environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n                                None for the Buy and Hold strategy.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - trainingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Execution of the trading strategy on the trading environment\n        trainingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n\n        # If required, print a feedback about the training\n        if verbose:\n            print(\"No training is required as the simple Buy and Hold trading strategy does not involve any tunable parameters.\")\n\n        # If required, render the trading environment backtested\n        if rendering:\n            trainingEnv.render()\n\n        # If required, plot the training results\n        if plotTraining:\n            print(\"No training results are available as the simple Buy and Hold trading strategy does not involve any tunable parameters.\")\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('B&H')\n\n        # Return the trading environment backtested (training set)\n        return trainingEnv\n\n\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        \"\"\"\n        GOAL: Test the trading strategy on another unknown trading\n              environment (called testing set) in order to evaluate\n              the trading strategy performance.\n\n        INPUTS: - testingEnv: Unknown trading environment (testing set).\n                - trainingEnv: Known trading environment (training set).\n                - rendering: Enable the trading environment rendering.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - testingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Execution of the trading strategy on the trading environment\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n\n        # If required, render the trading environment backtested\n        if rendering:\n            testingEnv.render()\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('B&H')\n\n        # Return the trading environment backtested (testing set)\n        return testingEnv\n\n\n\n###############################################################################\n############################## Class SellAndHold ###############################\n###############################################################################\n\nclass SellAndHold(tradingStrategy):\n    \"\"\"\n    GOAL: Implement a simple \"Sell and Hold\" trading strategy.\n\n    VARIABLES: /\n\n    METHODS: - chooseAction: Always make the short trading position decision\n                             (return 0).\n             - training: Train the trading strategy on a known trading\n                         environment (called training set) in order to\n                         tune the trading strategy parameters. However,\n                         there is no training required for a simple\n                         Sell and Hold strategy because the strategy does not\n                         involve any tunable parameter.\n             - testing: Test the trading strategy on another unknown trading\n                        environment (called testing set) in order to evaluate\n                        the trading strategy performance.\n    \"\"\"\n\n    def chooseAction(self, state):\n        \"\"\"\n        GOAL: Always make the short trading position decision.\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - action: Trading position decision (always short -> 0).\n        \"\"\"\n\n        # Trading position decision -> always short\n        return 0\n\n\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        \"\"\"\n        GOAL: Train the trading strategy on a known trading environment\n              (called training set) in order to tune the trading strategy\n              parameters. However, there is no training required for a\n              simple Sell and Hold strategy because the strategy does not\n              involve any tunable parameter.\n\n        INPUTS: - trainingEnv: Known trading environment (training set).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase. None for\n                                      the Sell and Hold strategy.\n                - verbose: Enable the printing of a training feedback. None\n                           for the Sell and Hold strategy.\n                - rendering: Enable the trading environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n                                None for the Sell and Hold strategy.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - trainingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Execution of the trading strategy on the trading environment\n        trainingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n\n        # If required, print a feedback about the training\n        if verbose:\n            print(\"No training is required as the simple Sell and Hold trading strategy does not involve any tunable parameters.\")\n\n        # If required, render the trading environment backtested\n        if rendering:\n            trainingEnv.render()\n\n        # If required, plot the training results\n        if plotTraining:\n            print(\"No training results are available as the simple Sell and Hold trading strategy does not involve any tunable parameters.\")\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('S&H')\n\n        # Return the trading environment backtested (training set)\n        return trainingEnv\n\n\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        \"\"\"\n        GOAL: Test the trading strategy on another unknown trading\n              environment (called testing set) in order to evaluate\n              the trading strategy performance.\n\n        INPUTS: - testingEnv: Unknown trading environment (testing set).\n                - trainingEnv: Known trading environment (training set).\n                - rendering: Enable the trading environment rendering.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - testingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Execution of the trading strategy on the trading environment\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n\n        # If required, render the trading environment backtested\n        if rendering:\n            testingEnv.render()\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('S&H')\n\n        # Return the trading environment backtested (testing set)\n        return testingEnv\n\n\n\n###############################################################################\n############################ Class MovingAveragesTF ###########################\n###############################################################################\n\nclass MovingAveragesTF(tradingStrategy):\n    \"\"\"\n    GOAL: Implement a Trend Following trading strategy based on moving averages.\n\n    VARIABLES: - parameters: Trading strategy parameters, which are the windows\n                             durations of the moving averages.\n\n    METHODS: - __init__: Object constructor initializing the strategy parameters.\n             - setParameters: Set new values for the parameters of the trading\n                              strategy, which are the two windows durations.\n             - processState: Process the trading environment state to obtain\n                             the required format.\n             - chooseAction: Make a decision regarding the next trading\n                             position (long=1 and short=0).\n             - training: Train the trading strategy on a known trading\n                         environment (called training set) in order to\n                         tune the trading strategy parameters.\n             - testing: Test the trading strategy on another unknown trading\n                        environment (called testing set) in order to evaluate\n                        the trading strategy performance.\n    \"\"\"\n\n    def __init__(self, parameters=[5, 10]):\n        \"\"\"\n        GOAL: Object constructor initializing the strategy parameters.\n\n        INPUTS: - parameters: Trading strategy parameters, which are the windows\n                              durations of the moving averages.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.parameters = parameters\n\n\n    def setParameters(self, parameters):\n        \"\"\"\n        GOAL: Set new values for the parameters of the trading strategy,\n              which are the two windows durations.\n\n        INPUTS: - parameters: List of new parameters to set.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.parameters = parameters\n\n\n    def processState(self, state):\n        \"\"\"\n        GOAL: Process the trading environment state to obtain the required format.\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - state: State of the trading environment in the required format.\n        \"\"\"\n\n        return state[0]\n\n\n    def chooseAction(self, state):\n        \"\"\"\n        GOAL: Make a decision regarding the next trading position\n              (long=1 and short=0) based on the moving averages.\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - action: Trading position decision (long=1 and short=0).\n        \"\"\"\n\n        # Processing of the trading environment state\n        state = self.processState(state)\n\n        # Computation of the two moving averages\n        shortAverage = np.mean(state[-self.parameters[0]:])\n        longAverage = np.mean(state[-self.parameters[1]:])\n\n        # Comparison of the two moving averages\n        if(shortAverage >= longAverage):\n            # Long position\n            return 1\n        else:\n            # Short position\n            return 0\n\n\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        \"\"\"\n        GOAL: Train the trading strategy on a known trading environment\n              (called training set) in order to tune the trading strategy\n              parameters, by simulating many combinations of parameters.\n\n        INPUTS: - trainingEnv: Known trading environment (training set).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase simulations.\n                - verbose: Enable the printing of a training feedback.\n                - rendering: Enable the trading environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - trainingEnv: Trading environment associated with the best\n                                trading strategy parameters backtested.\n        \"\"\"\n\n        # Compute the dimension of the parameter search space\n        bounds = trainingParameters[0]\n        step = trainingParameters[1]\n        dimension = math.ceil((bounds[1] - bounds[0])/step)\n\n        # Initialize some variables required for the simulations\n        trainingEnv.reset()\n        results = np.zeros((dimension, dimension))\n        bestShort = 0\n        bestLong = 0\n        bestPerformance = -100\n        i = 0\n        j = 0\n        count = 1\n\n        # If required, compute the number of simulation iterations\n        if verbose:\n            iterations = dimension - 1\n            length = 0\n            while iterations > 0:\n                length += iterations\n                iterations -= 1\n\n        # Loop through all the parameters combinations included in the parameter search space\n        for shorter in range(bounds[0], bounds[1], step):\n            for longer in range(bounds[0], bounds[1], step):\n\n                # Obvious restriction on the parameters\n                if(shorter < longer):\n\n                    # If required, print the progression of the training\n                    if(verbose):\n                        print(\"\".join([\"Training progression: \", str(count), \"/\", str(length)]), end='\\r', flush=True)\n\n                    # Apply the trading strategy with the current combination of parameters\n                    self.setParameters([shorter, longer])\n                    done = 0\n                    while done == 0:\n                        _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n\n                    # Retrieve the performance associated with this simulation (Sharpe Ratio)\n                    performanceAnalysis = PerformanceEstimator(trainingEnv.data)\n                    performance = performanceAnalysis.computeSharpeRatio()\n                    results[i][j] = performance\n\n                    # Track the best performance and parameters\n                    if(performance > bestPerformance):\n                        bestShort = shorter\n                        bestLong = longer\n                        bestPerformance = performance\n\n                    # Reset of the trading environment\n                    trainingEnv.reset()\n                    count += 1\n\n                j += 1\n            i += 1\n            j = 0\n\n        # Execute once again the strategy associated with the best parameters simulated\n        trainingEnv.reset()\n        self.setParameters([bestShort, bestLong])\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n\n        # If required, render the trading environment backtested\n        if rendering:\n            trainingEnv.render()\n\n        # If required, plot the training results\n        if plotTraining:\n            self.plotTraining(results, bounds, step, trainingEnv.marketSymbol)\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('MATF')\n\n        # Return the trading environment backtested (training set)\n        return trainingEnv\n\n\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        \"\"\"\n        GOAL: Test the trading strategy on another unknown trading\n              environment (called testing set) in order to evaluate\n              the trading strategy performance.\n\n        INPUTS: - testingEnv: Unknown trading environment (testing set).\n                - trainingEnv: Known trading environment (training set).\n                - rendering: Enable the trading environment rendering.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - testingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Execution of the trading strategy on the trading environment\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n\n        # If required, render the trading environment backtested\n        if rendering:\n            testingEnv.render()\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data,)\n            analyser.displayPerformance('MATF')\n\n        # Return the trading environment backtested (testing set)\n        return testingEnv\n\n\n    def plotTraining(self, results, bounds, step, marketSymbol):\n        \"\"\"\n        GOAL: Plot both 2D and 3D graphs illustrating the results of the entire\n              training phase set of simulations, depicting the performance\n              associated with each combination of parameters.\n\n        INPUTS: - results: Results of the entire set of simulations (training).\n                - bounds: Bounds of the parameter search space.\n                - step: Step of the parameter search space.\n                - marketSymbol: Stock market trading symbol.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Generate x, y vectors and a meshgrid for the surface plot\n        x = range(bounds[0], bounds[1], step)\n        y = range(bounds[0], bounds[1], step)\n        xx, yy = np.meshgrid(x, y, sparse=True)\n\n        # Initialization of the 3D figure\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111, projection='3d')\n        ax.set_xlabel('Long Window Duration')\n        ax.set_ylabel('Short Window Duration')\n        ax.set_zlabel('Sharpe Ratio')\n\n        # Generate and show the 3D surface plot\n        ax.plot_surface(xx, yy, results, cmap=plt.cm.get_cmap('jet'))\n        ax.view_init(45, 45)\n        plt.savefig(''.join(['Figures/', str(marketSymbol), '_MATFOptimization3D', '.png']))\n        #plt.show()\n\n        # Plot the same information as a 2D graph\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111,\n                             ylabel='Short Window Duration',\n                             xlabel='Long Window Duration')\n        graph = ax.imshow(results,\n                          cmap='jet',\n                          extent=(bounds[0], bounds[1], bounds[1], bounds[0]))\n        plt.colorbar(graph)\n        plt.gca().invert_yaxis()\n        plt.savefig(''.join(['Figures/', str(marketSymbol), '_MATFOptimization2D', '.png']))\n        #plt.show()\n\n\n\n###############################################################################\n############################ Class MovingAveragesMR ###########################\n###############################################################################\n\nclass MovingAveragesMR(tradingStrategy):\n    \"\"\"\n    GOAL: Implement a Mean Reversion trading strategy based on moving averages.\n\n    VARIABLES: - parameters: Trading strategy parameters, which are the windows\n                             durations of the moving averages.\n\n    METHODS: - __init__: Object constructor initializing the strategy parameters.\n             - setParameters: Set new values for the parameters of the trading\n                              strategy, which are the two windows durations.\n             - processState: Process the trading environment state to obtain\n                             the required format.\n             - chooseAction: Make a decision regarding the next trading\n                             position (long=1 and short=0).\n             - training: Train the trading strategy on a known trading\n                         environment (called training set) in order to\n                         tune the trading strategy parameters.\n             - testing: Test the trading strategy on another unknown trading\n                        environment (called testing set) in order to evaluate\n                        the trading strategy performance.\n    \"\"\"\n\n    def __init__(self, parameters=[5, 10]):\n        \"\"\"\n        GOAL: Object constructor initializing the strategy parameters.\n\n        INPUTS: - parameters: Trading strategy parameters, which are the windows\n                              durations of the moving averages.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.parameters = parameters\n\n\n    def setParameters(self, parameters):\n        \"\"\"\n        GOAL: Set new values for the parameters of the trading strategy,\n              which are the two windows durations.\n\n        INPUTS: - parameters: List of new parameters to set.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.parameters = parameters\n\n\n    def processState(self, state):\n        \"\"\"\n        GOAL: Process the trading environment state to obtain the required format.\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - state: State of the trading environment in the required format.\n        \"\"\"\n\n        return state[0]\n\n\n    def chooseAction(self, state):\n        \"\"\"\n        GOAL: Make a decision regarding the next trading position\n              (long=1 and short=0) based on the moving averages.\n\n        INPUTS: - state: State of the trading environment.\n\n        OUTPUTS: - action: Trading position decision (long=1 and short=0).\n        \"\"\"\n\n        # Processing of the trading environment state\n        state = self.processState(state)\n\n        # Computation of the two moving averages\n        shortAverage = np.mean(state[-self.parameters[0]:])\n        longAverage = np.mean(state[-self.parameters[1]:])\n\n        # Comparison of the two moving averages\n        if(shortAverage <= longAverage):\n            # Long position\n            return 1\n        else:\n            # Short position\n            return 0\n\n\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        \"\"\"\n        GOAL: Train the trading strategy on a known trading environment\n              (called training set) in order to tune the trading strategy\n              parameters, by simulating many combinations of parameters.\n\n        INPUTS: - trainingEnv: Known trading environment (training set).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase simulations.\n                - verbose: Enable the printing of a training feedback.\n                - rendering: Enable the trading environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - trainingEnv: Trading environment associated with the best\n                                trading strategy parameters backtested.\n        \"\"\"\n\n        # Compute the dimension of the parameter search space\n        bounds = trainingParameters[0]\n        step = trainingParameters[1]\n        dimension = math.ceil((bounds[1] - bounds[0])/step)\n\n        # Initialize some variables required for the simulations\n        trainingEnv.reset()\n        results = np.zeros((dimension, dimension))\n        bestShort = 0\n        bestLong = 0\n        bestPerformance = -100\n        i = 0\n        j = 0\n        count = 1\n\n        # If required, compute the number of simulation iterations\n        if verbose:\n            iterations = dimension - 1\n            length = 0\n            while iterations > 0:\n                length += iterations\n                iterations -= 1\n\n        # Loop through all the parameters combinations included in the parameter search space\n        for shorter in range(bounds[0], bounds[1], step):\n            for longer in range(bounds[0], bounds[1], step):\n\n                # Obvious restriction on the parameters\n                if(shorter < longer):\n\n                    # If required, print the progression of the training\n                    if(verbose):\n                        print(\"\".join([\"Training progression: \", str(count), \"/\", str(length)]), end='\\r', flush=True)\n\n                    # Apply the trading strategy with the current combination of parameters\n                    self.setParameters([shorter, longer])\n                    done = 0\n                    while done == 0:\n                        _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n\n                    # Retrieve the performance associated with this simulation (Sharpe Ratio)\n                    performanceAnalysis = PerformanceEstimator(trainingEnv.data)\n                    performance = performanceAnalysis.computeSharpeRatio()\n                    results[i][j] = performance\n\n                    # Track the best performance and parameters\n                    if(performance > bestPerformance):\n                        bestShort = shorter\n                        bestLong = longer\n                        bestPerformance = performance\n\n                    # Reset of the trading environment\n                    trainingEnv.reset()\n                    count += 1\n\n                j += 1\n            i += 1\n            j = 0\n\n        # Execute once again the strategy associated with the best parameters simulated\n        trainingEnv.reset()\n        self.setParameters([bestShort, bestLong])\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n\n        # If required, render the trading environment backtested\n        if rendering:\n            trainingEnv.render()\n\n        # If required, plot the training results\n        if plotTraining:\n            self.plotTraining(results, bounds, step, trainingEnv.marketSymbol)\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('MAMR')\n\n        # Return the trading environment backtested (training set)\n        return trainingEnv\n\n\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        \"\"\"\n        GOAL: Test the trading strategy on another unknown trading\n              environment (called testing set) in order to evaluate\n              the trading strategy performance.\n\n        INPUTS: - testingEnv: Unknown trading environment (testing set).\n                - trainingEnv: Known trading environment (training set).\n                - rendering: Enable the trading environment rendering.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - testingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Execution of the trading strategy on the trading environment\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n\n        # If required, render the trading environment backtested\n        if rendering:\n            testingEnv.render()\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('MAMR')\n\n        # Return the trading environment backtested (testing set)\n        return testingEnv\n\n\n    def plotTraining(self, results, bounds, step, marketSymbol):\n        \"\"\"\n        GOAL: Plot both 2D and 3D graphs illustrating the results of the entire\n              training phase set of simulations, depicting the performance\n              associated with each combination of parameters.\n\n        INPUTS: - results: Results of the entire set of simulations (training).\n                - bounds: Bounds of the parameter search space.\n                - step: Step of the parameter search space.\n                - marketSymbol: Stock market trading symbol.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Generate x, y vectors and a meshgrid for the surface plot\n        x = range(bounds[0], bounds[1], step)\n        y = range(bounds[0], bounds[1], step)\n        xx, yy = np.meshgrid(x, y, sparse=True)\n\n        # Initialization of the 3D figure\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111, projection='3d')\n        ax.set_xlabel('Long Window Duration')\n        ax.set_ylabel('Short Window Duration')\n        ax.set_zlabel('Sharpe Ratio')\n\n        # Generate and show the surface 3D surface plot\n        ax.plot_surface(xx, yy, results, cmap=plt.cm.get_cmap('jet'))\n        ax.view_init(45, 45)\n        plt.savefig(''.join(['Figures/', str(marketSymbol), '_MAMROptimization3D', '.png']))\n        #plt.show()\n\n        # Plot the same information as a 2D graph\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111,\n                             ylabel='Short Window Duration',\n                             xlabel='Long Window Duration')\n        graph = ax.imshow(results,\n                          cmap='jet',\n                          extent=(bounds[0], bounds[1], bounds[1], bounds[0]))\n        plt.colorbar(graph)\n        plt.gca().invert_yaxis()\n        plt.savefig(''.join(['Figures/', str(marketSymbol), '_MAMROptimization2D', '.png']))\n        #plt.show()\n\n\n###############################################################################\n############################## TimeSeriesAnalyser #############################\n###############################################################################\n\nclass TimeSeriesAnalyser:\n    \"\"\"\n    GOAL: Analysing time series (stationarity, cyclicity, etc.).\n\n    VARIABLES:  - timeSeries: Time series data to analyse.\n\n    METHODS:    - __init__: Initialization of the time series analyser.\n                - timeSeriesDecomposition: Decomposition of the time series into\n                                           its different components.\n                - stationarityAnalysis: Assess the stationarity of the time series.\n                - cyclicityAnalysis: Assess the cyclical component of the time series.\n    \"\"\"\n\n    def __init__(self, timeSeries):\n        \"\"\"\n        GOAL: Initialization of the time series analyser, by retrieving the time series\n              data to analyse.\n\n        INPUTS: - timeSeries: Time series data to analyse.\n\n        OUTPUTS: /\n        \"\"\"\n        self.timeSeries = timeSeries\n\n\n    def plotTimeSeries(self):\n        \"\"\"\n        GOAL: Draw a relevant plot of the time series to analyse.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Generation of the plot\n        pd.plotting.register_matplotlib_converters()\n        plt.figure(figsize=(10, 4))\n        plt.plot(self.timeSeries.index, self.timeSeries.values, color='blue')\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Price\")\n        plt.show()\n\n\n    def timeSeriesDecomposition(self, model='multiplicative'):\n        \"\"\"\n        GOAL: Decompose the time series into its different components\n              (trend, seasonality, residual).\n\n        INPUTS: - model: Either additive or multiplicative decomposition.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Decomposition of the time series and plotting of each component\n        # period=5 because there are 5 trading days in a week, and the decomposition looks for weekly seasonality\n        # period=21 should be used for monthly seasonality and period=252 for yearly seasonality\n        decomposition = seasonal_decompose(self.timeSeries, model=model, period=5, extrapolate_trend='freq')\n        plt.rcParams.update({'figure.figsize': (16,9)})\n        decomposition.plot()\n        plt.show()\n\n\n    def stationarityAnalysis(self):\n        \"\"\"\n        GOAL: Assess whether or not the time series is stationary.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Augmented Dickey-Fuller test\n        print(\"Stationarity analysis: Augmented Dickey-Fuller test (ADF):\")\n        results = adfuller(self.timeSeries, autolag='AIC')\n        print(\"ADF statistic: \" + str(results[0]))\n        print(\"p-value: \" + str(results[1]))\n        print('Critial values (the time series is not stationary with X% condifidence):')\n        for key, value in results[4].items():\n            print(str(key) + ': ' + str(value))\n        if results[1] < 0.05:\n            print(\"The ADF test affirms that the time series is stationary.\")\n        else:\n            print(\"The ADF test could not affirm whether or not the time series is stationary...\")\n\n\n    def cyclicityAnalysis(self):\n        \"\"\"\n        GOAL: Assess whether or not the time series presents a significant\n              seasonality component.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Generation of an Autoacorrelation function plot\n        plt.rcParams.update({'figure.figsize': (16,9)})\n        pd.plotting.autocorrelation_plot(self.timeSeries)\n        plt.show()\n\n        # Generation of both the autocorrelation and the partial autocorrelation plots\n        _, axes = plt.subplots(2, figsize=(16, 9))\n        plot_acf(self.timeSeries, lags=21, ax=axes[0])\n        plot_pacf(self.timeSeries, lags=21, ax=axes[1])\n        plt.show()\n\n        # Generation of several lag plots\n        _, axes = plt.subplots(1, 10, figsize=(17, 9), sharex=True, sharey=True)\n        for i, ax in enumerate(axes.flatten()[:10]):\n            pd.plotting.lag_plot(self.timeSeries, lag=i+1, ax=ax)\n            ax.set_title('Lag ' + str(i+1))\n        plt.show()\n\n\n\nclass DataAugmentation:\n    \"\"\"\n    GOAL: Implementing some data augmentation techniques for stock time series.\n\n    VARIABLES: /\n\n    METHODS:    - __init__: Initialization of some class variables.\n                - shiftTimeSeries: Generate a new trading environment by simply\n                                   shifting up or down the volume time series.\n                - stretching: Generate a new trading environment by stretching\n                              or contracting the original price time series.\n                - noiseAddition: Generate a new trading environment by adding\n                                 some noise to the original time series.\n                - lowPassFilter: Generate a new trading environment by filtering\n                                 (low-pass) the original time series.\n                - generate: Generate a set of new trading environments based on the\n                            data augmentation techniques implemented.\n    \"\"\"\n\n    def shiftTimeSeries(self, tradingEnv, shiftMagnitude=0):\n        \"\"\"\n        GOAL: Generate a new trading environment by simply shifting up or down\n              the volume time series.\n\n        INPUTS: - tradingEnv: Original trading environment to augment.\n                - shiftMagnitude: Magnitude of the shift.\n\n        OUTPUTS: - newTradingEnv: New trading environment generated.\n        \"\"\"\n\n        # Creation of the new trading environment\n        newTradingEnv = copy.deepcopy(tradingEnv)\n\n        # Constraint on the shift magnitude\n        if shiftMagnitude < 0:\n            minValue = np.min(tradingEnv.data['Volume'])\n            shiftMagnitude = max(-minValue, shiftMagnitude)\n\n        # Shifting of the volume time series\n        newTradingEnv.data['Volume'] += shiftMagnitude\n\n        # Return the new trading environment generated\n        return newTradingEnv\n\n\n    def streching(self, tradingEnv, factor=1):\n        \"\"\"\n        GOAL: Generate a new trading environment by stretching\n              or contracting the original price time series, by\n              multiplying the returns by a certain factor.\n\n        INPUTS: - tradingEnv: Original trading environment to augment.\n                - factor: Stretching/contraction factor.\n\n        OUTPUTS: - newTradingEnv: New trading environment generated.\n        \"\"\"\n\n        # Creation of the new trading environment\n        newTradingEnv = copy.deepcopy(tradingEnv)\n\n        # Application of the stretching/contraction operation\n        returns = newTradingEnv.data['Close'].pct_change() * factor\n        for i in range(1, len(newTradingEnv.data.index)):\n            newTradingEnv.data['Close'][i] = newTradingEnv.data['Close'][i-1] * (1 + returns[i])\n            newTradingEnv.data['Low'][i] = newTradingEnv.data['Close'][i] * tradingEnv.data['Low'][i]/tradingEnv.data['Close'][i]\n            newTradingEnv.data['High'][i] = newTradingEnv.data['Close'][i] * tradingEnv.data['High'][i]/tradingEnv.data['Close'][i]\n            newTradingEnv.data['Open'][i] = newTradingEnv.data['Close'][i-1]\n\n        # Return the new trading environment generated\n        return newTradingEnv\n\n\n    def noiseAddition(self, tradingEnv, stdev=1):\n        \"\"\"\n        GOAL: Generate a new trading environment by adding some gaussian\n              random noise to the original time series.\n\n        INPUTS: - tradingEnv: Original trading environment to augment.\n                - stdev: Standard deviation of the generated white noise.\n\n        OUTPUTS: - newTradingEnv: New trading environment generated.\n        \"\"\"\n\n        # Creation of a new trading environment\n        newTradingEnv = copy.deepcopy(tradingEnv)\n\n        # Generation of the new noisy time series\n        for i in range(1, len(newTradingEnv.data.index)):\n            # Generation of artificial gaussian random noises\n            price = newTradingEnv.data['Close'][i]\n            volume = newTradingEnv.data['Volume'][i]\n            priceNoise = np.random.normal(0, stdev*(price/100))\n            volumeNoise = np.random.normal(0, stdev*(volume/100))\n\n            # Addition of the artificial noise generated\n            newTradingEnv.data['Close'][i] *= (1 + priceNoise/100)\n            newTradingEnv.data['Low'][i] *= (1 + priceNoise/100)\n            newTradingEnv.data['High'][i] *= (1 + priceNoise/100)\n            newTradingEnv.data['Volume'][i] *= (1 + volumeNoise/100)\n            newTradingEnv.data['Open'][i] = newTradingEnv.data['Close'][i-1]\n\n        # Return the new trading environment generated\n        return newTradingEnv\n\n\n    def lowPassFilter(self, tradingEnv, order=5):\n        \"\"\"\n        GOAL: Generate a new trading environment by filtering\n              (low-pass filter) the original time series.\n\n        INPUTS: - tradingEnv: Original trading environment to augment.\n                - order: Order of the filtering operation.\n\n        OUTPUTS: - newTradingEnv: New trading environment generated.\n        \"\"\"\n\n        # Creation of a new trading environment\n        newTradingEnv = copy.deepcopy(tradingEnv)\n\n        # Application of a filtering (low-pass) operation\n        newTradingEnv.data['Close'] = newTradingEnv.data['Close'].rolling(window=order).mean()\n        newTradingEnv.data['Low'] = newTradingEnv.data['Low'].rolling(window=order).mean()\n        newTradingEnv.data['High'] = newTradingEnv.data['High'].rolling(window=order).mean()\n        newTradingEnv.data['Volume'] = newTradingEnv.data['Volume'].rolling(window=order).mean()\n        for i in range(order):\n            newTradingEnv.data['Close'][i] = tradingEnv.data['Close'][i]\n            newTradingEnv.data['Low'][i] = tradingEnv.data['Low'][i]\n            newTradingEnv.data['High'][i] = tradingEnv.data['High'][i]\n            newTradingEnv.data['Volume'][i] = tradingEnv.data['Volume'][i]\n        newTradingEnv.data['Open'] = newTradingEnv.data['Close'].shift(1)\n        newTradingEnv.data['Open'][0] = tradingEnv.data['Open'][0]\n\n        # Return the new trading environment generated\n        return newTradingEnv\n\n\n    def generate(self, tradingEnv):\n        \"\"\"\n        Generate a set of new trading environments based on the data\n        augmentation techniques implemented.\n\n        :param: - tradingEnv: Original trading environment to augment.\n\n        :return: - tradingEnvList: List of trading environments generated\n                                   by data augmentation techniques.\n        \"\"\"\n\n        # Application of the data augmentation techniques to generate the new trading environments\n        tradingEnvList = []\n        for shift in shiftRange:\n            tradingEnvShifted = self.shiftTimeSeries(tradingEnv, shift)\n            for stretch in stretchRange:\n                tradingEnvStretched = self.streching(tradingEnvShifted, stretch)\n                for order in filterRange:\n                    tradingEnvFiltered = self.lowPassFilter(tradingEnvStretched, order)\n                    for noise in noiseRange:\n                        tradingEnvList.append(self.noiseAddition(tradingEnvFiltered, noise))\n        return tradingEnvList\n\n\n# Variables defining the default trading horizon\nstartingDate = '2012-1-1'\nendingDate = '2020-1-1'\nsplitingDate = '2018-1-1'\n\n# Variables defining the default observation and state spaces\nstateLength = 30\nobservationSpace = 1 + (stateLength-1)*4\nactionSpace = 2\n\n# Variables setting up the default transaction costs\npercentageCosts = [0, 0.1, 0.2]\ntransactionCosts = percentageCosts[1]/100\n\n# Variables specifying the default capital at the disposal of the trader\nmoney = 100000\n\n# Variables specifying the default general training parameters\nbounds = [1, 30]\nstep = 1\nnumberOfEpisodes = 50\n\n# Dictionary listing the fictive stocks supported\nfictives = {\n    'Linear Upward' : 'LINEARUP',\n    'Linear Downward' : 'LINEARDOWN',\n    'Sinusoidal' : 'SINUSOIDAL',\n    'Triangle' : 'TRIANGLE',\n}\n\n # Dictionary listing the 30 stocks considered as testbench\nstocks = {\n    'Dow Jones' : 'DIA',\n    'S&P 500' : 'SPY',\n    'NASDAQ 100' : 'QQQ',\n    'FTSE 100' : 'EZU',\n    'Nikkei 225' : 'EWJ',\n    'Google' : 'GOOGL',\n    'Apple' : 'AAPL',\n    'Facebook' : 'FB',\n    'Amazon' : 'AMZN',\n    'Microsoft' : 'MSFT',\n    'Twitter' : 'TWTR',\n    'Nokia' : 'NOK',\n    'Philips' : 'PHIA.AS',\n    'Siemens' : 'SIE.DE',\n    'Baidu' : 'BIDU',\n    'Alibaba' : 'BABA',\n    'Tencent' : '0700.HK',\n    'Sony' : '6758.T',\n    'JPMorgan Chase' : 'JPM',\n    'HSBC' : 'HSBC',\n    'CCB' : '0939.HK',\n    'ExxonMobil' : 'XOM',\n    'Shell' : 'RDSA.AS',\n    'PetroChina' : 'PTR',\n    'Tesla' : 'TSLA',\n    'Volkswagen' : 'VOW3.DE',\n    'Toyota' : '7203.T',\n    'Coca Cola' : 'KO',\n    'AB InBev' : 'ABI.BR',\n    'Kirin' : '2503.T'\n}\n\n# Dictionary listing the 5 trading indices considered as testbench\nindices = {\n    'Dow Jones' : 'DIA',\n    'S&P 500' : 'SPY',\n    'NASDAQ 100' : 'QQQ',\n    'FTSE 100' : 'EZU',\n    'Nikkei 225' : 'EWJ'\n}\n\n# Dictionary listing the 25 company stocks considered as testbench\ncompanies = {\n    'Google' : 'GOOGL',\n    'Apple' : 'AAPL',\n    'Facebook' : 'FB',\n    'Amazon' : 'AMZN',\n    'Microsoft' : 'MSFT',\n    'Twitter' : 'TWTR',\n    'Nokia' : 'NOK',\n    'Philips' : 'PHIA.AS',\n    'Siemens' : 'SIE.DE',\n    'Baidu' : 'BIDU',\n    'Alibaba' : 'BABA',\n    'Tencent' : '0700.HK',\n    'Sony' : '6758.T',\n    'JPMorgan Chase' : 'JPM',\n    'HSBC' : 'HSBC',\n    'CCB' : '0939.HK',\n    'ExxonMobil' : 'XOM',\n    'Shell' : 'RDSA.AS',\n    'PetroChina' : 'PTR',\n    'Tesla' : 'TSLA',\n    'Volkswagen' : 'VOW3.DE',\n    'Toyota' : '7203.T',\n    'Coca Cola' : 'KO',\n    'AB InBev' : 'ABI.BR',\n    'Kirin' : '2503.T'\n}\n\n# Dictionary listing the classical trading strategies supported\nstrategies = {\n    'Buy and Hold' : 'BuyAndHold',\n    'Sell and Hold' : 'SellAndHold',\n    'Trend Following Moving Averages' : 'MovingAveragesTF',\n    'Mean Reversion Moving Averages' : 'MovingAveragesMR'\n}\n\n# Dictionary listing the AI trading strategies supported\nstrategiesAI = {\n    'TDQN' : 'TDQN'\n}\n\n\nimport os\nimport gym\nimport math\nimport numpy as np\n\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import signal\n\nimport pandas as pd\nimport pandas_datareader as pdr\nimport requests\n\nfrom io import StringIO\n\nclass AlphaVantage:\n    \"\"\"\n    GOAL: Downloading stock market data from the Alpha Vantage API. See the\n          AlphaVantage documentation for more information.\n\n    VARIABLES:  - link: Link to the Alpha Vantage website.\n                - apikey: Key required to access the Alpha Vantage API.\n                - datatype: 'csv' or 'json' data format.\n                - outputsize: 'full' or 'compact' (only 100 time steps).\n                - data: Pandas dataframe containing the stock market data.\n\n    METHODS:    - __init__: Object constructor initializing some variables.\n                - getDailyData: Retrieve daily stock market data.\n                - getIntradayData: Retrieve intraday stock market data.\n                - processDataframe: Process the dataframe to homogenize the format.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        GOAL: Object constructor initializing the class variables.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.link = 'https://www.alphavantage.co/query'\n        self.apikey = 'APIKEY'\n        self.datatype = 'csv'\n        self.outputsize = 'full'\n        self.data = pd.DataFrame()\n\n\n    def getDailyData(self, marketSymbol, startingDate, endingDate):\n        \"\"\"\n        GOAL: Downloading daily stock market data from the Alpha Vantage API.\n\n        INPUTS:     - marketSymbol: Stock market symbol.\n                    - startingDate: Beginning of the trading horizon.\n                    - endingDate: Ending of the trading horizon.\n\n        OUTPUTS:    - data: Pandas dataframe containing the stock market data.\n        \"\"\"\n\n        # Send an HTTP request to the Alpha Vantage API\n        payload = {'function': 'TIME_SERIES_DAILY_ADJUSTED', 'symbol': marketSymbol,\n                   'outputsize': self.outputsize, 'datatype': self.datatype,\n                   'apikey': self.apikey}\n        response = requests.get(self.link, params=payload)\n\n        # Process the CSV file retrieved\n        csvText = StringIO(response.text)\n        data = pd.read_csv(csvText, index_col='Date')\n\n        # Process the dataframe to homogenize the output format\n        self.data = self.processDataframe(data)\n        if(startingDate != 0 and endingDate != 0):\n            self.data = self.data.loc[startingDate:endingDate]\n\n        return self.data\n\n\n    def getIntradayData(self, marketSymbol, startingDate, endingDate, timePeriod=60):\n        \"\"\"\n        GOAL: Downloading intraday stock market data from the Alpha Vantage API.\n\n        INPUTS:     - marketSymbol: Stock market symbol.\n                    - startingDate: Beginning of the trading horizon.\n                    - endingDate: Ending of the trading horizon.\n                    - timePeriod: Time step of the stock market data (in seconds).\n\n        OUTPUTS:    - data: Pandas dataframe containing the stock market data.\n        \"\"\"\n\n        # Round the timePeriod value to the closest accepted value\n        possiblePeriods = [1, 5, 15, 30, 60]\n        timePeriod = min(possiblePeriods, key=lambda x:abs(x-timePeriod))\n\n        # Send a HTTP request to the AlphaVantage API\n        payload = {'function': 'TIME_SERIES_INTRADAY', 'symbol': marketSymbol,\n                   'outputsize': self.outputsize, 'datatype': self.datatype,\n                   'apikey': self.apikey, 'interval': str(timePeriod)+'min'}\n        response = requests.get(self.link, params=payload)\n\n        # Process the CSV file retrieved\n        csvText = StringIO(response.text)\n        data = pd.read_csv(csvText, index_col='Date')\n\n        # Process the dataframe to homogenize the output format\n        self.data = self.processDataframe(data)\n        if(startingDate != 0 and endingDate != 0):\n            self.data = self.data.loc[startingDate:endingDate]\n\n        return self.data\n\n\n    def processDataframe(self, dataframe):\n        \"\"\"\n        GOAL: Process a downloaded dataframe to homogenize the output format.\n\n        INPUTS:     - dataframe: Pandas dataframe to be processed.\n\n        OUTPUTS:    - dataframe: Processed Pandas dataframe.\n        \"\"\"\n\n        # Reverse the order of the dataframe (chronological order)\n        dataframe = dataframe[::-1]\n\n        # Remove useless columns\n        dataframe['close'] = dataframe['adjusted_close']\n        del dataframe['adjusted_close']\n        del dataframe['dividend_amount']\n        del dataframe['split_coefficient']\n\n        # Adapt the dataframe index and column names\n        dataframe.index.names = ['Date']\n        dataframe = dataframe.rename(index=str, columns={\"open\": \"Open\",\n                                                         \"high\": \"High\",\n                                                         \"low\": \"Low\",\n                                                         \"close\": \"Close\",\n                                                         \"volume\": \"Volume\"})\n        # Adjust the format of the index values\n        dataframe.index = dataframe.index.map(pd.Date)\n\n        return dataframe\n\n\n\n###############################################################################\n########################### Class YahooFinance ################################\n###############################################################################\n\nclass YahooFinance:\n    \"\"\"\n    GOAL: Downloading stock market data from the Yahoo Finance API. See the\n          pandas.datareader documentation for more information.\n\n    VARIABLES:  - data: Pandas dataframe containing the stock market data.\n\n    METHODS:    - __init__: Object constructor initializing some variables.\n                - getDailyData: Retrieve daily stock market data.\n                - processDataframe: Process a dataframe to homogenize the\n                                    output format.\n    \"\"\"\n\n\n    def __init__(self):\n        \"\"\"\n        GOAL: Object constructor initializing the class variables.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.data = pd.DataFrame()\n\n\n    def getDailyData(self, marketSymbol, startingDate, endingDate):\n        \"\"\"\n        GOAL: Downloding daily stock market data from the Yahoo Finance API.\n\n        INPUTS:     - marketSymbol: Stock market symbol.\n                    - startingDate: Beginning of the trading horizon.\n                    - endingDate: Ending of the trading horizon.\n\n        OUTPUTS:    - data: Pandas dataframe containing the stock market data.\n        \"\"\"\n\n        data = pdr.data.DataReader(marketSymbol, 'yahoo', startingDate, endingDate)\n        self.data = self.processDataframe(data)\n        return self.data\n\n\n    def processDataframe(self, dataframe):\n        \"\"\"\n        GOAL: Process a downloaded dataframe to homogenize the output format.\n\n        INPUTS:     - dataframe: Pandas dataframe to be processed.\n\n        OUTPUTS:    - dataframe: Processed Pandas dataframe.\n        \"\"\"\n\n        # Remove useless columns\n        dataframe['Close'] = dataframe['Adj Close']\n        del dataframe['Adj Close']\n\n        # Adapt the dataframe index and column names\n        dataframe.index.names = ['Date']\n        dataframe = dataframe[['Open', 'High', 'Low', 'Close', 'Volume']]\n\n        return dataframe\n\n\n\n###############################################################################\n############################# Class CSVHandler ################################\n###############################################################################\n\nclass CSVHandler:\n    \"\"\"\n    GOAL: Converting \"Pandas dataframe\" <-> \"CSV file\" (bidirectional).\n\n    VARIABLES: /\n\n    METHODS:    - dataframeToCSV: Saving a dataframe into a CSV file.\n                - CSVToDataframe: Loading a CSV file into a dataframe.\n    \"\"\"\n\n\n    def dataframeToCSV(self, name, dataframe):\n        \"\"\"\n        GOAL: Saving a dataframe into a CSV file.\n\n        INPUTS:     - name: Name of the CSV file.\n                    - dataframe: Pandas dataframe to be saved.\n\n        OUTPUTS: /\n        \"\"\"\n\n        path = name + '.csv'\n        dataframe.to_csv(path)\n\n\n    def CSVToDataframe(self, name):\n        \"\"\"\n        GOAL: Loading a CSV file into a dataframe.\n\n        INPUTS:     - name: Name of the CSV file.\n\n        OUTPUTS:    - dataframe: Pandas dataframe loaded.\n        \"\"\"\n\n        path = name + '.csv'\n        return pd.read_csv(path,\n                           header=0,\n                           index_col='Date',\n                           parse_dates=True)\n\n\n\n\n###############################################################################\n################################ Global variables #############################\n###############################################################################\n\n# Default values for the generation of the fictive stock market curves\nMIN = 100\nMAX = 200\nPERIOD = 252\n\n\n\n###############################################################################\n############################ Class StockGenerator #############################\n###############################################################################\n\nclass StockGenerator:\n    \"\"\"\n    GOAL: Generation of some fictive stock market curves\n          (linear, sinusoidal, triangle, etc.).\n\n    VARIABLES: /\n\n    METHODS: - linearUp: Generate a continuously increasing linear curve.\n             - linearDown: Generate a continuously decreasing linear curve.\n             - sinusoidal: Generate a (periodic) sinusoidal signal curve.\n             - triangle: Generate a (periodic) triangle signal curve.\n    \"\"\"\n\n    def linearUp (self, startingDate, endingDate, minValue=MIN, maxValue=MAX):\n        \"\"\"\n        GOAL: Generate a new fictive stock market as a continuously increasing\n              linear curve.\n\n        INPUTS: - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - minValue: Minimum price value.\n                - maxValue: Maximum price value.\n\n        OUTPUTS: - linearUpward: Generated fictive stock market dataframe.\n        \"\"\"\n\n        # Initialization of the new stock market dataframe\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        linearUpward = pd.DataFrame(index=DowJones.index)\n\n        # Generation of the fictive prices over the trading horizon\n        length = len(linearUpward.index)\n        prices = np.linspace(minValue, maxValue, num=length)\n\n        # Filling of the new fictive stock market dataframe\n        linearUpward['Open'] = prices\n        linearUpward['High'] = prices\n        linearUpward['Low'] = prices\n        linearUpward['Close'] = prices\n        linearUpward['Volume'] = 100000\n\n        return linearUpward\n\n\n    def linearDown (self, startingDate, endingDate, minValue=MIN, maxValue=MAX):\n        \"\"\"\n        GOAL: Generate a new fictive stock market as a continuously decreasing\n              linear curve.\n\n        INPUTS: - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - minValue: Minimum price value.\n                - maxValue: Maximum price value.\n\n        OUTPUTS: - linearDownward: Generated fictive stock market dataframe.\n        \"\"\"\n\n        # Initialization of the new stock market dataframe\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        linearDownward = pd.DataFrame(index=DowJones.index)\n\n        # Generation of the fictive prices over the trading horizon\n        length = len(linearDownward.index)\n        prices = np.linspace(minValue, maxValue, num=length)\n        prices = np.flip(prices)\n\n        # Filling of the new fictive stock market dataframe\n        linearDownward['Open'] = prices\n        linearDownward['High'] = prices\n        linearDownward['Low'] = prices\n        linearDownward['Close'] = prices\n        linearDownward['Volume'] = 100000\n\n        return linearDownward\n\n\n    def sinusoidal(self, startingDate, endingDate, minValue=MIN, maxValue=MAX, period=PERIOD):\n        \"\"\"\n        GOAL: Generate a new fictive stock market as a sinusoidal signal curve.\n\n        INPUTS: - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - minValue: Minimum price value.\n                - maxValue: Maximum price value.\n                - period: Period of the sinusoidal signal.\n\n        OUTPUTS: - sinusoidal: Generated fictive stock market dataframe.\n        \"\"\"\n\n        # Initialization of the new stock market dataframe\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        sinusoidal = pd.DataFrame(index=DowJones.index)\n\n        # Generation of the fictive prices over the trading horizon\n        length = len(sinusoidal.index)\n        t = np.linspace(0, length, num=length)\n        prices = minValue + maxValue / 2 * (np.sin(2 * np.pi * t / period) + 1) / 2\n\n        # Filling of the new fictive stock market dataframe\n        sinusoidal['Open'] = prices\n        sinusoidal['High'] = prices\n        sinusoidal['Low'] = prices\n        sinusoidal['Close'] = prices\n        sinusoidal['Volume'] = 100000\n\n        return sinusoidal\n\n\n    def triangle(self, startingDate, endingDate, minValue=MIN, maxValue=MAX, period=PERIOD):\n        \"\"\"\n        GOAL: Generate a new fictive stock market as a triangle signal curve.\n\n        INPUTS: - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - minValue: Minimum price value.\n                - maxValue: Maximum price value.\n                - period: Period of the triangle signal.\n\n        OUTPUTS: - triangle: Generated fictive stock market dataframe.\n        \"\"\"\n\n        # Initialization of the new stock market dataframe\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        triangle = pd.DataFrame(index=DowJones.index)\n\n        # Generation of the fictive prices over the trading horizon\n        length = len(triangle.index)\n        t = np.linspace(0, length, num=length)\n        prices = minValue + maxValue / 2 * np.abs(signal.sawtooth(2 * np.pi * t / period))\n\n        # Filling of the new fictive stock market dataframe\n        triangle['Open'] = prices\n        triangle['High'] = prices\n        triangle['Low'] = prices\n        triangle['Close'] = prices\n        triangle['Volume'] = 100000\n\n        return triangle\n\n\nsaving = True\nfictiveStocks = ('LINEARUP', 'LINEARDOWN', 'SINUSOIDAL', 'TRIANGLE')\n\n\n\n\nclass TradingEnv(gym.Env):\n    \"\"\"\n    GOAL: Implement a custom trading environment compatible with OpenAI Gym.\n\n    VARIABLES:  - data: Dataframe monitoring the trading activity.\n                - state: RL state to be returned to the RL agent.\n                - reward: RL reward to be returned to the RL agent.\n                - done: RL episode termination signal.\n                - t: Current trading time step.\n                - marketSymbol: Stock market symbol.\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - stateLength: Number of trading time steps included in the state.\n                - numberOfShares: Number of shares currently owned by the agent.\n                - transactionCosts: Transaction costs associated with the trading\n                                    activity (e.g. 0.01 is 1% of loss).\n\n    METHODS:    - __init__: Object constructor initializing the trading environment.\n                - reset: Perform a soft reset of the trading environment.\n                - step: Transition to the next trading time step.\n                - render: Illustrate graphically the trading environment.\n    \"\"\"\n\n    def __init__(self, marketSymbol, startingDate, endingDate, money, stateLength=30,\n                 transactionCosts=0, startingPoint=0, datadir='./data/'):\n        \"\"\"\n        GOAL: Object constructor initializing the trading environment by setting up\n              the trading activity dataframe as well as other important variables.\n\n        INPUTS: - marketSymbol: Stock market symbol.\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - money: Initial amount of money at the disposal of the agent.\n                - stateLength: Number of trading time steps included in the RL state.\n                - transactionCosts: Transaction costs associated with the trading\n                                    activity (e.g. 0.01 is 1% of loss).\n                - startingPoint: Optional starting point (iteration) of the trading activity.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # CASE 1: Fictive stock generation\n        if(marketSymbol in fictiveStocks):\n            stockGeneration = StockGenerator()\n            if(marketSymbol == 'LINEARUP'):\n                self.data = stockGeneration.linearUp(startingDate, endingDate)\n            elif(marketSymbol == 'LINEARDOWN'):\n                self.data = stockGeneration.linearDown(startingDate, endingDate)\n            elif(marketSymbol == 'SINUSOIDAL'):\n                self.data = stockGeneration.sinusoidal(startingDate, endingDate)\n            else:\n                self.data = stockGeneration.triangle(startingDate, endingDate)\n\n        # CASE 2: Real stock loading\n        else:\n            # Check if the stock market data is already present in the database\n            csvConverter = CSVHandler()\n            csvName = \"\".join([datadir, marketSymbol, '_', startingDate, '_', endingDate])\n            exists = os.path.isfile(csvName + '.csv')\n\n            # If affirmative, load the stock market data from the database\n            if(exists):\n                self.data = csvConverter.CSVToDataframe(csvName)\n            # Otherwise, download the stock market data from Yahoo Finance and save it in the database\n            else:\n                downloader1 = YahooFinance()\n                downloader2 = AlphaVantage()\n                try:\n                    self.data = downloader1.getDailyData(marketSymbol, startingDate, endingDate)\n                except:\n                    self.data = downloader2.getDailyData(marketSymbol, startingDate, endingDate)\n\n                if saving == True:\n                    csvConverter.dataframeToCSV(csvName, self.data)\n\n        # Interpolate in case of missing data\n        self.data.replace(0.0, np.nan, inplace=True)\n        self.data.interpolate(method='linear', limit=5, limit_area='inside', inplace=True)\n        self.data.fillna(method='ffill', inplace=True)\n        self.data.fillna(method='bfill', inplace=True)\n        self.data.fillna(0, inplace=True)\n\n        # Set the trading activity dataframe\n        self.data['Position'] = 0\n        self.data['Action'] = 0\n        self.data['Holdings'] = 0.\n        self.data['Cash'] = float(money)\n        self.data['Money'] = self.data['Holdings'] + self.data['Cash']\n        self.data['Returns'] = 0.\n\n        # Set the RL variables common to every OpenAI gym environments\n        self.state = [self.data['Close'][0:stateLength].tolist(),\n                      self.data['Low'][0:stateLength].tolist(),\n                      self.data['High'][0:stateLength].tolist(),\n                      self.data['Volume'][0:stateLength].tolist(),\n                      [0]]\n        self.reward = 0.\n        self.done = 0\n\n        # Set additional variables related to the trading activity\n        self.marketSymbol = marketSymbol\n        self.startingDate = startingDate\n        self.endingDate = endingDate\n        self.stateLength = stateLength\n        self.t = stateLength\n        self.numberOfShares = 0\n        self.transactionCosts = transactionCosts\n        self.epsilon = 0.1\n\n        # If required, set a custom starting point for the trading activity\n        if startingPoint:\n            self.setStartingPoint(startingPoint)\n\n\n    def reset(self):\n        \"\"\"\n        GOAL: Perform a soft reset of the trading environment.\n\n        INPUTS: /\n\n        OUTPUTS: - state: RL state returned to the trading strategy.\n        \"\"\"\n\n        # Reset the trading activity dataframe\n        self.data['Position'] = 0\n        self.data['Action'] = 0\n        self.data['Holdings'] = 0.\n        self.data['Cash'] = self.data['Cash'][0]\n        self.data['Money'] = self.data['Holdings'] + self.data['Cash']\n        self.data['Returns'] = 0.\n\n        # Reset the RL variables common to every OpenAI gym environments\n        self.state = [self.data['Close'][0:self.stateLength].tolist(),\n                      self.data['Low'][0:self.stateLength].tolist(),\n                      self.data['High'][0:self.stateLength].tolist(),\n                      self.data['Volume'][0:self.stateLength].tolist(),\n                      [0]]\n        self.reward = 0.\n        self.done = 0\n\n        # Reset additional variables related to the trading activity\n        self.t = self.stateLength\n        self.numberOfShares = 0\n\n        return self.state\n\n\n    def computeLowerBound(self, cash, numberOfShares, price):\n        \"\"\"\n        GOAL: Compute the lower bound of the complete RL action space,\n              i.e. the minimum number of share to trade.\n\n        INPUTS: - cash: Value of the cash owned by the agent.\n                - numberOfShares: Number of shares owned by the agent.\n                - price: Last price observed.\n\n        OUTPUTS: - lowerBound: Lower bound of the RL action space.\n        \"\"\"\n\n        # Computation of the RL action lower bound\n        deltaValues = - cash - numberOfShares * price * (1 + self.epsilon) * (1 + self.transactionCosts)\n        if deltaValues < 0:\n            lowerBound = deltaValues / (price * (2 * self.transactionCosts + (self.epsilon * (1 + self.transactionCosts))))\n        else:\n            lowerBound = deltaValues / (price * self.epsilon * (1 + self.transactionCosts))\n        return lowerBound\n\n\n    def step(self, action):\n        \"\"\"\n        GOAL: Transition to the next trading time step based on the\n              trading position decision made (either long or short).\n\n        INPUTS: - action: Trading decision (1 = long, 0 = short).\n\n        OUTPUTS: - state: RL state to be returned to the RL agent.\n                 - reward: RL reward to be returned to the RL agent.\n                 - done: RL episode termination signal (boolean).\n                 - info: Additional information returned to the RL agent.\n        \"\"\"\n\n        # Stting of some local variables\n        t = self.t\n        numberOfShares = self.numberOfShares\n        customReward = False\n\n        # CASE 1: LONG POSITION\n        if(action == 1):\n            self.data['Position'][t] = 1\n            # Case a: Long -> Long\n            if(self.data['Position'][t - 1] == 1):\n                self.data['Cash'][t] = self.data['Cash'][t - 1]\n                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n            # Case b: No position -> Long\n            elif(self.data['Position'][t - 1] == 0):\n                self.numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t - 1] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = 1\n            # Case c: Short -> Long\n            else:\n                self.data['Cash'][t] = self.data['Cash'][t - 1] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                self.numberOfShares = math.floor(self.data['Cash'][t]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = 1\n\n        # CASE 2: SHORT POSITION\n        elif(action == 0):\n            self.data['Position'][t] = -1\n            # Case a: Short -> Short\n            if(self.data['Position'][t - 1] == -1):\n                lowerBound = self.computeLowerBound(self.data['Cash'][t - 1], -numberOfShares, self.data['Close'][t-1])\n                if lowerBound <= 0:\n                    self.data['Cash'][t] = self.data['Cash'][t - 1]\n                    self.data['Holdings'][t] =  - self.numberOfShares * self.data['Close'][t]\n                else:\n                    numberOfSharesToBuy = min(math.floor(lowerBound), self.numberOfShares)\n                    self.numberOfShares -= numberOfSharesToBuy\n                    self.data['Cash'][t] = self.data['Cash'][t - 1] - numberOfSharesToBuy * self.data['Close'][t] * (1 + self.transactionCosts)\n                    self.data['Holdings'][t] =  - self.numberOfShares * self.data['Close'][t]\n                    customReward = True\n            # Case b: No position -> Short\n            elif(self.data['Position'][t - 1] == 0):\n                self.numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t - 1] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                self.data['Holdings'][t] = - self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = -1\n            # Case c: Long -> Short\n            else:\n                self.data['Cash'][t] = self.data['Cash'][t - 1] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                self.numberOfShares = math.floor(self.data['Cash'][t]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                self.data['Holdings'][t] = - self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = -1\n\n        # CASE 3: PROHIBITED ACTION\n        else:\n            raise SystemExit(\"Prohibited action! Action should be either 1 (long) or 0 (short).\")\n\n        # Update the total amount of money owned by the agent, as well as the return generated\n        self.data['Money'][t] = self.data['Holdings'][t] + self.data['Cash'][t]\n        self.data['Returns'][t] = (self.data['Money'][t] - self.data['Money'][t-1])/self.data['Money'][t-1]\n\n        # Set the RL reward returned to the trading agent\n        if not customReward:\n            self.reward = self.data['Returns'][t]\n        else:\n            self.reward = (self.data['Close'][t-1] - self.data['Close'][t])/self.data['Close'][t-1]\n\n        # Transition to the next trading time step\n        self.t = self.t + 1\n        self.state = [self.data['Close'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['Low'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['High'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['Volume'][self.t - self.stateLength : self.t].tolist(),\n                      [self.data['Position'][self.t - 1]]]\n        if(self.t == self.data.shape[0]):\n            self.done = 1\n\n        # Same reasoning with the other action (exploration trick)\n        otherAction = int(not bool(action))\n        customReward = False\n        if(otherAction == 1):\n            otherPosition = 1\n            if(self.data['Position'][t - 1] == 1):\n                otherCash = self.data['Cash'][t - 1]\n                otherHoldings = numberOfShares * self.data['Close'][t]\n            elif(self.data['Position'][t - 1] == 0):\n                numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = self.data['Cash'][t - 1] - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                otherHoldings = numberOfShares * self.data['Close'][t]\n            else:\n                otherCash = self.data['Cash'][t - 1] - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                numberOfShares = math.floor(otherCash/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = otherCash - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                otherHoldings = numberOfShares * self.data['Close'][t]\n        else:\n            otherPosition = -1\n            if(self.data['Position'][t - 1] == -1):\n                lowerBound = self.computeLowerBound(self.data['Cash'][t - 1], -numberOfShares, self.data['Close'][t-1])\n                if lowerBound <= 0:\n                    otherCash = self.data['Cash'][t - 1]\n                    otherHoldings =  - numberOfShares * self.data['Close'][t]\n                else:\n                    numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n                    numberOfShares -= numberOfSharesToBuy\n                    otherCash = self.data['Cash'][t - 1] - numberOfSharesToBuy * self.data['Close'][t] * (1 + self.transactionCosts)\n                    otherHoldings =  - numberOfShares * self.data['Close'][t]\n                    customReward = True\n            elif(self.data['Position'][t - 1] == 0):\n                numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = self.data['Cash'][t - 1] + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                otherHoldings = - numberOfShares * self.data['Close'][t]\n            else:\n                otherCash = self.data['Cash'][t - 1] + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                numberOfShares = math.floor(otherCash/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = otherCash + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                otherHoldings = - self.numberOfShares * self.data['Close'][t]\n        otherMoney = otherHoldings + otherCash\n        if not customReward:\n            otherReward = (otherMoney - self.data['Money'][t-1])/self.data['Money'][t-1]\n        else:\n            otherReward = (self.data['Close'][t-1] - self.data['Close'][t])/self.data['Close'][t-1]\n        otherState = [self.data['Close'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['Low'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['High'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['Volume'][self.t - self.stateLength : self.t].tolist(),\n                      [otherPosition]]\n        self.info = {'State' : otherState, 'Reward' : otherReward, 'Done' : self.done}\n\n        # Return the trading environment feedback to the RL trading agent\n        return self.state, self.reward, self.done, self.info\n\n\n    def render(self):\n        \"\"\"\n        GOAL: Illustrate graphically the trading activity, by plotting\n              both the evolution of the stock market price and the\n              evolution of the trading capital. All the trading decisions\n              (long and short positions) are displayed as well.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Set the Matplotlib figure and subplots\n        fig = plt.figure(figsize=(10, 8))\n        ax1 = fig.add_subplot(211, ylabel='Price', xlabel='Time')\n        ax2 = fig.add_subplot(212, ylabel='Capital', xlabel='Time', sharex=ax1)\n\n        # Plot the first graph -> Evolution of the stock market price\n        self.data['Close'].plot(ax=ax1, color='blue', lw=2)\n        ax1.plot(self.data.loc[self.data['Action'] == 1.0].index,\n                 self.data['Close'][self.data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax1.plot(self.data.loc[self.data['Action'] == -1.0].index,\n                 self.data['Close'][self.data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n\n        # Plot the second graph -> Evolution of the trading capital\n        self.data['Money'].plot(ax=ax2, color='blue', lw=2)\n        ax2.plot(self.data.loc[self.data['Action'] == 1.0].index,\n                 self.data['Money'][self.data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax2.plot(self.data.loc[self.data['Action'] == -1.0].index,\n                 self.data['Money'][self.data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n\n        # Generation of the two legends and plotting\n        ax1.legend([\"Price\", \"Long\",  \"Short\"])\n        ax2.legend([\"Capital\", \"Long\", \"Short\"])\n        plt.savefig(''.join(['Figures/', str(self.marketSymbol), '_Rendering', '.png']))\n        #plt.show()\n\n\n    def setStartingPoint(self, startingPoint):\n        \"\"\"\n        GOAL: Setting an arbitrary starting point regarding the trading activity.\n              This technique is used for better generalization of the RL agent.\n\n        INPUTS: - startingPoint: Optional starting point (iteration) of the trading activity.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Setting a custom starting point\n        self.t = np.clip(startingPoint, self.stateLength, len(self.data.index))\n\n        # Set the RL variables common to every OpenAI gym environments\n        self.state = [self.data['Close'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['Low'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['High'][self.t - self.stateLength : self.t].tolist(),\n                      self.data['Volume'][self.t - self.stateLength : self.t].tolist(),\n                      [self.data['Position'][self.t - 1]]]\n        if(self.t == self.data.shape[0]):\n            self.done = 1\n\n\nclass TradingSimulator:\n    \"\"\"\n    GOAL: Accurately simulating multiple trading strategies on different stocks\n          to analyze and compare their performance.\n\n    VARIABLES: /\n\n    METHODS:   - displayTestbench: Display consecutively all the stocks\n                                   included in the testbench.\n               - analyseTimeSeries: Perform a detailled analysis of the stock\n                                    market price time series.\n               - plotEntireTrading: Plot the entire trading activity, with both\n                                    the training and testing phases rendered on\n                                    the same graph.\n               - simulateNewStrategy: Simulate a new trading strategy on a\n                                      a certain stock of the testbench.\n               - simulateExistingStrategy: Simulate an already existing\n                                           trading strategy on a certain\n                                           stock of the testbench.\n               - evaluateStrategy: Evaluate a trading strategy on the\n                                   entire testbench.\n               - evaluateStock: Compare different trading strategies\n                                on a certain stock of the testbench.\n    \"\"\"\n\n    def displayTestbench(self, startingDate=startingDate, endingDate=endingDate):\n        \"\"\"\n        GOAL: Display consecutively all the stocks included in the\n              testbench (trading indices and companies).\n\n        INPUTS: - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Display the stocks included in the testbench (trading indices)\n        for _, stock in indices.items():\n            env = TradingEnv(stock, startingDate, endingDate, 0)\n            env.render()\n\n        # Display the stocks included in the testbench (companies)\n        for _, stock in companies.items():\n            env = TradingEnv(stock, startingDate, endingDate, 0)\n            env.render()\n\n\n    def analyseTimeSeries(self, stockName, startingDate=startingDate, endingDate=endingDate, splitingDate=splitingDate):\n        \"\"\"\n        GOAL: Perform a detailled analysis of the stock market\n              price time series.\n\n        INPUTS: - stockName: Name of the stock (in the testbench).\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - splitingDate: Spliting date between the training dataset\n                                and the testing dataset.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Retrieve the trading stock information\n        if(stockName in fictives):\n            stock = fictives[stockName]\n        elif(stockName in indices):\n            stock = indices[stockName]\n        elif(stockName in companies):\n            stock = companies[stockName]\n        # Error message if the stock specified is not valid or not supported\n        else:\n            print(\"The stock specified is not valid, only the following stocks are supported:\")\n            for stock in fictives:\n                print(\"\".join(['- ', stock]))\n            for stock in indices:\n                print(\"\".join(['- ', stock]))\n            for stock in companies:\n                print(\"\".join(['- ', stock]))\n            raise SystemError(\"Please check the stock specified.\")\n\n        # TRAINING DATA\n        print(\"\\n\\n\\nAnalysis of the TRAINING phase time series\")\n        print(\"------------------------------------------\\n\")\n        trainingEnv = TradingEnv(stock, startingDate, splitingDate, 0)\n        timeSeries = trainingEnv.data['Close']\n        analyser = TimeSeriesAnalyser(timeSeries)\n        analyser.timeSeriesDecomposition()\n        analyser.stationarityAnalysis()\n        analyser.cyclicityAnalysis()\n\n        # TESTING DATA\n        print(\"\\n\\n\\nAnalysis of the TESTING phase time series\")\n        print(\"------------------------------------------\\n\")\n        testingEnv = TradingEnv(stock, splitingDate, endingDate, 0)\n        timeSeries = testingEnv.data['Close']\n        analyser = TimeSeriesAnalyser(timeSeries)\n        analyser.timeSeriesDecomposition()\n        analyser.stationarityAnalysis()\n        analyser.cyclicityAnalysis()\n\n        # ENTIRE TRADING DATA\n        print(\"\\n\\n\\nAnalysis of the entire time series (both training and testing phases)\")\n        print(\"---------------------------------------------------------------------\\n\")\n        tradingEnv = TradingEnv(stock, startingDate, endingDate, 0)\n        timeSeries = tradingEnv.data['Close']\n        analyser = TimeSeriesAnalyser(timeSeries)\n        analyser.timeSeriesDecomposition()\n        analyser.stationarityAnalysis()\n        analyser.cyclicityAnalysis()\n\n\n    def plotEntireTrading(self, trainingEnv, testingEnv):\n        \"\"\"\n        GOAL: Plot the entire trading activity, with both the training\n              and testing phases rendered on the same graph for\n              comparison purposes.\n\n        INPUTS: - trainingEnv: Trading environment for training.\n                - testingEnv: Trading environment for testing.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Artificial trick to assert the continuity of the Money curve\n        ratio = trainingEnv.data['Money'][-1]/testingEnv.data['Money'][0]\n        testingEnv.data['Money'] = ratio * testingEnv.data['Money']\n\n        # Concatenation of the training and testing trading dataframes\n        dataframes = [trainingEnv.data, testingEnv.data]\n        data = pd.concat(dataframes)\n\n        # Set the Matplotlib figure and subplots\n        fig = plt.figure(figsize=(10, 8))\n        ax1 = fig.add_subplot(211, ylabel='Price', xlabel='Time')\n        ax2 = fig.add_subplot(212, ylabel='Capital', xlabel='Time', sharex=ax1)\n\n        # Plot the first graph -> Evolution of the stock market price\n        trainingEnv.data['Close'].plot(ax=ax1, color='blue', lw=2)\n        testingEnv.data['Close'].plot(ax=ax1, color='blue', lw=2, label='_nolegend_')\n        ax1.plot(data.loc[data['Action'] == 1.0].index,\n                 data['Close'][data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax1.plot(data.loc[data['Action'] == -1.0].index,\n                 data['Close'][data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n\n        # Plot the second graph -> Evolution of the trading capital\n        trainingEnv.data['Money'].plot(ax=ax2, color='blue', lw=2)\n        testingEnv.data['Money'].plot(ax=ax2, color='blue', lw=2, label='_nolegend_')\n        ax2.plot(data.loc[data['Action'] == 1.0].index,\n                 data['Money'][data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax2.plot(data.loc[data['Action'] == -1.0].index,\n                 data['Money'][data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n\n        # Plot the vertical line seperating the training and testing datasets\n        ax1.axvline(pd.Date(splitingDate), color='black', linewidth=2.0)\n        ax2.axvline(pd.Date(splitingDate), color='black', linewidth=2.0)\n\n        # Generation of the two legends and plotting\n        ax1.legend([\"Price\", \"Long\",  \"Short\", \"Train/Test separation\"])\n        ax2.legend([\"Capital\", \"Long\", \"Short\", \"Train/Test separation\"])\n        plt.savefig(''.join(['Figures/', str(trainingEnv.marketSymbol), '_TrainingTestingRendering', '.png']))\n        #plt.show()\n\n\n    def simulateNewStrategy(self, strategyName, stockName,\n                            startingDate=startingDate, endingDate=endingDate, splitingDate=splitingDate,\n                            observationSpace=observationSpace, actionSpace=actionSpace,\n                            money=money, stateLength=stateLength, transactionCosts=transactionCosts,\n                            bounds=bounds, step=step, numberOfEpisodes=numberOfEpisodes,\n                            verbose=True, plotTraining=True, rendering=True, showPerformance=True,\n                            saveStrategy=False):\n        \"\"\"\n        GOAL: Simulate a new trading strategy on a certain stock included in the\n              testbench, with both learning and testing phases.\n\n        INPUTS: - strategyName: Name of the trading strategy.\n                - stockName: Name of the stock (in the testbench).\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - splitingDate: Spliting date between the training dataset\n                                and the testing dataset.\n                - observationSpace: Size of the RL observation space.\n                - actionSpace: Size of the RL action space.\n                - money: Initial capital at the disposal of the agent.\n                - stateLength: Length of the trading agent state.\n                - transactionCosts: Additional costs incurred while trading\n                                    (e.g. 0.01 <=> 1% of transaction costs).\n                - bounds: Bounds of the parameter search space (training).\n                - step: Step of the parameter search space (training).\n                - numberOfEpisodes: Number of epsiodes of the RL training phase.\n                - verbose: Enable the printing of a simulation feedback.\n                - plotTraining: Enable the plotting of the training results.\n                - rendering: Enable the rendering of the trading environment.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n                - saveStrategy: Enable the saving of the trading strategy.\n\n        OUTPUTS: - tradingStrategy: Trading strategy simulated.\n                 - trainingEnv: Trading environment related to the training phase.\n                 - testingEnv: Trading environment related to the testing phase.\n        \"\"\"\n\n        # 1. INITIALIZATION PHASE\n\n        # Retrieve the trading strategy information\n        if(strategyName in strategies):\n            strategy = strategies[strategyName]\n            trainingParameters = [bounds, step]\n            ai = False\n        elif(strategyName in strategiesAI):\n            strategy = strategiesAI[strategyName]\n            trainingParameters = [numberOfEpisodes]\n            ai = True\n        # Error message if the strategy specified is not valid or not supported\n        else:\n            print(\"The strategy specified is not valid, only the following strategies are supported:\")\n            for strategy in strategies:\n                print(\"\".join(['- ', strategy]))\n            for strategy in strategiesAI:\n                print(\"\".join(['- ', strategy]))\n            raise SystemError(\"Please check the trading strategy specified.\")\n\n        # Retrieve the trading stock information\n        if(stockName in fictives):\n            stock = fictives[stockName]\n        elif(stockName in indices):\n            stock = indices[stockName]\n        elif(stockName in companies):\n            stock = companies[stockName]\n        # Error message if the stock specified is not valid or not supported\n        else:\n            print(\"The stock specified is not valid, only the following stocks are supported:\")\n            for stock in fictives:\n                print(\"\".join(['- ', stock]))\n            for stock in indices:\n                print(\"\".join(['- ', stock]))\n            for stock in companies:\n                print(\"\".join(['- ', stock]))\n            raise SystemError(\"Please check the stock specified.\")\n\n\n        # 2. TRAINING PHASE\n\n        # Initialize the trading environment associated with the training phase\n        trainingEnv = TradingEnv(stock, startingDate, splitingDate, money, stateLength, transactionCosts)\n\n        # Instanciate the strategy classes\n        if ai:\n            tradingStrategy = TDQN(observationSpace, actionSpace)\n        else:\n            tradingStrategy = ClassicalStrategy()\n\n        # Training of the trading strategy\n        trainingEnv = tradingStrategy.training(trainingEnv, trainingParameters=trainingParameters,\n                                               verbose=verbose, rendering=rendering,\n                                               plotTraining=plotTraining, showPerformance=showPerformance)\n\n\n        # 3. TESTING PHASE\n\n        # Initialize the trading environment associated with the testing phase\n        testingEnv = TradingEnv(stock, splitingDate, endingDate, money, stateLength, transactionCosts)\n\n        # Testing of the trading strategy\n        testingEnv = tradingStrategy.testing(trainingEnv, testingEnv, rendering=rendering, showPerformance=showPerformance)\n\n        # Show the entire unified rendering of the training and testing phases\n        if rendering:\n            self.plotEntireTrading(trainingEnv, testingEnv)\n\n\n        # 4. TERMINATION PHASE\n\n        # If required, save the trading strategy with Pickle\n        if(saveStrategy):\n            fileName = \"\".join([\"Strategies/\", strategy, \"_\", stock, \"_\", startingDate, \"_\", splitingDate])\n            if ai:\n                tradingStrategy.saveModel(fileName)\n            else:\n                fileHandler = open(fileName, 'wb')\n                pickle.dump(tradingStrategy, fileHandler)\n\n        # Return of the trading strategy simulated and of the trading environments backtested\n        return tradingStrategy, trainingEnv, testingEnv\n\n\n    def simulateExistingStrategy(self, strategyName, stockName,\n                                 startingDate=startingDate, endingDate=endingDate, splitingDate=splitingDate,\n                                 observationSpace=observationSpace, actionSpace=actionSpace,\n                                 money=money, stateLength=stateLength, transactionCosts=transactionCosts,\n                                 rendering=True, showPerformance=True, strategiesDir='./strategies/'):\n        \"\"\"\n        GOAL: Simulate an already existing trading strategy on a certain\n              stock of the testbench, the strategy being loaded from the\n              strategy dataset. There is no training phase, only a testing\n              phase.\n\n        INPUTS: - strategyName: Name of the trading strategy.\n                - stockName: Name of the stock (in the testbench).\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - splitingDate: Spliting date between the training dataset\n                                and the testing dataset.\n                - observationSpace: Size of the RL observation space.\n                - actionSpace: Size of the RL action space.\n                - money: Initial capital at the disposal of the agent.\n                - stateLength: Length of the trading agent state.\n                - transactionCosts: Additional costs incurred while trading\n                                    (e.g. 0.01 <=> 1% of transaction costs).\n                - rendering: Enable the rendering of the trading environment.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - tradingStrategy: Trading strategy simulated.\n                 - trainingEnv: Trading environment related to the training phase.\n                 - testingEnv: Trading environment related to the testing phase.\n        \"\"\"\n\n        # 1. INITIALIZATION PHASE\n\n        # Retrieve the trading strategy information\n        if(strategyName in strategies):\n            strategy = strategies[strategyName]\n            ai = False\n        elif(strategyName in strategiesAI):\n            strategy = strategiesAI[strategyName]\n            ai = True\n        # Error message if the strategy specified is not valid or not supported\n        else:\n            print(\"The strategy specified is not valid, only the following strategies are supported:\")\n            for strategy in strategies:\n                print(\"\".join(['- ', strategy]))\n            for strategy in strategiesAI:\n                print(\"\".join(['- ', strategy]))\n            raise SystemError(\"Please check the trading strategy specified.\")\n\n        # Retrieve the trading stock information\n        if(stockName in fictives):\n            stock = fictives[stockName]\n        elif(stockName in indices):\n            stock = indices[stockName]\n        elif(stockName in companies):\n            stock = companies[stockName]\n        # Error message if the stock specified is not valid or not supported\n        else:\n            print(\"The stock specified is not valid, only the following stocks are supported:\")\n            for stock in fictives:\n                print(\"\".join(['- ', stock]))\n            for stock in indices:\n                print(\"\".join(['- ', stock]))\n            for stock in companies:\n                print(\"\".join(['- ', stock]))\n            raise SystemError(\"Please check the stock specified.\")\n\n\n        # 2. LOADING PHASE\n\n        # Check that the strategy to load exists in the strategy dataset\n        fileName = \"\".join([strategiesDir, strategy, \"_\", stock, \"_\", startingDate, \"_\", splitingDate])\n        exists = os.path.isfile(fileName)\n        # If affirmative, load the trading strategy\n        if exists:\n            if ai:\n                tradingStrategy = TDQN(observationSpace, actionSpace)\n                tradingStrategy.loadModel(fileName)\n            else:\n                fileHandler = open(fileName, 'rb')\n                tradingStrategy = pickle.load(fileHandler)\n        else:\n            raise SystemError(\"The trading strategy specified does not exist, please provide a valid one.\")\n\n\n        # 3. TESTING PHASE\n\n        # Initialize the trading environments associated with the testing phase\n        trainingEnv = TradingEnv(stock, startingDate, splitingDate, money, stateLength, transactionCosts)\n        testingEnv = TradingEnv(stock, splitingDate, endingDate, money, stateLength, transactionCosts)\n\n        # Testing of the trading strategy\n        trainingEnv = tradingStrategy.testing(trainingEnv, trainingEnv, rendering=rendering, showPerformance=showPerformance)\n        testingEnv = tradingStrategy.testing(trainingEnv, testingEnv, rendering=rendering, showPerformance=showPerformance)\n\n        # Show the entire unified rendering of the training and testing phases\n        if rendering:\n            self.plotEntireTrading(trainingEnv, testingEnv)\n\n        return tradingStrategy, trainingEnv, testingEnv\n\n\n    def evaluateStrategy(self, strategyName,\n                         startingDate=startingDate, endingDate=endingDate, splitingDate=splitingDate,\n                         observationSpace=observationSpace, actionSpace=actionSpace,\n                         money=money, stateLength=stateLength, transactionCosts=transactionCosts,\n                         bounds=bounds, step=step, numberOfEpisodes=numberOfEpisodes,\n                         verbose=False, plotTraining=False, rendering=False, showPerformance=False,\n                         saveStrategy=False):\n        \"\"\"\n        GOAL: Evaluate the performance of a trading strategy on the entire\n              testbench of stocks designed.\n\n        INPUTS: - strategyName: Name of the trading strategy.\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - splitingDate: Spliting date between the training dataset\n                                and the testing dataset.\n                - observationSpace: Size of the RL observation space.\n                - actionSpace: Size of the RL action space.\n                - money: Initial capital at the disposal of the agent.\n                - stateLength: Length of the trading agent state.\n                - transactionCosts: Additional costs incurred while trading\n                                    (e.g. 0.01 <=> 1% of transaction costs).\n                - bounds: Bounds of the parameter search space (training).\n                - step: Step of the parameter search space (training).\n                - numberOfEpisodes: Number of epsiodes of the RL training phase.\n                - verbose: Enable the printing of simulation feedbacks.\n                - plotTraining: Enable the plotting of the training results.\n                - rendering: Enable the rendering of the trading environment.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n                - saveStrategy: Enable the saving of the trading strategy.\n\n        OUTPUTS: - performanceTable: Table summarizing the performance of\n                                     a trading strategy.\n        \"\"\"\n\n        # Initialization of some variables\n        performanceTable = [[\"Profit & Loss (P&L)\"], [\"Annualized Return\"], [\"Annualized Volatility\"], [\"Sharpe Ratio\"], [\"Sortino Ratio\"], [\"Maximum DrawDown\"], [\"Maximum DrawDown Duration\"], [\"Profitability\"], [\"Ratio Average Profit/Loss\"], [\"Skewness\"]]\n        headers = [\"Performance Indicator\"]\n\n        # Loop through each stock included in the testbench (progress bar)\n        print(\"Trading strategy evaluation progression:\")\n        #for stock in tqdm(itertools.chain(indices, companies)):\n        for stock in tqdm(stocks):\n\n            # Simulation of the trading strategy on the current stock\n            try:\n                # Simulate an already existing trading strategy on the current stock\n                _, _, testingEnv = self.simulateExistingStrategy(strategyName, stock, startingDate, endingDate, splitingDate, observationSpace, actionSpace, money, stateLength, transactionCosts, rendering, showPerformance)\n            except SystemError:\n                # Simulate a new trading strategy on the current stock\n                _, _, testingEnv = self.simulateNewStrategy(strategyName, stock, startingDate, endingDate, splitingDate, observationSpace, actionSpace, money, stateLength, transactionCosts, bounds, step, numberOfEpisodes, verbose, plotTraining, rendering, showPerformance, saveStrategy)\n\n            # Retrieve the trading performance associated with the trading strategy\n            analyser = PerformanceEstimator(testingEnv.data)\n            performance = analyser.computePerformance()\n\n            # Get the required format for the display of the performance table\n            headers.append(stock)\n            for i in range(len(performanceTable)):\n                performanceTable[i].append(performance[i][1])\n\n        # Display the performance table computed\n        tabulation = tabulate(performanceTable, headers, tablefmt=\"fancy_grid\", stralign=\"center\")\n        print(tabulation)\n\n        # Computation of the average Sharpe Ratio (default performance indicator)\n        sharpeRatio = np.mean([float(item) for item in performanceTable[3][1:]])\n        print(\"Average Sharpe Ratio: \" + \"{0:.3f}\".format(sharpeRatio))\n\n        return performanceTable\n\n\n    def evaluateStock(self, stockName,\n                      startingDate=startingDate, endingDate=endingDate, splitingDate=splitingDate,\n                      observationSpace=observationSpace, actionSpace=actionSpace,\n                      money=money, stateLength=stateLength, transactionCosts=transactionCosts,\n                      bounds=bounds, step=step, numberOfEpisodes=numberOfEpisodes,\n                      verbose=False, plotTraining=False, rendering=False, showPerformance=False,\n                      saveStrategy=False):\n\n        \"\"\"\n        GOAL: Simulate and compare the performance achieved by all the supported\n              trading strategies on a certain stock of the testbench.\n\n        INPUTS: - stockName: Name of the stock (in the testbench).\n                - startingDate: Beginning of the trading horizon.\n                - endingDate: Ending of the trading horizon.\n                - splitingDate: Spliting date between the training dataset\n                                and the testing dataset.\n                - money: Initial capital at the disposal of the agent.\n                - stateLength: Length of the trading agent state.\n                - transactionCosts: Additional costs incurred while trading\n                                    (e.g. 0.01 <=> 1% of transaction costs).\n                - bounds: Bounds of the parameter search space (training).\n                - step: Step of the parameter search space (training).\n                - numberOfEpisodes: Number of epsiodes of the RL training phase.\n                - verbose: Enable the printing of a simulation feedback.\n                - plotTraining: Enable the plotting of the training results.\n                - rendering: Enable the rendering of the trading environment.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n                - saveStrategy: Enable the saving of the trading strategy.\n\n        OUTPUTS: - performanceTable: Table summarizing the performance of\n                                     a trading strategy.\n        \"\"\"\n\n        # Initialization of some variables\n        performanceTable = [[\"Profit & Loss (P&L)\"], [\"Annualized Return\"], [\"Annualized Volatility\"], [\"Sharpe Ratio\"], [\"Sortino Ratio\"], [\"Maximum DrawDown\"], [\"Maximum DrawDown Duration\"], [\"Profitability\"], [\"Ratio Average Profit/Loss\"], [\"Skewness\"]]\n        headers = [\"Performance Indicator\"]\n\n        # Loop through all the trading strategies supported (progress bar)\n        print(\"Trading strategies evaluation progression:\")\n        for strategy in tqdm(itertools.chain(strategies, strategiesAI)):\n\n            # Simulation of the current trading strategy on the stock\n            try:\n                # Simulate an already existing trading strategy on the stock\n                _, _, testingEnv = self.simulateExistingStrategy(strategy, stockName, startingDate, endingDate, splitingDate, observationSpace, actionSpace, money, stateLength, transactionCosts, rendering, showPerformance)\n            except SystemError:\n                # Simulate a new trading strategy on the stock\n                _, _, testingEnv = self.simulateNewStrategy(strategy, stockName, startingDate, endingDate, splitingDate, observationSpace, actionSpace, money, stateLength, transactionCosts, bounds, step, numberOfEpisodes, verbose, plotTraining, rendering, showPerformance, saveStrategy)\n\n            # Retrieve the trading performance associated with the trading strategy\n            analyser = PerformanceEstimator(testingEnv.data)\n            performance = analyser.computePerformance()\n\n            # Get the required format for the display of the performance table\n            headers.append(strategy)\n            for i in range(len(performanceTable)):\n                performanceTable[i].append(performance[i][1])\n\n        # Display the performance table\n        tabulation = tabulate(performanceTable, headers, tablefmt=\"fancy_grid\", stralign=\"center\")\n        print(tabulation)\n\n        return performanceTable\n\nimport math\nimport random\nimport copy\nimport datetime\n\nimport numpy as np\n\nfrom collections import deque\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\n\n###############################################################################\n################################ Global variables #############################\n###############################################################################\n\n# Default parameters related to the DQN algorithm\ngamma = 0.4\nlearningRate = 0.0001\ntargetNetworkUpdate = 1000\nlearningUpdatePeriod = 1\n\n# Default parameters related to the Experience Replay mechanism\ncapacity = 100000\nbatchSize = 32\nexperiencesRequired = 1000\n\n# Default parameters related to the Deep Neural Network\nnumberOfNeurons = 512\ndropout = 0.2\n\n# Default parameters related to the Epsilon-Greedy exploration technique\nepsilonStart = 1.0\nepsilonEnd = 0.01\nepsilonDecay = 10000\n\n# Default parameters regarding the sticky actions RL generalization technique\nalpha = 0.1\n\n# Default parameters related to preprocessing\nfilterOrder = 5\n\n# Default paramters related to the clipping of both the gradient and the RL rewards\ngradientClipping = 1\nrewardClipping = 1\n\n# Default parameter related to the L2 Regularization\nL2Factor = 0.000001\n\n# Default paramter related to the hardware acceleration (CUDA)\nGPUNumber = 0\n\n\n\n###############################################################################\n############################### Class ReplayMemory ############################\n###############################################################################\n\nclass ReplayMemory:\n    \"\"\"\n    GOAL: Implementing the replay memory required for the Experience Replay\n          mechanism of the DQN Reinforcement Learning algorithm.\n\n    VARIABLES:  - memory: Data structure storing the experiences.\n\n    METHODS:    - __init__: Initialization of the memory data structure.\n                - push: Insert a new experience into the replay memory.\n                - sample: Sample a batch of experiences from the replay memory.\n                - __len__: Return the length of the replay memory.\n                - reset: Reset the replay memory.\n    \"\"\"\n\n    def __init__(self, capacity=capacity):\n        \"\"\"\n        GOAL: Initializating the replay memory data structure.\n\n        INPUTS: - capacity: Capacity of the data structure, specifying the\n                            maximum number of experiences to be stored\n                            simultaneously.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.memory = deque(maxlen=capacity)\n\n\n    def push(self, state, action, reward, nextState, done):\n        \"\"\"\n        GOAL: Insert a new experience into the replay memory. An experience\n              is composed of a state, an action, a reward, a next state and\n              a termination signal.\n\n        INPUTS: - state: RL state of the experience to be stored.\n                - action: RL action of the experience to be stored.\n                - reward: RL reward of the experience to be stored.\n                - nextState: RL next state of the experience to be stored.\n                - done: RL termination signal of the experience to be stored.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.memory.append((state, action, reward, nextState, done))\n\n\n    def sample(self, batchSize):\n        \"\"\"\n        GOAL: Sample a batch of experiences from the replay memory.\n\n        INPUTS: - batchSize: Size of the batch to sample.\n\n        OUTPUTS: - state: RL states of the experience batch sampled.\n                 - action: RL actions of the experience batch sampled.\n                 - reward: RL rewards of the experience batch sampled.\n                 - nextState: RL next states of the experience batch sampled.\n                 - done: RL termination signals of the experience batch sampled.\n        \"\"\"\n\n        state, action, reward, nextState, done = zip(*random.sample(self.memory, batchSize))\n        return state, action, reward, nextState, done\n\n\n    def __len__(self):\n        \"\"\"\n        GOAL: Return the capicity of the replay memory, which is the maximum number of\n              experiences which can be simultaneously stored in the replay memory.\n\n        INPUTS: /\n\n        OUTPUTS: - length: Capacity of the replay memory.\n        \"\"\"\n\n        return len(self.memory)\n\n\n    def reset(self):\n        \"\"\"\n        GOAL: Reset (empty) the replay memory.\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.memory = deque(maxlen=capacity)\n\n\n\n\n###############################################################################\n################################### Class DQN #################################\n###############################################################################\n\nclass DQN(nn.Module):\n    \"\"\"\n    GOAL: Implementing the Deep Neural Network of the DQN Reinforcement\n          Learning algorithm.\n\n    VARIABLES:  - fc1: Fully Connected layer number 1.\n                - fc2: Fully Connected layer number 2.\n                - fc3: Fully Connected layer number 3.\n                - fc4: Fully Connected layer number 4.\n                - fc5: Fully Connected layer number 5.\n                - dropout1: Dropout layer number 1.\n                - dropout2: Dropout layer number 2.\n                - dropout3: Dropout layer number 3.\n                - dropout4: Dropout layer number 4.\n                - bn1: Batch normalization layer number 1.\n                - bn2: Batch normalization layer number 2.\n                - bn3: Batch normalization layer number 3.\n                - bn4: Batch normalization layer number 4.\n\n    METHODS:    - __init__: Initialization of the Deep Neural Network.\n                - forward: Forward pass of the Deep Neural Network.\n    \"\"\"\n\n    def __init__(self, numberOfInputs, numberOfOutputs, numberOfNeurons=numberOfNeurons, dropout=dropout):\n        \"\"\"\n        GOAL: Defining and initializing the Deep Neural Network of the\n              DQN Reinforcement Learning algorithm.\n\n        INPUTS: - numberOfInputs: Number of inputs of the Deep Neural Network.\n                - numberOfOutputs: Number of outputs of the Deep Neural Network.\n                - numberOfNeurons: Number of neurons per layer in the Deep Neural Network.\n                - dropout: Droupout probability value (handling of overfitting).\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Call the constructor of the parent class (Pytorch torch.nn.Module)\n        super(DQN, self).__init__()\n\n        # Definition of some Fully Connected layers\n        self.fc1 = nn.Linear(numberOfInputs, numberOfNeurons)\n        self.fc2 = nn.Linear(numberOfNeurons, numberOfNeurons)\n        self.fc3 = nn.Linear(numberOfNeurons, numberOfNeurons)\n        self.fc4 = nn.Linear(numberOfNeurons, numberOfNeurons)\n        self.fc5 = nn.Linear(numberOfNeurons, numberOfOutputs)\n\n        # Definition of some Batch Normalization layers\n        self.bn1 = nn.BatchNorm1d(numberOfNeurons)\n        self.bn2 = nn.BatchNorm1d(numberOfNeurons)\n        self.bn3 = nn.BatchNorm1d(numberOfNeurons)\n        self.bn4 = nn.BatchNorm1d(numberOfNeurons)\n\n        # Definition of some Dropout layers.\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n        self.dropout4 = nn.Dropout(dropout)\n\n        # Xavier initialization for the entire neural network\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\n        torch.nn.init.xavier_uniform_(self.fc3.weight)\n        torch.nn.init.xavier_uniform_(self.fc4.weight)\n        torch.nn.init.xavier_uniform_(self.fc5.weight)\n\n\n    def forward(self, input):\n        \"\"\"\n        GOAL: Implementing the forward pass of the Deep Neural Network.\n\n        INPUTS: - input: Input of the Deep Neural Network.\n\n        OUTPUTS: - output: Output of the Deep Neural Network.\n        \"\"\"\n\n        x = self.dropout1(F.leaky_relu(self.bn1(self.fc1(input))))\n        x = self.dropout2(F.leaky_relu(self.bn2(self.fc2(x))))\n        x = self.dropout3(F.leaky_relu(self.bn3(self.fc3(x))))\n        x = self.dropout4(F.leaky_relu(self.bn4(self.fc4(x))))\n        output = self.fc5(x)\n        return output\n\n\n###############################################################################\n################################ Class TDQN ###################################\n###############################################################################\n\nclass TDQN:\n    \"\"\"\n    GOAL: Implementing an intelligent trading agent based on the DQN\n          Reinforcement Learning algorithm.\n\n    VARIABLES:  - device: Hardware specification (CPU or GPU).\n                - gamma: Discount factor of the DQN algorithm.\n                - learningRate: Learning rate of the ADAM optimizer.\n                - capacity: Capacity of the experience replay memory.\n                - batchSize: Size of the batch to sample from the replay memory.\n                - targetNetworkUpdate: Frequency at which the target neural\n                                       network is updated.\n                - observationSpace: Size of the RL observation space.\n                - actionSpace: Size of the RL action space.\n                - policyNetwork: Deep Neural Network representing the RL policy.\n                - targetNetwork: Deep Neural Network representing a target\n                                 for the policy Deep Neural Network.\n                - optimizer: Deep Neural Network optimizer (ADAM).\n                - replayMemory: Experience replay memory.\n                - epsilonValue: Value of the Epsilon, from the\n                                Epsilon-Greedy exploration technique.\n                - iterations: Counter of the number of iterations.\n\n    METHODS:    - __init__: Initialization of the RL trading agent, by setting up\n                            many variables and parameters.\n                - getNormalizationCoefficients: Retrieve the coefficients required\n                                                for the normalization of input data.\n                - processState: Process the RL state received.\n                - processReward: Clipping of the RL reward received.\n                - updateTargetNetwork: Update the target network, by transfering\n                                       the policy network parameters.\n                - chooseAction: Choose a valid action based on the current state\n                                observed, according to the RL policy learned.\n                - chooseActionEpsilonGreedy: Choose a valid action based on the\n                                             current state observed, according to\n                                             the RL policy learned, following the\n                                             Epsilon Greedy exploration mechanism.\n                - learn: Sample a batch of experiences and learn from that info.\n                - training: Train the trading DQN agent by interacting with its\n                            trading environment.\n                - testing: Test the DQN agent trading policy on a new trading environment.\n                - plotExpectedPerformance: Plot the expected performance of the intelligent\n                                   DRL trading agent.\n                - saveModel: Save the RL policy model.\n                - loadModel: Load the RL policy model.\n                - plotTraining: Plot the training results (score evolution, etc.).\n                - plotEpsilonAnnealing: Plot the annealing behaviour of the Epsilon\n                                     (Epsilon-Greedy exploration technique).\n    \"\"\"\n\n    def __init__(self, observationSpace, actionSpace, numberOfNeurons=numberOfNeurons, dropout=dropout,\n                 gamma=gamma, learningRate=learningRate, targetNetworkUpdate=targetNetworkUpdate,\n                 epsilonStart=epsilonStart, epsilonEnd=epsilonEnd, epsilonDecay=epsilonDecay,\n                 capacity=capacity, batchSize=batchSize):\n        \"\"\"\n        GOAL: Initializing the RL agent based on the DQN Reinforcement Learning\n              algorithm, by setting up the DQN algorithm parameters as well as\n              the DQN Deep Neural Network.\n\n        INPUTS: - observationSpace: Size of the RL observation space.\n                - actionSpace: Size of the RL action space.\n                - numberOfNeurons: Number of neurons per layer in the Deep Neural Network.\n                - dropout: Droupout probability value (handling of overfitting).\n                - gamma: Discount factor of the DQN algorithm.\n                - learningRate: Learning rate of the ADAM optimizer.\n                - targetNetworkUpdate: Update frequency of the target network.\n                - epsilonStart: Initial (maximum) value of Epsilon, from the\n                                Epsilon-Greedy exploration technique.\n                - epsilonEnd: Final (minimum) value of Epsilon, from the\n                                Epsilon-Greedy exploration technique.\n                - epsilonDecay: Decay factor (exponential) of Epsilon, from the\n                                Epsilon-Greedy exploration technique.\n                - capacity: Capacity of the Experience Replay memory.\n                - batchSize: Size of the batch to sample from the replay memory.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Initialise the random function with a new random seed\n        random.seed(0)\n\n        # Check availability of CUDA for the hardware (CPU or GPU)\n        self.device = torch.device('cuda:'+str(GPUNumber) if torch.cuda.is_available() else 'cpu')\n\n        # Set the general parameters of the DQN algorithm\n        self.gamma = gamma\n        self.learningRate = learningRate\n        self.targetNetworkUpdate = targetNetworkUpdate\n\n        # Set the Experience Replay mechnism\n        self.capacity = capacity\n        self.batchSize = batchSize\n        self.replayMemory = ReplayMemory(capacity)\n\n        # Set both the observation and action spaces\n        self.observationSpace = observationSpace\n        self.actionSpace = actionSpace\n\n        # Set the two Deep Neural Networks of the DQN algorithm (policy and target)\n        self.policyNetwork = DQN(observationSpace, actionSpace, numberOfNeurons, dropout).to(self.device)\n        self.targetNetwork = DQN(observationSpace, actionSpace, numberOfNeurons, dropout).to(self.device)\n        self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n        self.policyNetwork.eval()\n        self.targetNetwork.eval()\n\n        # Set the Deep Learning optimizer\n        self.optimizer = optim.Adam(self.policyNetwork.parameters(), lr=learningRate, weight_decay=L2Factor)\n\n        # Set the Epsilon-Greedy exploration technique\n        self.epsilonValue = lambda iteration: epsilonEnd + (epsilonStart - epsilonEnd) * math.exp(-1 * iteration / epsilonDecay)\n\n        # Initialization of the iterations counter\n        self.iterations = 0\n\n        # Initialization of the tensorboard writer\n        self.writer = SummaryWriter('runs/' + datetime.datetime.now().strftime(\"%d/%m/%Y-%H:%M:%S\"))\n\n\n    def getNormalizationCoefficients(self, tradingEnv):\n        \"\"\"\n        GOAL: Retrieve the coefficients required for the normalization\n              of input data.\n\n        INPUTS: - tradingEnv: RL trading environement to process.\n\n        OUTPUTS: - coefficients: Normalization coefficients.\n        \"\"\"\n\n        # Retrieve the available trading data\n        tradingData = tradingEnv.data\n        closePrices = tradingData['Close'].tolist()\n        lowPrices = tradingData['Low'].tolist()\n        highPrices = tradingData['High'].tolist()\n        volumes = tradingData['Volume'].tolist()\n\n        # Retrieve the coefficients required for the normalization\n        coefficients = []\n        margin = 1\n        # 1. Close price => returns (absolute) => maximum value (absolute)\n        returns = [abs((closePrices[i]-closePrices[i-1])/closePrices[i-1]) for i in range(1, len(closePrices))]\n        coeffs = (0, np.max(returns)*margin)\n        coefficients.append(coeffs)\n        # 2. Low/High prices => Delta prices => maximum value\n        deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(len(lowPrices))]\n        coeffs = (0, np.max(deltaPrice)*margin)\n        coefficients.append(coeffs)\n        # 3. Close/Low/High prices => Close price position => no normalization required\n        coeffs = (0, 1)\n        coefficients.append(coeffs)\n        # 4. Volumes => minimum and maximum values\n        coeffs = (np.min(volumes)/margin, np.max(volumes)*margin)\n        coefficients.append(coeffs)\n\n        return coefficients\n\n\n    def processState(self, state, coefficients):\n        \"\"\"\n        GOAL: Process the RL state returned by the environment\n              (appropriate format and normalization).\n\n        INPUTS: - state: RL state returned by the environment.\n\n        OUTPUTS: - state: Processed RL state.\n        \"\"\"\n\n        # Normalization of the RL state\n        closePrices = [state[0][i] for i in range(len(state[0]))]\n        lowPrices = [state[1][i] for i in range(len(state[1]))]\n        highPrices = [state[2][i] for i in range(len(state[2]))]\n        volumes = [state[3][i] for i in range(len(state[3]))]\n\n        # 1. Close price => returns => MinMax normalization\n        returns = [(closePrices[i]-closePrices[i-1])/closePrices[i-1] for i in range(1, len(closePrices))]\n        if coefficients[0][0] != coefficients[0][1]:\n            state[0] = [((x - coefficients[0][0])/(coefficients[0][1] - coefficients[0][0])) for x in returns]\n        else:\n            state[0] = [0 for x in returns]\n        # 2. Low/High prices => Delta prices => MinMax normalization\n        deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(1, len(lowPrices))]\n        if coefficients[1][0] != coefficients[1][1]:\n            state[1] = [((x - coefficients[1][0])/(coefficients[1][1] - coefficients[1][0])) for x in deltaPrice]\n        else:\n            state[1] = [0 for x in deltaPrice]\n        # 3. Close/Low/High prices => Close price position => No normalization required\n        closePricePosition = []\n        for i in range(1, len(closePrices)):\n            deltaPrice = abs(highPrices[i]-lowPrices[i])\n            if deltaPrice != 0:\n                item = abs(closePrices[i]-lowPrices[i])/deltaPrice\n            else:\n                item = 0.5\n            closePricePosition.append(item)\n        if coefficients[2][0] != coefficients[2][1]:\n            state[2] = [((x - coefficients[2][0])/(coefficients[2][1] - coefficients[2][0])) for x in closePricePosition]\n        else:\n            state[2] = [0.5 for x in closePricePosition]\n        # 4. Volumes => MinMax normalization\n        volumes = [volumes[i] for i in range(1, len(volumes))]\n        if coefficients[3][0] != coefficients[3][1]:\n            state[3] = [((x - coefficients[3][0])/(coefficients[3][1] - coefficients[3][0])) for x in volumes]\n        else:\n            state[3] = [0 for x in volumes]\n\n        # Process the state structure to obtain the appropriate format\n        state = [item for sublist in state for item in sublist]\n\n        return state\n\n\n    def processReward(self, reward):\n        \"\"\"\n        GOAL: Process the RL reward returned by the environment by clipping\n              its value. Such technique has been shown to improve the stability\n              the DQN algorithm.\n\n        INPUTS: - reward: RL reward returned by the environment.\n\n        OUTPUTS: - reward: Process RL reward.\n        \"\"\"\n\n        return np.clip(reward, -rewardClipping, rewardClipping)\n\n\n    def updateTargetNetwork(self):\n        \"\"\"\n        GOAL: Taking into account the update frequency (parameter), update the\n              target Deep Neural Network by copying the policy Deep Neural Network\n              parameters (weights, bias, etc.).\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Check if an update is required (update frequency)\n        if(self.iterations % targetNetworkUpdate == 0):\n            # Transfer the DNN parameters (policy network -> target network)\n            self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n\n\n    def chooseAction(self, state):\n        \"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed.\n\n        INPUTS: - state: RL state returned by the environment.\n\n        OUTPUTS: - action: RL action chosen from the action space.\n                 - Q: State-action value function associated.\n                 - QValues: Array of all the Qvalues outputted by the\n                            Deep Neural Network.\n        \"\"\"\n\n        # Choose the best action based on the RL policy\n        with torch.no_grad():\n            tensorState = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)\n            QValues = self.policyNetwork(tensorState).squeeze(0)\n            Q, action = QValues.max(0)\n            action = action.item()\n            Q = Q.item()\n            QValues = QValues.cpu().numpy()\n            return action, Q, QValues\n\n\n    def chooseActionEpsilonGreedy(self, state, previousAction):\n        \"\"\"\n        GOAL: Choose a valid RL action from the action space according to the\n              RL policy as well as the current RL state observed, following the\n              Epsilon Greedy exploration mechanism.\n\n        INPUTS: - state: RL state returned by the environment.\n                - previousAction: Previous RL action executed by the agent.\n\n        OUTPUTS: - action: RL action chosen from the action space.\n                 - Q: State-action value function associated.\n                 - QValues: Array of all the Qvalues outputted by the\n                            Deep Neural Network.\n        \"\"\"\n\n        # EXPLOITATION -> RL policy\n        if(random.random() > self.epsilonValue(self.iterations)):\n            # Sticky action (RL generalization mechanism)\n            if(random.random() > alpha):\n                action, Q, QValues = self.chooseAction(state)\n            else:\n                action = previousAction\n                Q = 0\n                QValues = [0, 0]\n\n        # EXPLORATION -> Random\n        else:\n            action = random.randrange(self.actionSpace)\n            Q = 0\n            QValues = [0, 0]\n\n        # Increment the iterations counter (for Epsilon Greedy)\n        self.iterations += 1\n\n        return action, Q, QValues\n\n\n    def learning(self, batchSize=batchSize):\n        \"\"\"\n        GOAL: Sample a batch of past experiences and learn from it\n              by updating the Reinforcement Learning policy.\n\n        INPUTS: batchSize: Size of the batch to sample from the replay memory.\n\n        OUTPUTS: /\n        \"\"\"\n\n        # Check that the replay memory is filled enough\n        if (len(self.replayMemory) >= batchSize):\n\n            # Set the Deep Neural Network in training mode\n            self.policyNetwork.train()\n\n            # Sample a batch of experiences from the replay memory\n            state, action, reward, nextState, done = self.replayMemory.sample(batchSize)\n\n            # Initialization of Pytorch tensors for the RL experience elements\n            state = torch.tensor(state, dtype=torch.float, device=self.device)\n            action = torch.tensor(action, dtype=torch.long, device=self.device)\n            reward = torch.tensor(reward, dtype=torch.float, device=self.device)\n            nextState = torch.tensor(nextState, dtype=torch.float, device=self.device)\n            done = torch.tensor(done, dtype=torch.float, device=self.device)\n\n            # Compute the current Q values returned by the policy network\n            currentQValues = self.policyNetwork(state).gather(1, action.unsqueeze(1)).squeeze(1)\n\n            # Compute the next Q values returned by the target network\n            with torch.no_grad():\n                nextActions = torch.max(self.policyNetwork(nextState), 1)[1]\n                nextQValues = self.targetNetwork(nextState).gather(1, nextActions.unsqueeze(1)).squeeze(1)\n                expectedQValues = reward + gamma * nextQValues * (1 - done)\n\n            # Compute the Huber loss\n            loss = F.smooth_l1_loss(currentQValues, expectedQValues)\n\n            # Computation of the gradients\n            self.optimizer.zero_grad()\n            loss.backward()\n\n            # Gradient Clipping\n            torch.nn.utils.clip_grad_norm_(self.policyNetwork.parameters(), gradientClipping)\n\n            # Perform the Deep Neural Network optimization\n            self.optimizer.step()\n\n            # If required, update the target deep neural network (update frequency)\n            self.updateTargetNetwork()\n\n            # Set back the Deep Neural Network in evaluation mode\n            self.policyNetwork.eval()\n\n\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        \"\"\"\n        GOAL: Train the RL trading agent by interacting with its trading environment.\n\n        INPUTS: - trainingEnv: Training RL environment (known).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase (e.g. the number\n                                      of episodes).\n                - verbose: Enable the printing of a training feedback.\n                - rendering: Enable the training environment rendering.\n                - plotTraining: Enable the plotting of the training results.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - trainingEnv: Training RL environment.\n        \"\"\"\n\n        \"\"\"\n        # Compute and plot the expected performance of the trading policy\n        trainingEnv = self.plotExpectedPerformance(trainingEnv, trainingParameters, iterations=50)\n        return trainingEnv\n        \"\"\"\n\n        # Apply data augmentation techniques to improve the training set\n        dataAugmentation = DataAugmentation()\n        trainingEnvList = dataAugmentation.generate(trainingEnv)\n\n        # Initialization of some variables tracking the training and testing performances\n        if plotTraining:\n            # Training performance\n            performanceTrain = []\n            score = np.zeros((len(trainingEnvList), trainingParameters[0]))\n            # Testing performance\n            marketSymbol = trainingEnv.marketSymbol\n            startingDate = trainingEnv.endingDate\n            endingDate = '2020-1-1'\n            money = trainingEnv.data['Money'][0]\n            stateLength = trainingEnv.stateLength\n            transactionCosts = trainingEnv.transactionCosts\n            testingEnv = TradingEnv(marketSymbol, startingDate, endingDate, money, stateLength, transactionCosts)\n            performanceTest = []\n\n        try:\n            # If required, print the training progression\n            if verbose:\n                print(\"Training progression (hardware selected => \" + str(self.device) + \"):\")\n\n            # Training phase for the number of episodes specified as parameter\n            for episode in tqdm(range(trainingParameters[0]), disable=not(verbose)):\n\n                # For each episode, train on the entire set of training environments\n                for i in range(len(trainingEnvList)):\n\n                    # Set the initial RL variables\n                    coefficients = self.getNormalizationCoefficients(trainingEnvList[i])\n                    trainingEnvList[i].reset()\n                    startingPoint = random.randrange(len(trainingEnvList[i].data.index))\n                    trainingEnvList[i].setStartingPoint(startingPoint)\n                    state = self.processState(trainingEnvList[i].state, coefficients)\n                    previousAction = 0\n                    done = 0\n                    stepsCounter = 0\n\n                    # Set the performance tracking veriables\n                    if plotTraining:\n                        totalReward = 0\n\n                    # Interact with the training environment until termination\n                    while done == 0:\n\n                        # Choose an action according to the RL policy and the current RL state\n                        action, _, _ = self.chooseActionEpsilonGreedy(state, previousAction)\n\n                        # Interact with the environment with the chosen action\n                        nextState, reward, done, info = trainingEnvList[i].step(action)\n\n                        # Process the RL variables retrieved and insert this new experience into the Experience Replay memory\n                        reward = self.processReward(reward)\n                        nextState = self.processState(nextState, coefficients)\n                        self.replayMemory.push(state, action, reward, nextState, done)\n\n                        # Trick for better exploration\n                        otherAction = int(not bool(action))\n                        otherReward = self.processReward(info['Reward'])\n                        otherNextState = self.processState(info['State'], coefficients)\n                        otherDone = info['Done']\n                        self.replayMemory.push(state, otherAction, otherReward, otherNextState, otherDone)\n\n                        # Execute the DQN learning procedure\n                        stepsCounter += 1\n                        if stepsCounter == learningUpdatePeriod:\n                            self.learning()\n                            stepsCounter = 0\n\n                        # Update the RL state\n                        state = nextState\n                        previousAction = action\n\n                        # Continuous tracking of the training performance\n                        if plotTraining:\n                            totalReward += reward\n\n                    # Store the current training results\n                    if plotTraining:\n                        score[i][episode] = totalReward\n\n                # Compute the current performance on both the training and testing sets\n                if plotTraining:\n                    # Training set performance\n                    trainingEnv = self.testing(trainingEnv, trainingEnv)\n                    analyser = PerformanceEstimator(trainingEnv.data)\n                    performance = analyser.computeSharpeRatio()\n                    performanceTrain.append(performance)\n                    self.writer.add_scalar('Training performance (Sharpe Ratio)', performance, episode)\n                    trainingEnv.reset()\n                    # Testing set performance\n                    testingEnv = self.testing(trainingEnv, testingEnv)\n                    analyser = PerformanceEstimator(testingEnv.data)\n                    performance = analyser.computeSharpeRatio()\n                    performanceTest.append(performance)\n                    self.writer.add_scalar('Testing performance (Sharpe Ratio)', performance, episode)\n                    testingEnv.reset()\n\n        except KeyboardInterrupt:\n            print()\n            print(\"WARNING: Training prematurely interrupted...\")\n            print()\n            self.policyNetwork.eval()\n\n        # Assess the algorithm performance on the training trading environment\n        trainingEnv = self.testing(trainingEnv, trainingEnv)\n\n        # If required, show the rendering of the trading environment\n        if rendering:\n            trainingEnv.render()\n\n        # If required, plot the training results\n        if plotTraining:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, ylabel='Performance (Sharpe Ratio)', xlabel='Episode')\n            ax.plot(performanceTrain)\n            ax.plot(performanceTest)\n            ax.legend([\"Training\", \"Testing\"])\n            plt.savefig(''.join(['Figures/', str(marketSymbol), '_TrainingTestingPerformance', '.png']))\n            #plt.show()\n            for i in range(len(trainingEnvList)):\n                self.plotTraining(score[i][:episode], marketSymbol)\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('TDQN')\n\n        # Closing of the tensorboard writer\n        self.writer.close()\n\n        return trainingEnv\n\n\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        \"\"\"\n        GOAL: Test the RL agent trading policy on a new trading environment\n              in order to assess the trading strategy performance.\n\n        INPUTS: - trainingEnv: Training RL environment (known).\n                - testingEnv: Unknown trading RL environment.\n                - rendering: Enable the trading environment rendering.\n                - showPerformance: Enable the printing of a table summarizing\n                                   the trading strategy performance.\n\n        OUTPUTS: - testingEnv: Trading environment backtested.\n        \"\"\"\n\n        # Apply data augmentation techniques to process the testing set\n        dataAugmentation = DataAugmentation()\n        testingEnvSmoothed = dataAugmentation.lowPassFilter(testingEnv, filterOrder)\n        trainingEnv = dataAugmentation.lowPassFilter(trainingEnv, filterOrder)\n\n        # Initialization of some RL variables\n        coefficients = self.getNormalizationCoefficients(trainingEnv)\n        state = self.processState(testingEnvSmoothed.reset(), coefficients)\n        testingEnv.reset()\n        QValues0 = []\n        QValues1 = []\n        done = 0\n\n        # Interact with the environment until the episode termination\n        while done == 0:\n\n            # Choose an action according to the RL policy and the current RL state\n            action, _, QValues = self.chooseAction(state)\n\n            # Interact with the environment with the chosen action\n            nextState, _, done, _ = testingEnvSmoothed.step(action)\n            testingEnv.step(action)\n\n            # Update the new state\n            state = self.processState(nextState, coefficients)\n\n            # Storing of the Q values\n            QValues0.append(QValues[0])\n            QValues1.append(QValues[1])\n\n        # If required, show the rendering of the trading environment\n        if rendering:\n            testingEnv.render()\n            self.plotQValues(QValues0, QValues1, testingEnv.marketSymbol)\n\n        # If required, print the strategy performance in a table\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('TDQN')\n\n        return testingEnv\n\n\n    def plotTraining(self, score, marketSymbol):\n        \"\"\"\n        GOAL: Plot the training phase results\n              (score, sum of rewards).\n\n        INPUTS: - score: Array of total episode rewards.\n                - marketSymbol: Stock market trading symbol.\n\n        OUTPUTS: /\n        \"\"\"\n\n        fig = plt.figure()\n        ax1 = fig.add_subplot(111, ylabel='Total reward collected', xlabel='Episode')\n        ax1.plot(score)\n        plt.savefig(''.join(['Figures/', str(marketSymbol), 'TrainingResults', '.png']))\n        #plt.show()\n\n\n    def plotQValues(self, QValues0, QValues1, marketSymbol):\n        \"\"\"\n        Plot sequentially the Q values related to both actions.\n\n        :param: - QValues0: Array of Q values linked to action 0.\n                - QValues1: Array of Q values linked to action 1.\n                - marketSymbol: Stock market trading symbol.\n\n        :return: /\n        \"\"\"\n\n        fig = plt.figure()\n        ax1 = fig.add_subplot(111, ylabel='Q values', xlabel='Time')\n        ax1.plot(QValues0)\n        ax1.plot(QValues1)\n        ax1.legend(['Short', 'Long'])\n        plt.savefig(''.join(['Figures/', str(marketSymbol), '_QValues', '.png']))\n        #plt.show()\n\n\n    def plotExpectedPerformance(self, trainingEnv, trainingParameters=[], iterations=10):\n        \"\"\"\n        GOAL: Plot the expected performance of the intelligent DRL trading agent.\n\n        INPUTS: - trainingEnv: Training RL environment (known).\n                - trainingParameters: Additional parameters associated\n                                      with the training phase (e.g. the number\n                                      of episodes).\n                - iterations: Number of training/testing iterations to compute\n                              the expected performance.\n\n        OUTPUTS: - trainingEnv: Training RL environment.\n        \"\"\"\n\n        # Preprocessing of the training set\n        dataAugmentation = DataAugmentation()\n        trainingEnvList = dataAugmentation.generate(trainingEnv)\n\n        # Save the initial Deep Neural Network weights\n        initialWeights =  copy.deepcopy(self.policyNetwork.state_dict())\n\n        # Initialization of some variables tracking both training and testing performances\n        performanceTrain = np.zeros((trainingParameters[0], iterations))\n        performanceTest = np.zeros((trainingParameters[0], iterations))\n\n        # Initialization of the testing trading environment\n        marketSymbol = trainingEnv.marketSymbol\n        startingDate = trainingEnv.endingDate\n        endingDate = '2020-1-1'\n        money = trainingEnv.data['Money'][0]\n        stateLength = trainingEnv.stateLength\n        transactionCosts = trainingEnv.transactionCosts\n        testingEnv = TradingEnv(marketSymbol, startingDate, endingDate, money, stateLength, transactionCosts)\n\n        # Print the hardware selected for the training of the Deep Neural Network (either CPU or GPU)\n        print(\"Hardware selected for training: \" + str(self.device))\n\n        try:\n\n            # Apply the training/testing procedure for the number of iterations specified\n            for iteration in range(iterations):\n\n                # Print the progression\n                print(''.join([\"Expected performance evaluation progression: \", str(iteration+1), \"/\", str(iterations)]))\n\n                # Training phase for the number of episodes specified as parameter\n                for episode in tqdm(range(trainingParameters[0])):\n\n                    # For each episode, train on the entire set of training environments\n                    for i in range(len(trainingEnvList)):\n\n                        # Set the initial RL variables\n                        coefficients = self.getNormalizationCoefficients(trainingEnvList[i])\n                        trainingEnvList[i].reset()\n                        startingPoint = random.randrange(len(trainingEnvList[i].data.index))\n                        trainingEnvList[i].setStartingPoint(startingPoint)\n                        state = self.processState(trainingEnvList[i].state, coefficients)\n                        previousAction = 0\n                        done = 0\n                        stepsCounter = 0\n\n                        # Interact with the training environment until termination\n                        while done == 0:\n\n                            # Choose an action according to the RL policy and the current RL state\n                            action, _, _ = self.chooseActionEpsilonGreedy(state, previousAction)\n\n                            # Interact with the environment with the chosen action\n                            nextState, reward, done, info = trainingEnvList[i].step(action)\n\n                            # Process the RL variables retrieved and insert this new experience into the Experience Replay memory\n                            reward = self.processReward(reward)\n                            nextState = self.processState(nextState, coefficients)\n                            self.replayMemory.push(state, action, reward, nextState, done)\n\n                            # Trick for better exploration\n                            otherAction = int(not bool(action))\n                            otherReward = self.processReward(info['Reward'])\n                            otherDone = info['Done']\n                            otherNextState = self.processState(info['State'], coefficients)\n                            self.replayMemory.push(state, otherAction, otherReward, otherNextState, otherDone)\n\n                            # Execute the DQN learning procedure\n                            stepsCounter += 1\n                            if stepsCounter == learningUpdatePeriod:\n                                self.learning()\n                                stepsCounter = 0\n\n                            # Update the RL state\n                            state = nextState\n                            previousAction = action\n\n                    # Compute both training and testing  current performances\n                    trainingEnv = self.testing(trainingEnv, trainingEnv)\n                    analyser = PerformanceEstimator(trainingEnv.data)\n                    performanceTrain[episode][iteration] = analyser.computeSharpeRatio()\n                    self.writer.add_scalar('Training performance (Sharpe Ratio)', performanceTrain[episode][iteration], episode)\n                    testingEnv = self.testing(trainingEnv, testingEnv)\n                    analyser = PerformanceEstimator(testingEnv.data)\n                    performanceTest[episode][iteration] = analyser.computeSharpeRatio()\n                    self.writer.add_scalar('Testing performance (Sharpe Ratio)', performanceTest[episode][iteration], episode)\n\n                # Restore the initial state of the intelligent RL agent\n                if iteration < (iterations-1):\n                    trainingEnv.reset()\n                    testingEnv.reset()\n                    self.policyNetwork.load_state_dict(initialWeights)\n                    self.targetNetwork.load_state_dict(initialWeights)\n                    self.optimizer = optim.Adam(self.policyNetwork.parameters(), lr=learningRate, weight_decay=L2Factor)\n                    self.replayMemory.reset()\n                    self.iterations = 0\n                    stepsCounter = 0\n\n            iteration += 1\n\n        except KeyboardInterrupt:\n            print()\n            print(\"WARNING: Expected performance evaluation prematurely interrupted...\")\n            print()\n            self.policyNetwork.eval()\n\n        # Compute the expected performance of the intelligent DRL trading agent\n        expectedPerformanceTrain = []\n        expectedPerformanceTest = []\n        stdPerformanceTrain = []\n        stdPerformanceTest = []\n        for episode in range(trainingParameters[0]):\n            expectedPerformanceTrain.append(np.mean(performanceTrain[episode][:iteration]))\n            expectedPerformanceTest.append(np.mean(performanceTest[episode][:iteration]))\n            stdPerformanceTrain.append(np.std(performanceTrain[episode][:iteration]))\n            stdPerformanceTest.append(np.std(performanceTest[episode][:iteration]))\n        expectedPerformanceTrain = np.array(expectedPerformanceTrain)\n        expectedPerformanceTest = np.array(expectedPerformanceTest)\n        stdPerformanceTrain = np.array(stdPerformanceTrain)\n        stdPerformanceTest = np.array(stdPerformanceTest)\n\n        # Plot each training/testing iteration performance of the intelligent DRL trading agent\n        for i in range(iteration):\n            fig = plt.figure()\n            ax = fig.add_subplot(111, ylabel='Performance (Sharpe Ratio)', xlabel='Episode')\n            ax.plot([performanceTrain[e][i] for e in range(trainingParameters[0])])\n            ax.plot([performanceTest[e][i] for e in range(trainingParameters[0])])\n            ax.legend([\"Training\", \"Testing\"])\n            plt.savefig(''.join(['Figures/', str(marketSymbol), '_TrainingTestingPerformance', str(i+1), '.png']))\n            #plt.show()\n\n        # Plot the expected performance of the intelligent DRL trading agent\n        fig = plt.figure()\n        ax = fig.add_subplot(111, ylabel='Performance (Sharpe Ratio)', xlabel='Episode')\n        ax.plot(expectedPerformanceTrain)\n        ax.plot(expectedPerformanceTest)\n        ax.fill_between(range(len(expectedPerformanceTrain)), expectedPerformanceTrain-stdPerformanceTrain, expectedPerformanceTrain+stdPerformanceTrain, alpha=0.25)\n        ax.fill_between(range(len(expectedPerformanceTest)), expectedPerformanceTest-stdPerformanceTest, expectedPerformanceTest+stdPerformanceTest, alpha=0.25)\n        ax.legend([\"Training\", \"Testing\"])\n        plt.savefig(''.join(['Figures/', str(marketSymbol), '_TrainingTestingExpectedPerformance', '.png']))\n        #plt.show()\n\n        # Closing of the tensorboard writer\n        self.writer.close()\n\n        return trainingEnv\n\n\n    def saveModel(self, fileName):\n        \"\"\"\n        GOAL: Save the RL policy, which is the policy Deep Neural Network.\n\n        INPUTS: - fileName: Name of the file.\n\n        OUTPUTS: /\n        \"\"\"\n\n        torch.save(self.policyNetwork.state_dict(), fileName)\n\n\n    def loadModel(self, fileName):\n        \"\"\"\n        GOAL: Load a RL policy, which is the policy Deep Neural Network.\n\n        INPUTS: - fileName: Name of the file.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.policyNetwork.load_state_dict(torch.load(fileName, map_location=self.device))\n        self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n\n\n    def plotEpsilonAnnealing(self):\n        \"\"\"\n        GOAL: Plot the annealing behaviour of the Epsilon variable\n              (Epsilon-Greedy exploration technique).\n\n        INPUTS: /\n\n        OUTPUTS: /\n        \"\"\"\n\n        plt.figure()\n        plt.plot([self.epsilonValue(i) for i in range(10*epsilonDecay)])\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Epsilon value\")\n        plt.savefig(''.join(['Figures/', 'EpsilonAnnealing', '.png']))\n        #plt.show()\n\n\nimport numpy as np\n\nfrom tabulate import tabulate\nfrom matplotlib import pyplot as plt\n\n\n\n###############################################################################\n######################### Class PerformanceEstimator ##########################\n###############################################################################\n\nclass PerformanceEstimator:\n    \"\"\"\n    GOAL: Accurately estimating the performance of a trading strategy, by\n          computing many different performance indicators.\n\n    VARIABLES: - data: Trading activity data from the trading environment.\n               - PnL: Profit & Loss (performance indicator).\n               - annualizedReturn: Annualized Return (performance indicator).\n               - annualizedVolatily: Annualized Volatility (performance indicator).\n               - profitability: Profitability (performance indicator).\n               - averageProfitLossRatio: Average Profit/Loss Ratio (performance indicator).\n               - sharpeRatio: Sharpe Ratio (performance indicator).\n               - sortinoRatio: Sortino Ratio (performance indicator).\n               - maxDD: Maximum Drawdown (performance indicator).\n               - maxDDD: Maximum Drawdown Duration (performance indicator).\n               - skewness: Skewness of the returns (performance indicator).\n\n    METHODS:   -  __init__: Object constructor initializing some class variables.\n               - computePnL: Compute the P&L.\n               - computeAnnualizedReturn: Compute the Annualized Return.\n               - computeAnnualizedVolatility: Compute the Annualized Volatility.\n               - computeProfitability: Computate both the Profitability and the Average Profit/Loss Ratio.\n               - computeSharpeRatio: Compute the Sharpe Ratio.\n               - computeSortinoRatio: Compute the Sortino Ratio.\n               - computeMaxDrawdown: Compute both the Maximum Drawdown and Maximum Drawdown Duration.\n               - computeSkewness: Compute the Skewness of the returns.\n               - computePerformance: Compute all the performance indicators.\n               - displayPerformance: Display the entire set of performance indicators in a table.\n    \"\"\"\n\n    def __init__(self, tradingData):\n        \"\"\"\n        GOAL: Object constructor initializing the class variables.\n\n        INPUTS: - tradingData: Trading data from the trading strategy execution.\n\n        OUTPUTS: /\n        \"\"\"\n\n        self.data = tradingData\n\n\n    def computePnL(self):\n        \"\"\"\n        GOAL: Compute the Profit & Loss (P&L) performance indicator, which\n              quantifies the money gained or lost during the trading activity.\n\n        INPUTS: /\n\n        OUTPUTS:    - PnL: Profit or loss (P&L) performance indicator.\n        \"\"\"\n\n        # Compute the PnL\n        self.PnL = self.data[\"Money\"][-1] - self.data[\"Money\"][0]\n        return self.PnL\n\n\n    def computeAnnualizedReturn(self):\n        \"\"\"\n        GOAL: Compute the yearly average profit or loss (in %), called\n              the Annualized Return performance indicator.\n\n        INPUTS: /\n\n        OUTPUTS:    - annualizedReturn: Annualized Return performance indicator.\n        \"\"\"\n\n        # Compute the cumulative return over the entire trading horizon\n        cumulativeReturn = self.data['Returns'].cumsum()\n        cumulativeReturn = cumulativeReturn[-1]\n\n        # Compute the time elapsed (in days)\n        start = self.data.index[0].to_pydatetime()\n        end = self.data.index[-1].to_pydatetime()\n        timeElapsed = end - start\n        timeElapsed = timeElapsed.days\n\n        # Compute the Annualized Return\n        if(cumulativeReturn > -1):\n            self.annualizedReturn = 100 * (((1 + cumulativeReturn) ** (365/timeElapsed)) - 1)\n        else:\n            self.annualizedReturn = -100\n        return self.annualizedReturn\n\n\n    def computeAnnualizedVolatility(self):\n        \"\"\"\n        GOAL: Compute the Yearly Voltility of the returns (in %), which is\n              a measurement of the risk associated with the trading activity.\n\n        INPUTS: /\n\n        OUTPUTS:    - annualizedVolatily: Annualized Volatility performance indicator.\n        \"\"\"\n\n        # Compute the Annualized Volatility (252 trading days in 1 trading year)\n        self.annualizedVolatily = 100 * np.sqrt(252) * self.data['Returns'].std()\n        return self.annualizedVolatily\n\n\n    def computeSharpeRatio(self, riskFreeRate=0):\n        \"\"\"\n        GOAL: Compute the Sharpe Ratio of the trading activity, which is one of\n              the most suited performance indicator as it balances the brute\n              performance and the risk associated with a trading activity.\n\n        INPUTS:     - riskFreeRate: Return of an investment with a risk null.\n\n        OUTPUTS:    - sharpeRatio: Sharpe Ratio performance indicator.\n        \"\"\"\n\n        # Compute the expected return\n        expectedReturn = self.data['Returns'].mean()\n\n        # Compute the returns volatility\n        volatility = self.data['Returns'].std()\n\n        # Compute the Sharpe Ratio (252 trading days in 1 year)\n        if expectedReturn != 0 and volatility != 0:\n            self.sharpeRatio = np.sqrt(252) * (expectedReturn - riskFreeRate)/volatility\n        else:\n            self.sharpeRatio = 0\n        return self.sharpeRatio\n\n\n    def computeSortinoRatio(self, riskFreeRate=0):\n        \"\"\"\n        GOAL: Compute the Sortino Ratio of the trading activity, which is similar\n              to the Sharpe Ratio but does no longer penalize positive risk.\n\n        INPUTS:     - riskFreeRate: Return of an investment with a risk null.\n\n        OUTPUTS:    - sortinoRatio: Sortino Ratio performance indicator.\n        \"\"\"\n\n        # Compute the expected return\n        expectedReturn = np.mean(self.data['Returns'])\n\n        # Compute the negative returns volatility\n        negativeReturns = [returns for returns in self.data['Returns'] if returns < 0]\n        volatility = np.std(negativeReturns)\n\n        # Compute the Sortino Ratio (252 trading days in 1 year)\n        if expectedReturn != 0 and volatility != 0:\n            self.sortinoRatio = np.sqrt(252) * (expectedReturn - riskFreeRate)/volatility\n        else:\n            self.sortinoRatio = 0\n        return self.sortinoRatio\n\n\n    def computeMaxDrawdown(self, plotting=False):\n        \"\"\"\n        GOAL: Compute both the Maximum Drawdown and the Maximum Drawdown Duration\n              performance indicators of the trading activity, which are measurements\n              of the risk associated with the trading activity.\n\n        INPUTS: - plotting: Boolean enabling the maximum drawdown plotting.\n\n        OUTPUTS:    - maxDD: Maximum Drawdown performance indicator.\n                    - maxDDD: Maximum Drawdown Duration performance indicator.\n        \"\"\"\n\n        # Compute both the Maximum Drawdown and Maximum Drawdown Duration\n        capital = self.data['Money'].values\n        through = np.argmax(np.maximum.accumulate(capital) - capital)\n        if through != 0:\n            peak = np.argmax(capital[:through])\n            self.maxDD = 100 * (capital[peak] - capital[through])/capital[peak]\n            self.maxDDD = through - peak\n        else:\n            self.maxDD = 0\n            self.maxDDD = 0\n            return self.maxDD, self.maxDDD\n\n        # Plotting of the Maximum Drawdown if required\n        if plotting:\n            plt.figure(figsize=(10, 4))\n            plt.plot(self.data['Money'], lw=2, color='Blue')\n            plt.plot([self.data.iloc[[peak]].index, self.data.iloc[[through]].index],\n                     [capital[peak], capital[through]], 'o', color='Red', markersize=5)\n            plt.xlabel('Time')\n            plt.ylabel('Price')\n            plt.savefig(''.join(['Figures/', 'MaximumDrawDown', '.png']))\n            #plt.show()\n\n        # Return of the results\n        return self.maxDD, self.maxDDD\n\n\n    def computeProfitability(self):\n        \"\"\"\n        GOAL: Compute both the percentage of trades that resulted\n              in profit (Profitability), and the ratio between the\n              average profit and the average loss (AverageProfitLossRatio).\n\n        INPUTS: /\n\n        OUTPUTS:    - profitability: Percentage of trades that resulted in profit.\n                    - averageProfitLossRatio: Ratio between the average profit\n                                              and the average loss.\n        \"\"\"\n\n        # Initialization of some variables\n        good = 0\n        bad = 0\n        profit = 0\n        loss = 0\n        index = next((i for i in range(len(self.data.index)) if self.data['Action'][i] != 0), None)\n        if index == None:\n            self.profitability = 0\n            self.averageProfitLossRatio = 0\n            return self.profitability, self.averageProfitLossRatio\n        money = self.data['Money'][index]\n\n        # Monitor the success of each trade over the entire trading horizon\n        for i in range(index+1, len(self.data.index)):\n            if(self.data['Action'][i] != 0):\n                delta = self.data['Money'][i] - money\n                money = self.data['Money'][i]\n                if(delta >= 0):\n                    good += 1\n                    profit += delta\n                else:\n                    bad += 1\n                    loss -= delta\n\n        # Special case of the termination trade\n        delta = self.data['Money'][-1] - money\n        if(delta >= 0):\n            good += 1\n            profit += delta\n        else:\n            bad += 1\n            loss -= delta\n\n        # Compute the Profitability\n        self.profitability = 100 * good/(good + bad)\n\n        # Compute the ratio average Profit/Loss\n        if(good != 0):\n            profit /= good\n        if(bad != 0):\n            loss /= bad\n        if(loss != 0):\n            self.averageProfitLossRatio = profit/loss\n        else:\n            self.averageProfitLossRatio = float('Inf')\n\n        return self.profitability, self.averageProfitLossRatio\n\n\n    def computeSkewness(self):\n        \"\"\"\n        GOAL: Compute the skewness of the returns, which is\n              a measurement of the degree of distorsion\n              from the symmetrical bell curve.\n\n        INPUTS: /\n\n        OUTPUTS:    - skewness: Skewness performance indicator.\n        \"\"\"\n\n        # Compute the Skewness of the returns\n        self.skewness = self.data[\"Returns\"].skew()\n        return self.skewness\n\n\n    def computePerformance(self):\n        \"\"\"\n        GOAL: Compute the entire set of performance indicators.\n\n        INPUTS: /\n\n        OUTPUTS:    - performanceTable: Table summarizing the performance of\n                                        a trading strategy.\n        \"\"\"\n\n        # Compute the entire set of performance indicators\n        self.computePnL()\n        self.computeAnnualizedReturn()\n        self.computeAnnualizedVolatility()\n        self.computeProfitability()\n        self.computeSharpeRatio()\n        self.computeSortinoRatio()\n        self.computeMaxDrawdown()\n        self.computeSkewness()\n\n        # Generate the performance table\n        self.performanceTable = [[\"Profit & Loss (P&L)\", \"{0:.0f}\".format(self.PnL)],\n                                 [\"Annualized Return\", \"{0:.2f}\".format(self.annualizedReturn) + '%'],\n                                 [\"Annualized Volatility\", \"{0:.2f}\".format(self.annualizedVolatily) + '%'],\n                                 [\"Sharpe Ratio\", \"{0:.3f}\".format(self.sharpeRatio)],\n                                 [\"Sortino Ratio\", \"{0:.3f}\".format(self.sortinoRatio)],\n                                 [\"Maximum Drawdown\", \"{0:.2f}\".format(self.maxDD) + '%'],\n                                 [\"Maximum Drawdown Duration\", \"{0:.0f}\".format(self.maxDDD) + ' days'],\n                                 [\"Profitability\", \"{0:.2f}\".format(self.profitability) + '%'],\n                                 [\"Ratio Average Profit/Loss\", \"{0:.3f}\".format(self.averageProfitLossRatio)],\n                                 [\"Skewness\", \"{0:.3f}\".format(self.skewness)]]\n\n        return self.performanceTable\n\n\n    def displayPerformance(self, name):\n        \"\"\"\n        GOAL: Compute and display the entire set of performance indicators\n              in a table.\n\n        INPUTS: - name: Name of the element (strategy or stock) analysed.\n\n        OUTPUTS:    - performanceTable: Table summarizing the performance of\n                                        a trading activity.\n        \"\"\"\n\n        # Generation of the performance table\n        self.computePerformance()\n\n        # Display the table in the console (Tabulate for the beauty of the print operation)\n        headers = [\"Performance Indicator\", name]\n        tabulation = tabulate(self.performanceTable, headers, tablefmt=\"fancy_grid\", stralign=\"center\")\n        print(tabulation)","metadata":{"_uuid":"7adee86b-7e62-4b5c-9805-866897cf48ce","_cell_guid":"c7aa1260-54b4-4495-b3d7-b7772d2edf04","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}