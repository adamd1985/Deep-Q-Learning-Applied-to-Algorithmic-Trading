{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import logging\nlogging.basicConfig(level=logging.INFO)\n\nimport os\nimport sys\nimport importlib\nimport pickle\nimport itertools\nimport numpy as np\nimport pandas as pd\n\nimport yfinance as yf\n\nfrom tabulate import tabulate\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nimport copy\nshiftRange = [0]\nstretchRange = [1]\nfilterRange = [5]\nnoiseRange = [0]\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport math\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom abc import ABC, abstractmethod\nimport gym\npd.options.mode.chained_assignment = None\nfrom scipy import signal\nimport pandas_datareader as pdr\nimport requests\nfrom io import StringIO\n\nimport random\nimport datetime\nfrom collections import deque\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\ngamma = 0.4\nlearningRate = 0.0001\ntargetNetworkUpdate = 1000\nlearningUpdatePeriod = 1\ncapacity = 100000\nbatchSize = 32\nexperiencesRequired = 1000\nnumberOfNeurons = 512\ndropout = 0.2\nepsilonStart = 1.0\nepsilonEnd = 0.01\nepsilonDecay = 10000\nalpha = 0.1\nfilterOrder = 5\ngradientClipping = 1\nrewardClipping = 1\nL2Factor = 0.000001\nGPUNumber = 0\n\nfictives = {\n    'Linear Upward' : 'LINEARUP',\n    'Linear Downward' : 'LINEARDOWN',\n    'Sinusoidal' : 'SINUSOIDAL',\n    'Triangle' : 'TRIANGLE',\n}\nstocks = {\n    'Dow Jones' : 'DIA',\n    'S&P 500' : 'SPY',\n    'NASDAQ 100' : 'QQQ',\n    'FTSE 100' : 'EZU',\n    'Nikkei 225' : 'EWJ',\n    'Google' : 'GOOGL',\n    'Apple' : 'AAPL',\n    'Facebook' : 'FB',\n    'Amazon' : 'AMZN',\n    'Microsoft' : 'MSFT',\n    'Twitter' : 'TWTR',\n    'Nokia' : 'NOK',\n    'Philips' : 'PHIA.AS',\n    'Siemens' : 'SIE.DE',\n    'Baidu' : 'BIDU',\n    'Alibaba' : 'BABA',\n    'Tencent' : '0700.HK',\n    'Sony' : '6758.T',\n    'JPMorgan Chase' : 'JPM',\n    'HSBC' : 'HSBC',\n    'CCB' : '0939.HK',\n    'ExxonMobil' : 'XOM',\n    'Shell' : 'RDSA.AS',\n    'PetroChina' : 'PTR',\n    'Tesla' : 'TSLA',\n    'Volkswagen' : 'VOW3.DE',\n    'Toyota' : '7203.T',\n    'Coca Cola' : 'KO',\n    'AB InBev' : 'ABI.BR',\n    'Kirin' : '2503.T'\n}\nindices = {\n    'Dow Jones' : 'DIA',\n    'S&P 500' : 'SPY',\n    'NASDAQ 100' : 'QQQ',\n    'FTSE 100' : 'EZU',\n    'Nikkei 225' : 'EWJ'\n}\ncompanies = {\n    'Google' : 'GOOGL',\n    'Apple' : 'AAPL',\n    'Facebook' : 'FB',\n    'Amazon' : 'AMZN',\n    'Microsoft' : 'MSFT',\n    'Twitter' : 'TWTR',\n    'Nokia' : 'NOK',\n    'Philips' : 'PHIA.AS',\n    'Siemens' : 'SIE.DE',\n    'Baidu' : 'BIDU',\n    'Alibaba' : 'BABA',\n    'Tencent' : '0700.HK',\n    'Sony' : '6758.T',\n    'JPMorgan Chase' : 'JPM',\n    'HSBC' : 'HSBC',\n    'CCB' : '0939.HK',\n    'ExxonMobil' : 'XOM',\n    'Shell' : 'RDSA.AS',\n    'PetroChina' : 'PTR',\n    'Tesla' : 'TSLA',\n    'Volkswagen' : 'VOW3.DE',\n    'Toyota' : '7203.T',\n    'Coca Cola' : 'KO',\n    'AB InBev' : 'ABI.BR',\n    'Kirin' : '2503.T'\n}\nmodels = {\n    'Buy and Hold' : 'BuyAndHold',\n    'Sell and Hold' : 'SellAndHold',\n    'Trend Following Moving Averages' : 'MovingAveragesTF',\n    'Mean Reversion Moving Averages' : 'MovingAveragesMR'\n}\nstrategiesAI = {\n    'TDQN' : 'TDQN'\n}\nMIN = 100\nMAX = 200\nPERIOD = 252\nsaving = False\nfictiveStocks = ('LINEARUP', 'LINEARDOWN', 'SINUSOIDAL', 'TRIANGLE')\n\nclass tradingStrategy(ABC):\n    @abstractmethod\n    def chooseAction(self, state):\n        pass\n    @abstractmethod\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        pass\n    @abstractmethod\n    def testing(self, testingEnv, trainingEnv, rendering=False, showPerformance=False):\n        pass\nclass BuyAndHold(tradingStrategy):\n    def chooseAction(self, state):\n        return 1\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        trainingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n        if verbose:\n            logging.info(\"No training is required as the simple Buy and Hold trading strategy does not involve any tunable parameters.\")\n        if rendering:\n            trainingEnv.render()\n        if plotTraining:\n            logging.info(\"No training results are available as the simple Buy and Hold trading strategy does not involve any tunable parameters.\")\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('B&H')\n        return trainingEnv\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n        if rendering:\n            testingEnv.render()\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('B&H')\n        return testingEnv\nclass SellAndHold(tradingStrategy):\n    def chooseAction(self, state):\n        return 0\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        trainingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n        if verbose:\n            logging.info(\"No training is required as the simple Sell and Hold trading strategy does not involve any tunable parameters.\")\n        if rendering:\n            trainingEnv.render()\n        if plotTraining:\n            logging.info(\"No training results are available as the simple Sell and Hold trading strategy does not involve any tunable parameters.\")\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('S&H')\n        return trainingEnv\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n        if rendering:\n            testingEnv.render()\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('S&H')\n        return testingEnv\nclass MovingAveragesTF(tradingStrategy):\n    def __init__(self, parameters=[5, 10]):\n        self.parameters = parameters\n    def setParameters(self, parameters):\n        self.parameters = parameters\n    def processState(self, state):\n        return state[0]\n    def chooseAction(self, state):\n        state = self.processState(state)\n        shortAverage = np.mean(state[-self.parameters[0]:])\n        longAverage = np.mean(state[-self.parameters[1]:])\n        if(shortAverage >= longAverage):\n            return 1\n        else:\n            return 0\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        bounds = trainingParameters[0]\n        step = trainingParameters[1]\n        dimension = math.ceil((bounds[1] - bounds[0])/step)\n        trainingEnv.reset()\n        results = np.zeros((dimension, dimension))\n        bestShort = 0\n        bestLong = 0\n        bestPerformance = -100\n        i = 0\n        j = 0\n        count = 1\n        if verbose:\n            iterations = dimension - 1\n            length = 0\n            while iterations > 0:\n                length += iterations\n                iterations -= 1\n        for shorter in range(bounds[0], bounds[1], step):\n            for longer in range(bounds[0], bounds[1], step):\n                if(shorter < longer):\n                    if(verbose):\n                        logging.info(\"\".join([\"Training progression: \", str(count), \"/\", str(length)]), end='\\r', flush=True)\n                    self.setParameters([shorter, longer])\n                    done = 0\n                    while done == 0:\n                        _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n                    performanceAnalysis = PerformanceEstimator(trainingEnv.data)\n                    performance = performanceAnalysis.computeSharpeRatio()\n                    results[i][j] = performance\n                    if(performance > bestPerformance):\n                        bestShort = shorter\n                        bestLong = longer\n                        bestPerformance = performance\n                    trainingEnv.reset()\n                    count += 1\n                j += 1\n            i += 1\n            j = 0\n        trainingEnv.reset()\n        self.setParameters([bestShort, bestLong])\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n        if rendering:\n            trainingEnv.render()\n        if plotTraining:\n            self.plotTraining(results, bounds, step, trainingEnv.marketSymbol)\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('MATF')\n        return trainingEnv\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False):\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n        if rendering:\n            testingEnv.render()\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data,)\n            analyser.displayPerformance('MATF')\n        return testingEnv\n    def plotTraining(self, results, bounds, step, marketSymbol, savePlots=False):\n        x = range(bounds[0], bounds[1], step)\n        y = range(bounds[0], bounds[1], step)\n        xx, yy = np.meshgrid(x, y, sparse=True)\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111, projection='3d')\n        ax.set_xlabel('Long Window Duration')\n        ax.set_ylabel('Short Window Duration')\n        ax.set_zlabel('Sharpe Ratio')\n        ax.plot_surface(xx, yy, results, cmap=plt.cm.get_cmap('jet'))\n        ax.view_init(45, 45)\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), '_MATFOptimization3D', '.png']))\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111,\n                             ylabel='Short Window Duration',\n                             xlabel='Long Window Duration')\n        graph = ax.imshow(results,\n                          cmap='jet',\n                          extent=(bounds[0], bounds[1], bounds[1], bounds[0]))\n        plt.colorbar(graph)\n        plt.gca().invert_yaxis()\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), '_MATFOptimization2D', '.png']))\nclass MovingAveragesMR(tradingStrategy):\n    def __init__(self, parameters=[5, 10]):\n        self.parameters = parameters\n    def setParameters(self, parameters):\n        self.parameters = parameters\n    def processState(self, state):\n        return state[0]\n    def chooseAction(self, state):\n        state = self.processState(state)\n        shortAverage = np.mean(state[-self.parameters[0]:])\n        longAverage = np.mean(state[-self.parameters[1]:])\n        if(shortAverage <= longAverage):\n            return 1\n        else:\n            return 0\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, plotTraining=False, showPerformance=False):\n        bounds = trainingParameters[0]\n        step = trainingParameters[1]\n        dimension = math.ceil((bounds[1] - bounds[0])/step)\n        trainingEnv.reset()\n        results = np.zeros((dimension, dimension))\n        bestShort = 0\n        bestLong = 0\n        bestPerformance = -100\n        i = 0\n        j = 0\n        count = 1\n        if verbose:\n            iterations = dimension - 1\n            length = 0\n            while iterations > 0:\n                length += iterations\n                iterations -= 1\n        for shorter in range(bounds[0], bounds[1], step):\n            for longer in range(bounds[0], bounds[1], step):\n                if(shorter < longer):\n                    if(verbose):\n                        logging.info(\"\".join([\"Training progression: \", str(count), \"/\", str(length)]), end='\\r', flush=True)\n                    self.setParameters([shorter, longer])\n                    done = 0\n                    while done == 0:\n                        _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n                    performanceAnalysis = PerformanceEstimator(trainingEnv.data)\n                    performance = performanceAnalysis.computeSharpeRatio()\n                    results[i][j] = performance\n                    if(performance > bestPerformance):\n                        bestShort = shorter\n                        bestLong = longer\n                        bestPerformance = performance\n                    trainingEnv.reset()\n                    count += 1\n                j += 1\n            i += 1\n            j = 0\n        trainingEnv.reset()\n        self.setParameters([bestShort, bestLong])\n        done = 0\n        while done == 0:\n            _, _, done, _ = trainingEnv.step(self.chooseAction(trainingEnv.state))\n        if rendering:\n            trainingEnv.render()\n        if plotTraining:\n            self.plotTraining(results, bounds, step, trainingEnv.marketSymbol)\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('MAMR')\n        return trainingEnv\n    def testing(self, trainingEnv, testingEnv, rendering=False, showPerformance=False, savePlots=False):\n        testingEnv.reset()\n        done = 0\n        while done == 0:\n            _, _, done, _ = testingEnv.step(self.chooseAction(testingEnv.state))\n        if rendering:\n            testingEnv.render()\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('MAMR')\n        return testingEnv\n    def plotTraining(self, results, bounds, step, marketSymbol, savePlots=False):\n        x = range(bounds[0], bounds[1], step)\n        y = range(bounds[0], bounds[1], step)\n        xx, yy = np.meshgrid(x, y, sparse=True)\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111, projection='3d')\n        ax.set_xlabel('Long Window Duration')\n        ax.set_ylabel('Short Window Duration')\n        ax.set_zlabel('Sharpe Ratio')\n        ax.plot_surface(xx, yy, results, cmap=plt.cm.get_cmap('jet'))\n        ax.view_init(45, 45)\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), '_MAMROptimization3D', '.png']))\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_subplot(111,\n                             ylabel='Short Window Duration',\n                             xlabel='Long Window Duration')\n        graph = ax.imshow(results,\n                          cmap='jet',\n                          extent=(bounds[0], bounds[1], bounds[1], bounds[0]))\n        plt.colorbar(graph)\n        plt.gca().invert_yaxis()\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), '_MAMROptimization2D', '.png']))\nclass TimeSeriesAnalyser:\n    def __init__(self, timeSeries):\n        self.timeSeries = timeSeries\n    def plotTimeSeries(self):\n        pd.plotting.register_matplotlib_converters()\n        plt.figure(figsize=(10, 4))\n        plt.plot(self.timeSeries.index, self.timeSeries.values, color='blue')\n        plt.title(\"Plot Title\")\n        plt.xlabel(\"X-axis\")\n        plt.ylabel(\"Y-axis\")\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Price\")\n        plt.show()\n    def timeSeriesDecomposition(self, model='multiplicative'):\n        decomposition = seasonal_decompose(self.timeSeries, model=model, period=5, extrapolate_trend='freq')\n        plt.rcParams.update({'figure.figsize': (16,9)})\n        decomposition.plot()\n        plt.show()\n    def stationarityAnalysis(self):\n        logging.info(\"Stationarity analysis: Augmented Dickey-Fuller test (ADF):\")\n        results = adfuller(self.timeSeries, autolag='AIC')\n        logging.info(\"ADF statistic: \" + str(results[0]))\n        logging.info(\"p-value: \" + str(results[1]))\n        logging.info('Critial values (the time series is not stationary with X% condifidence):')\n        for key, value in results[4].items():\n            logging.info(str(key) + ': ' + str(value))\n        if results[1] < 0.05:\n            logging.info(\"The ADF test affirms that the time series is stationary.\")\n        else:\n            logging.info(\"The ADF test could not affirm whether or not the time series is stationary...\")\n    def cyclicityAnalysis(self):\n        plt.rcParams.update({'figure.figsize': (16,9)})\n        pd.plotting.autocorrelation_plot(self.timeSeries)\n        plt.show()\n        _, axes = plt.subplots(2, figsize=(16, 9))\n        plot_acf(self.timeSeries, lags=21, ax=axes[0])\n        plot_pacf(self.timeSeries, lags=21, ax=axes[1])\n        plt.show()\n        _, axes = plt.subplots(1, 10, figsize=(17, 9), sharex=True, sharey=True)\n        for i, ax in enumerate(axes.flatten()[:10]):\n            pd.plotting.lag_plot(self.timeSeries, lag=i+1, ax=ax)\n            ax.set_title('Lag ' + str(i+1))\n        plt.show()\nclass DataAugmentation:\n    def shiftTimeSeries(self, tradingEnv, shiftMagnitude=0):\n        newTradingEnv = copy.deepcopy(tradingEnv)\n        if shiftMagnitude < 0:\n            minValue = np.min(tradingEnv.data['Volume'])\n            shiftMagnitude = max(-minValue, shiftMagnitude)\n        newTradingEnv.data['Volume'] += shiftMagnitude\n        return newTradingEnv\n    def streching(self, tradingEnv, factor=1):\n        newTradingEnv = copy.deepcopy(tradingEnv)\n        returns = newTradingEnv.data['Close'].pct_change() * factor\n        for i in range(1, len(newTradingEnv.data.index)):\n            newTradingEnv.data['Close'][i] = newTradingEnv.data['Close'][i-1] * (1 + returns[i])\n            newTradingEnv.data['Low'][i] = newTradingEnv.data['Close'][i] * tradingEnv.data['Low'][i]/tradingEnv.data['Close'][i]\n            newTradingEnv.data['High'][i] = newTradingEnv.data['Close'][i] * tradingEnv.data['High'][i]/tradingEnv.data['Close'][i]\n            newTradingEnv.data['Open'][i] = newTradingEnv.data['Close'][i-1]\n        return newTradingEnv\n    def noiseAddition(self, tradingEnv, stdev=1):\n        newTradingEnv = copy.deepcopy(tradingEnv)\n        for i in range(1, len(newTradingEnv.data.index)):\n            price = newTradingEnv.data['Close'][i]\n            volume = newTradingEnv.data['Volume'][i]\n            priceNoise = np.random.normal(0, stdev*(price/100))\n            volumeNoise = np.random.normal(0, stdev*(volume/100))\n            newTradingEnv.data['Close'][i] *= (1 + priceNoise/100)\n            newTradingEnv.data['Low'][i] *= (1 + priceNoise/100)\n            newTradingEnv.data['High'][i] *= (1 + priceNoise/100)\n            newTradingEnv.data['Volume'][i] *= (1 + volumeNoise/100)\n            newTradingEnv.data['Open'][i] = newTradingEnv.data['Close'][i-1]\n        return newTradingEnv\n    def lowPassFilter(self, tradingEnv, order=5):\n        newTradingEnv = copy.deepcopy(tradingEnv)\n        newTradingEnv.data['Close'] = newTradingEnv.data['Close'].rolling(window=order).mean()\n        newTradingEnv.data['Low'] = newTradingEnv.data['Low'].rolling(window=order).mean()\n        newTradingEnv.data['High'] = newTradingEnv.data['High'].rolling(window=order).mean()\n        newTradingEnv.data['Volume'] = newTradingEnv.data['Volume'].rolling(window=order).mean()\n        for i in range(order):\n            newTradingEnv.data['Close'][i] = tradingEnv.data['Close'][i]\n            newTradingEnv.data['Low'][i] = tradingEnv.data['Low'][i]\n            newTradingEnv.data['High'][i] = tradingEnv.data['High'][i]\n            newTradingEnv.data['Volume'][i] = tradingEnv.data['Volume'][i]\n        newTradingEnv.data['Open'] = newTradingEnv.data['Close'].shift(1)\n        newTradingEnv.data['Open'][0] = tradingEnv.data['Open'][0]\n        return newTradingEnv\n    def generate(self, tradingEnv):\n        tradingEnvList = []\n        for shift in shiftRange:\n            tradingEnvShifted = self.shiftTimeSeries(tradingEnv, shift)\n            for stretch in stretchRange:\n                tradingEnvStretched = self.streching(tradingEnvShifted, stretch)\n                for order in filterRange:\n                    tradingEnvFiltered = self.lowPassFilter(tradingEnvStretched, order)\n                    for noise in noiseRange:\n                        tradingEnvList.append(self.noiseAddition(tradingEnvFiltered, noise))\n        return tradingEnvList\n\n\nclass YahooFinance:\n    def __init__(self):\n        self.data = pd.DataFrame()\n\n    def getDailyData(self, marketSymbol, startingDate, endingDate):\n        try:\n            ticker = yf.Ticker(marketSymbol)\n            data = ticker.history(start=startingDate, end=endingDate)\n            self.data = self.processDataframe(data)\n        except Exception as e:\n            logging.error(f\"Failed to download {marketSymbol}, from {startingDate} to {endingDate}: {e}\")\n            self.data = pd.DataFrame()\n        return self.data\n\n    def processDataframe(self, dataframe):\n        dataframe = dataframe.reset_index()\n        dataframe['Date'] = dataframe['Date'].dt.date\n        dataframe.set_index('Date', inplace=True)\n        dataframe = dataframe[['Open', 'High', 'Low', 'Close', 'Volume']]\n        return dataframe\n\nclass CSVHandler:\n    def dataframeToCSV(self, name, dataframe):\n        path = name + '.csv'\n        dataframe.to_csv(path)\n\n    def CSVToDataframe(self, name):\n        path = name + '.csv'\n        return pd.read_csv(path,\n                           header=0,\n                           index_col='Date',\n                           parse_dates=True)\n\n\nclass StockGenerator:\n    def linearUp (self, startingDate, endingDate, minValue=MIN, maxValue=MAX):\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        linearUpward = pd.DataFrame(index=DowJones.index)\n        length = len(linearUpward.index)\n        prices = np.linspace(minValue, maxValue, num=length)\n        linearUpward['Open'] = prices\n        linearUpward['High'] = prices\n        linearUpward['Low'] = prices\n        linearUpward['Close'] = prices\n        linearUpward['Volume'] = 100000\n        return linearUpward\n    def linearDown (self, startingDate, endingDate, minValue=MIN, maxValue=MAX):\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        linearDownward = pd.DataFrame(index=DowJones.index)\n        length = len(linearDownward.index)\n        prices = np.linspace(minValue, maxValue, num=length)\n        prices = np.flip(prices)\n        linearDownward['Open'] = prices\n        linearDownward['High'] = prices\n        linearDownward['Low'] = prices\n        linearDownward['Close'] = prices\n        linearDownward['Volume'] = 100000\n        return linearDownward\n    def sinusoidal(self, startingDate, endingDate, minValue=MIN, maxValue=MAX, period=PERIOD):\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        sinusoidal = pd.DataFrame(index=DowJones.index)\n        length = len(sinusoidal.index)\n        t = np.linspace(0, length, num=length)\n        prices = minValue + maxValue / 2 * (np.sin(2 * np.pi * t / period) + 1) / 2\n        sinusoidal['Open'] = prices\n        sinusoidal['High'] = prices\n        sinusoidal['Low'] = prices\n        sinusoidal['Close'] = prices\n        sinusoidal['Volume'] = 100000\n        return sinusoidal\n    def triangle(self, startingDate, endingDate, minValue=MIN, maxValue=MAX, period=PERIOD):\n        downloader = YahooFinance()\n        DowJones = downloader.getDailyData('DIA', startingDate, endingDate)\n        triangle = pd.DataFrame(index=DowJones.index)\n        length = len(triangle.index)\n        t = np.linspace(0, length, num=length)\n        prices = minValue + maxValue / 2 * np.abs(signal.sawtooth(2 * np.pi * t / period))\n        triangle['Open'] = prices\n        triangle['High'] = prices\n        triangle['Low'] = prices\n        triangle['Close'] = prices\n        triangle['Volume'] = 100000\n        return triangle\n\nclass TradingEnv(gym.Env):\n    def __init__(self, marketSymbol, startingDate, endingDate, money, stateLength=30,\n                 transactionCosts=0, startingPoint=0, data_dir='./data/', features=['High', 'Low', 'Open', 'Close']):\n        self.features = features\n        if(marketSymbol in fictiveStocks):\n            stockGeneration = StockGenerator()\n            if(marketSymbol == 'LINEARUP'):\n                self.data = stockGeneration.linearUp(startingDate, endingDate)\n            elif(marketSymbol == 'LINEARDOWN'):\n                self.data = stockGeneration.linearDown(startingDate, endingDate)\n            elif(marketSymbol == 'SINUSOIDAL'):\n                self.data = stockGeneration.sinusoidal(startingDate, endingDate)\n            else:\n                self.data = stockGeneration.triangle(startingDate, endingDate)\n        else:\n            csvConverter = CSVHandler()\n            csvName = \"\".join([data_dir, marketSymbol, '_', startingDate, '_', endingDate])\n            exists = os.path.isfile(csvName + '.csv')\n            if(exists):\n                self.data = csvConverter.CSVToDataframe(csvName)\n            else:\n                downloader1 = YahooFinance()\n                try:\n                    self.data = downloader1.getDailyData(marketSymbol, startingDate, endingDate)\n                except Exception as e:\n                    logging.error(f\"Error in downloading data: {e}\")\n                if saving == True:\n                    logging.info(f\"Saving to CSV: {csvName}\")\n                    csvConverter.dataframeToCSV(csvName, self.data)\n        self.data.replace(0.0, np.nan, inplace=True)\n        self.data.interpolate(method='linear', limit=5, limit_area='inside', inplace=True)\n        self.data.fillna(method='ffill', inplace=True)\n        self.data.fillna(method='bfill', inplace=True)\n        self.data.fillna(0, inplace=True)\n        self.data['Position'] = 0\n        self.data['Action'] = 0\n        self.data['Holdings'] = 0.\n        self.data['Cash'] = float(money)\n        self.data['Money'] = self.data['Holdings'] + self.data['Cash']\n        self.data['Returns'] = 0.\n        self.state = [self.data[feature][0:stateLength].tolist() for feature in self.features] + [[0]]\n        self.reward = 0.\n        self.done = 0\n        self.marketSymbol = marketSymbol\n        self.startingDate = startingDate\n        self.endingDate = endingDate\n        self.stateLength = stateLength\n        self.t = stateLength\n        self.numberOfShares = 0\n        self.transactionCosts = transactionCosts\n        self.epsilon = 0.1\n        if startingPoint:\n            self.setStartingPoint(startingPoint)\n\n    def reset(self):\n        self.data['Position'] = 0\n        self.data['Action'] = 0\n        self.data['Holdings'] = 0.\n        self.data['Cash'] = self.data['Cash'][0]\n        self.data['Money'] = self.data['Holdings'] + self.data['Cash']\n        self.data['Returns'] = 0.\n        self.state = [self.data[feature][0:self.stateLength].tolist() for feature in self.features] + [[0]]\n        self.reward = 0.\n        self.done = 0\n        self.t = self.stateLength\n        self.numberOfShares = 0\n        return self.state\n\n    def computeLowerBound(self, cash, numberOfShares, price):\n        deltaValues = - cash - numberOfShares * price * (1 + self.epsilon) * (1 + self.transactionCosts)\n        if deltaValues < 0:\n            lowerBound = deltaValues / (price * (2 * self.transactionCosts + (self.epsilon * (1 + self.transactionCosts))))\n        else:\n            lowerBound = deltaValues / (price * self.epsilon * (1 + self.transactionCosts))\n        return lowerBound\n\n    def step(self, action):\n        t = self.t\n        numberOfShares = self.numberOfShares\n        customReward = False\n        if(action == 1):\n            self.data['Position'][t] = 1\n            if(self.data['Position'][t - 1] == 1):\n                self.data['Cash'][t] = self.data['Cash'][t - 1]\n                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n            elif(self.data['Position'][t - 1] == 0):\n                self.numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t - 1] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = 1\n            else:\n                self.data['Cash'][t] = self.data['Cash'][t - 1] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                self.numberOfShares = math.floor(self.data['Cash'][t]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t] - self.numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                self.data['Holdings'][t] = self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = 1\n        elif(action == 0):\n            self.data['Position'][t] = -1\n            if(self.data['Position'][t - 1] == -1):\n                lowerBound = self.computeLowerBound(self.data['Cash'][t - 1], -numberOfShares, self.data['Close'][t-1])\n                if lowerBound <= 0:\n                    self.data['Cash'][t] = self.data['Cash'][t - 1]\n                    self.data['Holdings'][t] =  - self.numberOfShares * self.data['Close'][t]\n                else:\n                    numberOfSharesToBuy = min(math.floor(lowerBound), self.numberOfShares)\n                    self.numberOfShares -= numberOfSharesToBuy\n                    self.data['Cash'][t] = self.data['Cash'][t - 1] - numberOfSharesToBuy * self.data['Close'][t] * (1 + self.transactionCosts)\n                    self.data['Holdings'][t] =  - self.numberOfShares * self.data['Close'][t]\n                    customReward = True\n            elif(self.data['Position'][t - 1] == 0):\n                self.numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t - 1] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                self.data['Holdings'][t] = - self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = -1\n            else:\n                self.data['Cash'][t] = self.data['Cash'][t - 1] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                self.numberOfShares = math.floor(self.data['Cash'][t]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                self.data['Cash'][t] = self.data['Cash'][t] + self.numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                self.data['Holdings'][t] = - self.numberOfShares * self.data['Close'][t]\n                self.data['Action'][t] = -1\n        else:\n            raise SystemExit(\"Prohibited action! Action should be either 1 (long) or 0 (short).\")\n        self.data['Money'][t] = self.data['Holdings'][t] + self.data['Cash'][t]\n        self.data['Returns'][t] = (self.data['Money'][t] - self.data['Money'][t-1])/self.data['Money'][t-1]\n        if not customReward:\n            self.reward = self.data['Returns'][t]\n        else:\n            self.reward = (self.data['Close'][t-1] - self.data['Close'][t])/self.data['Close'][t-1]\n        self.t = self.t + 1\n\n        self.state = [self.data[feature].iloc[self.t - self.stateLength : self.t].tolist() for feature in self.features] + [[self.data['Position'][self.t - 1]]]\n        if(self.t == self.data.shape[0]):\n            self.done = 1\n\n        otherAction = int(not bool(action))\n        customReward = False\n        if(otherAction == 1):\n            otherPosition = 1\n            if(self.data['Position'][t - 1] == 1):\n                otherCash = self.data['Cash'][t - 1]\n                otherHoldings = numberOfShares * self.data['Close'][t]\n            elif(self.data['Position'][t - 1] == 0):\n                numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = self.data['Cash'][t - 1] - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                otherHoldings = numberOfShares * self.data['Close'][t]\n            else:\n                otherCash = self.data['Cash'][t - 1] - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                numberOfShares = math.floor(otherCash/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = otherCash - numberOfShares * self.data['Close'][t] * (1 + self.transactionCosts)\n                otherHoldings = numberOfShares * self.data['Close'][t]\n        else:\n            otherPosition = -1\n            if(self.data['Position'][t - 1] == -1):\n                lowerBound = self.computeLowerBound(self.data['Cash'][t - 1], -numberOfShares, self.data['Close'][t-1])\n                if lowerBound <= 0:\n                    otherCash = self.data['Cash'][t - 1]\n                    otherHoldings =  - numberOfShares * self.data['Close'][t]\n                else:\n                    numberOfSharesToBuy = min(math.floor(lowerBound), numberOfShares)\n                    numberOfShares -= numberOfSharesToBuy\n                    otherCash = self.data['Cash'][t - 1] - numberOfSharesToBuy * self.data['Close'][t] * (1 + self.transactionCosts)\n                    otherHoldings =  - numberOfShares * self.data['Close'][t]\n                    customReward = True\n            elif(self.data['Position'][t - 1] == 0):\n                numberOfShares = math.floor(self.data['Cash'][t - 1]/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = self.data['Cash'][t - 1] + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                otherHoldings = - numberOfShares * self.data['Close'][t]\n            else:\n                otherCash = self.data['Cash'][t - 1] + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                numberOfShares = math.floor(otherCash/(self.data['Close'][t] * (1 + self.transactionCosts)))\n                otherCash = otherCash + numberOfShares * self.data['Close'][t] * (1 - self.transactionCosts)\n                otherHoldings = - self.numberOfShares * self.data['Close'][t]\n        otherMoney = otherHoldings + otherCash\n        if not customReward:\n            otherReward = (otherMoney - self.data['Money'][t-1])/self.data['Money'][t-1]\n        else:\n            otherReward = (self.data['Close'][t-1] - self.data['Close'][t])/self.data['Close'][t-1]\n        otherState = [self.data[feature][self.t - self.stateLength : self.t].tolist() for feature in self.features] + [[otherPosition]]\n        self.info = {'State' : otherState, 'Reward' : otherReward, 'Done' : self.done}\n        return self.state, self.reward, self.done, self.info\n\n    def render(self, savePlots=False):\n        fig = plt.figure(figsize=(10, 8))\n        ax1 = fig.add_subplot(211, ylabel='Price', xlabel='Time')\n        ax2 = fig.add_subplot(212, ylabel='Capital', xlabel='Time', sharex=ax1)\n        self.data['Close'].plot(ax=ax1, color='blue', lw=2)\n        ax1.plot(self.data.loc[self.data['Action'] == 1.0].index,\n                 self.data['Close'][self.data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax1.plot(self.data.loc[self.data['Action'] == -1.0].index,\n                 self.data['Close'][self.data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n        self.data['Money'].plot(ax=ax2, color='blue', lw=2)\n        ax2.plot(self.data.loc[self.data['Action'] == 1.0].index,\n                 self.data['Money'][self.data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax2.plot(self.data.loc[self.data['Action'] == -1.0].index,\n                 self.data['Money'][self.data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n        ax1.legend([\"Price\", \"Long\",  \"Short\"])\n        ax2.legend([\"Capital\", \"Long\", \"Short\"])\n        if savePlots:\n            plt.savefig(''.join(['images/', str(self.marketSymbol), '_Rendering', '.png']))\n\n    def setStartingPoint(self, startingPoint):\n        self.t = np.clip(startingPoint, self.stateLength, len(self.data.index))\n        self.state = [self.data[feature][self.t - self.stateLength : self.t].tolist() for feature in self.features] + [[self.data['Position'][self.t - 1]]]\n        if(self.t == self.data.shape[0]):\n            self.done = 1\n\nclass TradingSimulator:\n    def displayTestbench(self, startingDate, endingDate, features=['High', 'Low', 'Open', 'Close']):\n        for _, stock in indices.items():\n            env = TradingEnv(stock, startingDate, endingDate, 0, features=features)\n            env.render()\n        for _, stock in companies.items():\n            env = TradingEnv(stock, startingDate, endingDate, 0, features=features)\n            env.render()\n\n    def analyseTimeSeries(self, stockName, startingDate, endingDate, splitingDate, features=['High', 'Low', 'Open', 'Close']):\n        if(stockName in fictives):\n            stock = fictives[stockName]\n        elif(stockName in indices):\n            stock = indices[stockName]\n        elif(stockName in companies):\n            stock = companies[stockName]\n        else:\n            logging.info(\"The stock specified is not valid, only the following stocks are supported:\")\n            for stock in fictives:\n                logging.info(\"\".join(['- ', stock]))\n            for stock in indices:\n                logging.info(\"\".join(['- ', stock]))\n            for stock in companies:\n                logging.info(\"\".join(['- ', stock]))\n            raise SystemError(\"Please check the stock specified.\")\n\n        logging.info(\"\\n\\n\\nAnalysis of the TRAINING phase time series\")\n        logging.info(\"------------------------------------------\\n\")\n        trainingEnv = TradingEnv(stock, startingDate, splitingDate, 0, features=features)\n        timeSeries = trainingEnv.data['Close']\n        analyser = TimeSeriesAnalyser(timeSeries)\n        analyser.timeSeriesDecomposition()\n        analyser.stationarityAnalysis()\n        analyser.cyclicityAnalysis()\n\n        logging.info(\"\\n\\n\\nAnalysis of the TESTING phase time series\")\n        logging.info(\"------------------------------------------\\n\")\n        testingEnv = TradingEnv(stock, splitingDate, endingDate, 0, features=features)\n        timeSeries = testingEnv.data['Close']\n        analyser = TimeSeriesAnalyser(timeSeries)\n        analyser.timeSeriesDecomposition()\n        analyser.stationarityAnalysis()\n        analyser.cyclicityAnalysis()\n\n        logging.info(\"\\n\\n\\nAnalysis of the entire time series (both training and testing phases)\")\n        logging.info(\"---------------------------------------------------------------------\\n\")\n        tradingEnv = TradingEnv(stock, startingDate, endingDate, 0, features=features)\n        timeSeries = tradingEnv.data['Close']\n        analyser = TimeSeriesAnalyser(timeSeries)\n        analyser.timeSeriesDecomposition()\n        analyser.stationarityAnalysis()\n        analyser.cyclicityAnalysis()\n\n    def plotEntireTrading(self, trainingEnv, testingEnv, splitingDate, savePlots=False):\n        ratio = trainingEnv.data['Money'][-1]/testingEnv.data['Money'][0]\n        testingEnv.data['Money'] = ratio * testingEnv.data['Money']\n        dataframes = [trainingEnv.data, testingEnv.data]\n        data = pd.concat(dataframes)\n        fig = plt.figure(figsize=(10, 8))\n        ax1 = fig.add_subplot(211, ylabel='Price', xlabel='Time')\n        ax2 = fig.add_subplot(212, ylabel='Capital', xlabel='Time', sharex=ax1)\n        trainingEnv.data['Close'].plot(ax=ax1, color='blue', lw=2)\n        testingEnv.data['Close'].plot(ax=ax1, color='blue', lw=2, label='_nolegend_')\n        ax1.plot(data.loc[data['Action'] == 1.0].index,\n                 data['Close'][data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax1.plot(data.loc[data['Action'] == -1.0].index,\n                 data['Close'][data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n        trainingEnv.data['Money'].plot(ax=ax2, color='blue', lw=2)\n        testingEnv.data['Money'].plot(ax=ax2, color='blue', lw=2, label='_nolegend_')\n        ax2.plot(data.loc[data['Action'] == 1.0].index,\n                 data['Money'][data['Action'] == 1.0],\n                 '^', markersize=5, color='green')\n        ax2.plot(data.loc[data['Action'] == -1.0].index,\n                 data['Money'][data['Action'] == -1.0],\n                 'v', markersize=5, color='red')\n        ax1.axvline(pd.to_datetime(splitingDate), color='black', linewidth=2.0)\n        ax2.axvline(pd.to_datetime(splitingDate), color='black', linewidth=2.0)\n        ax1.legend([\"Price\", \"Long\",  \"Short\", \"Train/Test separation\"])\n        ax2.legend([\"Capital\", \"Long\", \"Short\", \"Train/Test separation\"])\n        if savePlots:\n            plt.savefig(''.join(['images/', str(trainingEnv.marketSymbol), '_TrainingTestingRendering', '.png']))\n\n    def simulateNewStrategy(self, strategyName, stockName,\n                            startingDate, endingDate, splitingDate,\n                            observationSpace, actionSpace,\n                            money, stateLength, transactionCosts,\n                            bounds, step, numberOfEpisodes,\n                            verbose=True, plotTraining=True, rendering=True, showPerformance=True,\n                            saveStrategy=False,\n                            savePlots=False,\n                            data_dir='./data/',\n                            strategies_dir='./models/',\n                            features=['High', 'Low', 'Open', 'Close']):\n        if(strategyName in models):\n            strategy = models[strategyName]\n            trainingParameters = [bounds, step]\n            ai = False\n        elif(strategyName in strategiesAI):\n            strategy = strategiesAI[strategyName]\n            trainingParameters = [numberOfEpisodes]\n            ai = True\n        else:\n            logging.info(\"The strategy specified is not valid, only the following models are supported:\")\n            for strategy in models:\n                logging.info(\"\".join(['- ', strategy]))\n            for strategy in strategiesAI:\n                logging.info(\"\".join(['- ', strategy]))\n            raise SystemError(\"Please check the trading strategy specified.\")\n\n        if(stockName in fictives):\n            stock = fictives[stockName]\n        elif(stockName in indices):\n            stock = indices[stockName]\n        elif(stockName in companies):\n            stock = companies[stockName]\n        else:\n            logging.info(\"The stock specified is not valid, only the following stocks are supported:\")\n            for stock in fictives:\n                logging.info(\"\".join(['- ', stock]))\n            for stock in indices:\n                logging.info(\"\".join(['- ', stock]))\n            for stock in companies:\n                logging.info(\"\".join(['- ', stock]))\n            raise SystemError(\"Please check the stock specified.\")\n\n        trainingEnv = TradingEnv(stock, startingDate, splitingDate, money, stateLength, transactionCosts, data_dir=data_dir, features=features)\n        if ai:\n            tradingStrategy = TDQN(observationSpace, actionSpace)\n        else:\n            if strategyName == 'Buy and Hold':\n                tradingStrategy = BuyAndHold()\n            elif strategyName == 'Sell and Hold':\n                tradingStrategy = SellAndHold()\n            elif strategyName == 'Trend Following Moving Averages':\n                tradingStrategy = MovingAveragesTF()\n            elif strategyName == 'Mean Reversion Moving Averages' :\n                tradingStrategy = MovingAveragesMR()\n            else:\n                raise SystemError(strategyName)\n\n        trainingEnv = tradingStrategy.training(trainingEnv, trainingParameters=trainingParameters,\n                                               verbose=verbose, rendering=rendering,\n                                               plotTraining=plotTraining, showPerformance=showPerformance,\n                                               features=features)\n        testingEnv = TradingEnv(stock, splitingDate, endingDate, money, stateLength, transactionCosts, features=features)\n        testingEnv = tradingStrategy.testing(trainingEnv, testingEnv, rendering=rendering, showPerformance=showPerformance, features=features)\n        if rendering:\n            self.plotEntireTrading(trainingEnv, testingEnv, splitingDate,savePlots=savePlots)\n        if(saveStrategy):\n            fileName = \"\".join([strategies_dir, strategy, \"_\", stock, \"_\", startingDate, \"_\", splitingDate])\n            if ai:\n                tradingStrategy.saveModel(fileName)\n            else:\n                fileHandler = open(fileName, 'wb')\n                pickle.dump(tradingStrategy, fileHandler)\n        return tradingStrategy, trainingEnv, testingEnv\n\n    def simulateExistingStrategy(self, strategyName, stockName,\n                                 startingDate, endingDate, splitingDate,\n                                 observationSpace, actionSpace,\n                                 money, stateLength, transactionCosts,\n                                 rendering=True, showPerformance=True, strategiesDir='./models/', data_dir='./data/', features=['High', 'Low', 'Open', 'Close']):\n        if(strategyName in models):\n            strategy = models[strategyName]\n            ai = False\n        elif(strategyName in strategiesAI):\n            strategy = strategiesAI[strategyName]\n            ai = True\n        else:\n            logging.info(\"The strategy specified is not valid, only the following models are supported:\")\n            for strategy in models:\n                logging.info(\"\".join(['- ', strategy]))\n            for strategy in strategiesAI:\n                logging.info(\"\".join(['- ', strategy]))\n            raise SystemError(\"Please check the trading strategy specified.\")\n\n        if(stockName in fictives):\n            stock = fictives[stockName]\n        elif(stockName in indices):\n            stock = indices[stockName]\n        elif(stockName in companies):\n            stock = companies[stockName]\n        else:\n            logging.info(\"The stock specified is not valid, only the following stocks are supported:\")\n            for stock in fictives:\n                logging.info(\"\".join(['- ', stock]))\n            for stock in indices:\n                logging.info(\"\".join(['- ', stock]))\n            for stock in companies:\n                logging.info(\"\".join(['- ', stock]))\n            raise SystemError(\"Please check the stock specified.\")\n\n        fileName = \"\".join([strategiesDir, strategy, \"_\", stock, \"_\", startingDate, \"_\", splitingDate])\n        exists = os.path.isfile(fileName)\n        if exists:\n            if ai:\n                tradingStrategy = TDQN(observationSpace, actionSpace)\n                tradingStrategy.loadModel(fileName)\n            else:\n                fileHandler = open(fileName, 'rb')\n                tradingStrategy = pickle.load(fileHandler)\n        else:\n            raise SystemError(\"The trading strategy specified does not exist, please provide a valid one.\")\n\n        trainingEnv = TradingEnv(stock, startingDate, splitingDate, money, stateLength, transactionCosts, data_dir=data_dir, features=features)\n        testingEnv = TradingEnv(stock, splitingDate, endingDate, money, stateLength, transactionCosts, data_dir=data_dir, features=features)\n        trainingEnv = tradingStrategy.testing(trainingEnv, trainingEnv, rendering=rendering, showPerformance=showPerformance)\n        testingEnv = tradingStrategy.testing(trainingEnv, testingEnv, rendering=rendering, showPerformance=showPerformance)\n        if rendering:\n            self.plotEntireTrading(trainingEnv, testingEnv, splitingDate)\n        return tradingStrategy, trainingEnv, testingEnv\n\n\nclass ReplayMemory:\n    def __init__(self, capacity=capacity):\n        self.memory = deque(maxlen=capacity)\n    def push(self, state, action, reward, nextState, done):\n        self.memory.append((state, action, reward, nextState, done))\n    def sample(self, batchSize):\n        state, action, reward, nextState, done = zip(*random.sample(self.memory, batchSize))\n        return state, action, reward, nextState, done\n    def __len__(self):\n        return len(self.memory)\n    def reset(self):\n        self.memory = deque(maxlen=capacity)\n\nclass DQN(nn.Module):\n    def __init__(self, numberOfInputs, numberOfOutputs, numberOfNeurons=numberOfNeurons, dropout=dropout):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(numberOfInputs, numberOfNeurons)\n        self.fc2 = nn.Linear(numberOfNeurons, numberOfNeurons)\n        self.fc3 = nn.Linear(numberOfNeurons, numberOfNeurons)\n        self.fc4 = nn.Linear(numberOfNeurons, numberOfNeurons)\n        self.fc5 = nn.Linear(numberOfNeurons, numberOfOutputs)\n        self.bn1 = nn.BatchNorm1d(numberOfNeurons)\n        self.bn2 = nn.BatchNorm1d(numberOfNeurons)\n        self.bn3 = nn.BatchNorm1d(numberOfNeurons)\n        self.bn4 = nn.BatchNorm1d(numberOfNeurons)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n        self.dropout4 = nn.Dropout(dropout)\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\n        torch.nn.init.xavier_uniform_(self.fc3.weight)\n        torch.nn.init.xavier_uniform_(self.fc4.weight)\n        torch.nn.init.xavier_uniform_(self.fc5.weight)\n    def forward(self, input):\n        x = self.dropout1(F.leaky_relu(self.bn1(self.fc1(input))))\n        x = self.dropout2(F.leaky_relu(self.bn2(self.fc2(x))))\n        x = self.dropout3(F.leaky_relu(self.bn3(self.fc3(x))))\n        x = self.dropout4(F.leaky_relu(self.bn4(self.fc4(x))))\n        output = self.fc5(x)\n        return output\n\nclass TDQN:\n    def __init__(self, observationSpace, actionSpace, numberOfNeurons=numberOfNeurons, dropout=dropout,\n                 gamma=gamma, learningRate=learningRate, targetNetworkUpdate=targetNetworkUpdate,\n                 epsilonStart=epsilonStart, epsilonEnd=epsilonEnd, epsilonDecay=epsilonDecay,\n                 capacity=capacity, batchSize=batchSize):\n        random.seed(0)\n        self.device = torch.device('cuda:'+str(GPUNumber) if torch.cuda.is_available() else 'cpu')\n        self.gamma = gamma\n        self.learningRate = learningRate\n        self.targetNetworkUpdate = targetNetworkUpdate\n        self.capacity = capacity\n        self.batchSize = batchSize\n        self.replayMemory = ReplayMemory(capacity)\n        self.observationSpace = observationSpace\n        self.actionSpace = actionSpace\n\n        self.policyNetwork = DQN(observationSpace, actionSpace, numberOfNeurons, dropout).to(self.device)\n        self.targetNetwork = DQN(observationSpace, actionSpace, numberOfNeurons, dropout).to(self.device)\n        self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n        self.policyNetwork.eval()\n        self.targetNetwork.eval()\n        self.optimizer = optim.Adam(self.policyNetwork.parameters(), lr=learningRate, weight_decay=L2Factor)\n        self.epsilonValue = lambda iteration: epsilonEnd + (epsilonStart - epsilonEnd) * math.exp(-1 * iteration / epsilonDecay)\n        self.iterations = 0\n        self.writer = SummaryWriter('runs/' + datetime.datetime.now().strftime(\"%d/%m/%Y-%H:%M:%S\"))\n    def getNormalizationCoefficients(self, tradingEnv):\n        tradingData = tradingEnv.data\n        closePrices = tradingData['Close'].tolist()\n        lowPrices = tradingData['Low'].tolist()\n        highPrices = tradingData['High'].tolist()\n        volumes = tradingData['Volume'].tolist()\n        coefficients = []\n        margin = 1\n        returns = [abs((closePrices[i]-closePrices[i-1])/closePrices[i-1]) for i in range(1, len(closePrices))]\n        coeffs = (0, np.max(returns)*margin)\n        coefficients.append(coeffs)\n        deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(len(lowPrices))]\n        coeffs = (0, np.max(deltaPrice)*margin)\n        coefficients.append(coeffs)\n        coeffs = (0, 1)\n        coefficients.append(coeffs)\n        coeffs = (np.min(volumes)/margin, np.max(volumes)*margin)\n        coefficients.append(coeffs)\n        return coefficients\n    def processState(self, state, coefficients):\n        closePrices = [state[0][i] for i in range(len(state[0]))]\n        lowPrices = [state[1][i] for i in range(len(state[1]))]\n        highPrices = [state[2][i] for i in range(len(state[2]))]\n        volumes = [state[3][i] for i in range(len(state[3]))]\n\n        returns = [(closePrices[i]-closePrices[i-1])/closePrices[i-1] for i in range(1, len(closePrices))]\n        if coefficients[0][0] != coefficients[0][1]:\n            state[0] = [((x - coefficients[0][0])/(coefficients[0][1] - coefficients[0][0])) for x in returns]\n        else:\n            state[0] = [0 for x in returns]\n        deltaPrice = [abs(highPrices[i]-lowPrices[i]) for i in range(1, len(lowPrices))]\n        if coefficients[1][0] != coefficients[1][1]:\n            state[1] = [((x - coefficients[1][0])/(coefficients[1][1] - coefficients[1][0])) for x in deltaPrice]\n        else:\n            state[1] = [0 for x in deltaPrice]\n        closePricePosition = []\n        for i in range(1, len(closePrices)):\n            deltaPrice = abs(highPrices[i]-lowPrices[i])\n            if deltaPrice != 0:\n                item = abs(closePrices[i]-lowPrices[i])/deltaPrice\n            else:\n                item = 0.5\n            closePricePosition.append(item)\n        if coefficients[2][0] != coefficients[2][1]:\n            state[2] = [((x - coefficients[2][0])/(coefficients[2][1] - coefficients[2][0])) for x in closePricePosition]\n        else:\n            state[2] = [0.5 for x in closePricePosition]\n        volumes = [volumes[i] for i in range(1, len(volumes))]\n        if coefficients[3][0] != coefficients[3][1]:\n            state[3] = [((x - coefficients[3][0])/(coefficients[3][1] - coefficients[3][0])) for x in volumes]\n        else:\n            state[3] = [0 for x in volumes]\n\n        # The last feature is always the position.\n        # By default they have 4 features and position:\n        # [CLOSE, LOW, HIGH, VOLUME], xxx, [POSITION]\n        # Where xxx are additional features we use.\n        if len(state)-1 > 4:\n            print(len(state))\n            for idx in range(4, len(state)):\n                current_state = [state[idx][i] for i in range(len(state[idx]))]\n                current_state = [abs(current_state[i] - current_state[i-1]) for i in range(1, len(current_state))]\n                state[idx] = [x for x in current_state]\n\n        state = [item for sublist in state for item in sublist]\n\n        return state\n    def processReward(self, reward):\n        return np.clip(reward, -rewardClipping, rewardClipping)\n    def updateTargetNetwork(self):\n        if(self.iterations % targetNetworkUpdate == 0):\n            self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n    def chooseAction(self, state):\n        with torch.no_grad():\n            tensorState = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze(0)\n            QValues = self.policyNetwork(tensorState).squeeze(0)\n            Q, action = QValues.max(0)\n            action = action.item()\n            Q = Q.item()\n            QValues = QValues.cpu().numpy()\n            return action, Q, QValues\n    def chooseActionEpsilonGreedy(self, state, previousAction):\n        if(random.random() > self.epsilonValue(self.iterations)):\n            if(random.random() > alpha):\n                action, Q, QValues = self.chooseAction(state)\n            else:\n                action = previousAction\n                Q = 0\n                QValues = [0, 0]\n        else:\n            action = random.randrange(self.actionSpace)\n            Q = 0\n            QValues = [0, 0]\n        self.iterations += 1\n        return action, Q, QValues\n    def learning(self, batchSize=batchSize):\n        if (len(self.replayMemory) >= batchSize):\n            self.policyNetwork.train()\n            state, action, reward, nextState, done = self.replayMemory.sample(batchSize)\n\n            state = torch.tensor(state, dtype=torch.float, device=self.device)\n            action = torch.tensor(action, dtype=torch.long, device=self.device)\n            reward = torch.tensor(reward, dtype=torch.float, device=self.device)\n            nextState = torch.tensor(nextState, dtype=torch.float, device=self.device)\n            done = torch.tensor(done, dtype=torch.float, device=self.device)\n            currentQValues = self.policyNetwork(state).gather(1, action.unsqueeze(1)).squeeze(1)\n            with torch.no_grad():\n                nextActions = torch.max(self.policyNetwork(nextState), 1)[1]\n                nextQValues = self.targetNetwork(nextState).gather(1, nextActions.unsqueeze(1)).squeeze(1)\n                expectedQValues = reward + gamma * nextQValues * (1 - done)\n            loss = F.smooth_l1_loss(currentQValues, expectedQValues)\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.policyNetwork.parameters(), gradientClipping)\n            self.optimizer.step()\n            self.updateTargetNetwork()\n            self.policyNetwork.eval()\n    def training(self, trainingEnv, trainingParameters=[],\n                 verbose=False, rendering=False, savePlots=False, plotTraining=False, showPerformance=False, features=['Open', 'High', 'Low', 'Close']):\n        dataAugmentation = DataAugmentation()\n        trainingEnvList = dataAugmentation.generate(trainingEnv)\n        if plotTraining:\n            performanceTrain = []\n            score = np.zeros((len(trainingEnvList), trainingParameters[0]))\n            marketSymbol = trainingEnv.marketSymbol\n            startingDate = trainingEnv.endingDate\n            endingDate = '2020-1-1'\n            money = trainingEnv.data['Money'][0]\n            stateLength = trainingEnv.stateLength\n            transactionCosts = trainingEnv.transactionCosts\n            testingEnv = TradingEnv(marketSymbol, startingDate, endingDate, money, stateLength, transactionCosts, features=features)\n            performanceTest = []\n        try:\n            if verbose:\n                logging.info(\"Training progression (hardware selected => \" + str(self.device) + \"):\")\n            for episode in tqdm(range(trainingParameters[0]), disable=not(verbose)):\n                for i in range(len(trainingEnvList)):\n                    coefficients = self.getNormalizationCoefficients(trainingEnvList[i])\n                    trainingEnvList[i].reset()\n                    startingPoint = random.randrange(len(trainingEnvList[i].data.index))\n                    trainingEnvList[i].setStartingPoint(startingPoint)\n                    state = self.processState(trainingEnvList[i].state, coefficients)\n                    previousAction = 0\n                    done = 0\n                    stepsCounter = 0\n                    if plotTraining:\n                        totalReward = 0\n                    while done == 0:\n                        action, _, _ = self.chooseActionEpsilonGreedy(state, previousAction)\n                        nextState, reward, done, info = trainingEnvList[i].step(action)\n                        reward = self.processReward(reward)\n                        nextState = self.processState(nextState, coefficients)\n                        self.replayMemory.push(state, action, reward, nextState, done)\n                        otherAction = int(not bool(action))\n                        otherReward = self.processReward(info['Reward'])\n                        otherNextState = self.processState(info['State'], coefficients)\n                        otherDone = info['Done']\n                        self.replayMemory.push(state, otherAction, otherReward, otherNextState, otherDone)\n                        stepsCounter += 1\n                        if stepsCounter == learningUpdatePeriod:\n                            self.learning()\n                            stepsCounter = 0\n                        state = nextState\n                        previousAction = action\n                        if plotTraining:\n                            totalReward += reward\n                    if plotTraining:\n                        score[i][episode] = totalReward\n                if plotTraining:\n                    trainingEnv = self.testing(trainingEnv, trainingEnv)\n                    analyser = PerformanceEstimator(trainingEnv.data)\n                    performance = analyser.computeSharpeRatio()\n                    performanceTrain.append(performance)\n                    self.writer.add_scalar('Training performance (Sharpe Ratio)', performance, episode)\n                    trainingEnv.reset()\n                    testingEnv = self.testing(trainingEnv, testingEnv)\n                    analyser = PerformanceEstimator(testingEnv.data)\n                    performance = analyser.computeSharpeRatio()\n                    performanceTest.append(performance)\n                    self.writer.add_scalar('Testing performance (Sharpe Ratio)', performance, episode)\n                    testingEnv.reset()\n        except KeyboardInterrupt:\n            logging.info()\n            logging.info(\"WARNING: Training prematurely interrupted...\")\n            logging.info()\n            self.policyNetwork.eval()\n        trainingEnv = self.testing(trainingEnv, trainingEnv)\n        if rendering:\n            trainingEnv.render()\n        if plotTraining:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, ylabel='Performance (Sharpe Ratio)', xlabel='Episode')\n            ax.plot(performanceTrain)\n            ax.plot(performanceTest)\n            ax.legend([\"Training\", \"Testing\"])\n            if savePlots:\n                plt.savefig(''.join(['images/', str(marketSymbol), '_TrainingTestingPerformance', '.png']))\n            for i in range(len(trainingEnvList)):\n                self.plotTraining(score[i][:episode], marketSymbol)\n        if showPerformance:\n            analyser = PerformanceEstimator(trainingEnv.data)\n            analyser.displayPerformance('TDQN')\n        self.writer.close()\n        return trainingEnv\n    def testing(self, trainingEnv, testingEnv, rendering=False, savePlots=False, showPerformance=False, features=['Open', 'High', 'Low', 'Close']):\n        dataAugmentation = DataAugmentation()\n        testingEnvSmoothed = dataAugmentation.lowPassFilter(testingEnv, filterOrder)\n        trainingEnv = dataAugmentation.lowPassFilter(trainingEnv, filterOrder)\n        coefficients = self.getNormalizationCoefficients(trainingEnv)\n        state = self.processState(testingEnvSmoothed.reset(), coefficients)\n        testingEnv.reset()\n        QValues0 = []\n        QValues1 = []\n        done = 0\n        while done == 0:\n            action, _, QValues = self.chooseAction(state)\n            nextState, _, done, _ = testingEnvSmoothed.step(action)\n            testingEnv.step(action)\n            state = self.processState(nextState, coefficients)\n            QValues0.append(QValues[0])\n            QValues1.append(QValues[1])\n        if rendering:\n            testingEnv.render()\n            self.plotQValues(QValues0, QValues1, testingEnv.marketSymbol)\n        if showPerformance:\n            analyser = PerformanceEstimator(testingEnv.data)\n            analyser.displayPerformance('TDQN')\n        return testingEnv\n    def plotTraining(self, score, marketSymbol, savePlots=False):\n        fig = plt.figure()\n        ax1 = fig.add_subplot(111, ylabel='Total reward collected', xlabel='Episode')\n        ax1.plot(score)\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), 'TrainingResults', '.png']))\n    def plotQValues(self, QValues0, QValues1, marketSymbol, savePlots=False):\n        fig = plt.figure()\n        ax1 = fig.add_subplot(111, ylabel='Q values', xlabel='Time')\n        ax1.plot(QValues0)\n        ax1.plot(QValues1)\n        ax1.legend(['Short', 'Long'])\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), '_QValues', '.png']))\n    def plotExpectedPerformance(self, trainingEnv, trainingParameters=[], iterations=10, savePlots = False, features=['Open', 'High', 'Low', 'Close']):\n        dataAugmentation = DataAugmentation()\n        trainingEnvList = dataAugmentation.generate(trainingEnv)\n        initialWeights =  copy.deepcopy(self.policyNetwork.state_dict())\n        performanceTrain = np.zeros((trainingParameters[0], iterations))\n        performanceTest = np.zeros((trainingParameters[0], iterations))\n        marketSymbol = trainingEnv.marketSymbol\n        startingDate = trainingEnv.endingDate\n        endingDate = '2020-1-1'\n        money = trainingEnv.data['Money'][0]\n        stateLength = trainingEnv.stateLength\n        transactionCosts = trainingEnv.transactionCosts\n        testingEnv = TradingEnv(marketSymbol, startingDate, endingDate, money, stateLength, transactionCosts, features=features)\n        logging.info(\"Hardware selected for training: \" + str(self.device))\n        try:\n            for iteration in range(iterations):\n                logging.info(''.join([\"Expected performance evaluation progression: \", str(iteration+1), \"/\", str(iterations)]))\n                for episode in tqdm(range(trainingParameters[0])):\n                    for i in range(len(trainingEnvList)):\n                        coefficients = self.getNormalizationCoefficients(trainingEnvList[i])\n                        trainingEnvList[i].reset()\n                        startingPoint = random.randrange(len(trainingEnvList[i].data.index))\n                        trainingEnvList[i].setStartingPoint(startingPoint)\n                        state = self.processState(trainingEnvList[i].state, coefficients)\n                        previousAction = 0\n                        done = 0\n                        stepsCounter = 0\n                        while done == 0:\n                            action, _, _ = self.chooseActionEpsilonGreedy(state, previousAction)\n                            nextState, reward, done, info = trainingEnvList[i].step(action)\n                            reward = self.processReward(reward)\n                            nextState = self.processState(nextState, coefficients)\n                            self.replayMemory.push(state, action, reward, nextState, done)\n                            otherAction = int(not bool(action))\n                            otherReward = self.processReward(info['Reward'])\n                            otherDone = info['Done']\n                            otherNextState = self.processState(info['State'], coefficients)\n                            self.replayMemory.push(state, otherAction, otherReward, otherNextState, otherDone)\n                            stepsCounter += 1\n                            if stepsCounter == learningUpdatePeriod:\n                                self.learning()\n                                stepsCounter = 0\n                            state = nextState\n                            previousAction = action\n                    trainingEnv = self.testing(trainingEnv, trainingEnv)\n                    analyser = PerformanceEstimator(trainingEnv.data)\n                    performanceTrain[episode][iteration] = analyser.computeSharpeRatio()\n                    self.writer.add_scalar('Training performance (Sharpe Ratio)', performanceTrain[episode][iteration], episode)\n                    testingEnv = self.testing(trainingEnv, testingEnv)\n                    analyser = PerformanceEstimator(testingEnv.data)\n                    performanceTest[episode][iteration] = analyser.computeSharpeRatio()\n                    self.writer.add_scalar('Testing performance (Sharpe Ratio)', performanceTest[episode][iteration], episode)\n                if iteration < (iterations-1):\n                    trainingEnv.reset()\n                    testingEnv.reset()\n                    self.policyNetwork.load_state_dict(initialWeights)\n                    self.targetNetwork.load_state_dict(initialWeights)\n                    self.optimizer = optim.Adam(self.policyNetwork.parameters(), lr=learningRate, weight_decay=L2Factor)\n                    self.replayMemory.reset()\n                    self.iterations = 0\n                    stepsCounter = 0\n            iteration += 1\n        except KeyboardInterrupt:\n            logging.info()\n            logging.info(\"WARNING: Expected performance evaluation prematurely interrupted...\")\n            logging.info()\n            self.policyNetwork.eval()\n        expectedPerformanceTrain = []\n        expectedPerformanceTest = []\n        stdPerformanceTrain = []\n        stdPerformanceTest = []\n        for episode in range(trainingParameters[0]):\n            expectedPerformanceTrain.append(np.mean(performanceTrain[episode][:iteration]))\n            expectedPerformanceTest.append(np.mean(performanceTest[episode][:iteration]))\n            stdPerformanceTrain.append(np.std(performanceTrain[episode][:iteration]))\n            stdPerformanceTest.append(np.std(performanceTest[episode][:iteration]))\n        expectedPerformanceTrain = np.array(expectedPerformanceTrain)\n        expectedPerformanceTest = np.array(expectedPerformanceTest)\n        stdPerformanceTrain = np.array(stdPerformanceTrain)\n        stdPerformanceTest = np.array(stdPerformanceTest)\n        for i in range(iteration):\n            fig = plt.figure()\n            ax = fig.add_subplot(111, ylabel='Performance (Sharpe Ratio)', xlabel='Episode')\n            ax.plot([performanceTrain[e][i] for e in range(trainingParameters[0])])\n            ax.plot([performanceTest[e][i] for e in range(trainingParameters[0])])\n            ax.legend([\"Training\", \"Testing\"])\n            if savePlots:\n                plt.savefig(''.join(['images/', str(marketSymbol), '_TrainingTestingPerformance', str(i+1), '.png']))\n        fig = plt.figure()\n        ax = fig.add_subplot(111, ylabel='Performance (Sharpe Ratio)', xlabel='Episode')\n        ax.plot(expectedPerformanceTrain)\n        ax.plot(expectedPerformanceTest)\n        ax.fill_between(range(len(expectedPerformanceTrain)), expectedPerformanceTrain-stdPerformanceTrain, expectedPerformanceTrain+stdPerformanceTrain, alpha=0.25)\n        ax.fill_between(range(len(expectedPerformanceTest)), expectedPerformanceTest-stdPerformanceTest, expectedPerformanceTest+stdPerformanceTest, alpha=0.25)\n        ax.legend([\"Training\", \"Testing\"])\n        if savePlots:\n            plt.savefig(''.join(['images/', str(marketSymbol), '_TrainingTestingExpectedPerformance', '.png']))\n        self.writer.close()\n        return trainingEnv\n    def saveModel(self, fileName):\n        torch.save(self.policyNetwork.state_dict(), fileName)\n    def loadModel(self, fileName):\n        self.policyNetwork.load_state_dict(torch.load(fileName, map_location=self.device))\n        self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n    def plotEpsilonAnnealing(self, savePlots=False):\n        plt.figure()\n        plt.plot([self.epsilonValue(i) for i in range(10*epsilonDecay)])\n        plt.title(\"Plot Title\")\n        plt.xlabel(\"X-axis\")\n        plt.ylabel(\"Y-axis\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Epsilon value\")\n        if savePlots:\n            plt.savefig(''.join(['images/', 'EpsilonAnnealing', '.png']))\n\nclass PerformanceEstimator:\n    def __init__(self, tradingData):\n        self.data = tradingData\n    def computePnL(self):\n        self.PnL = self.data[\"Money\"][-1] - self.data[\"Money\"][0]\n        return self.PnL\n    def computeAnnualizedReturn(self):\n        cumulativeReturn = self.data['Returns'].cumsum()\n        cumulativeReturn = cumulativeReturn[-1]\n        start = self.data.index[0].to_pydatetime()\n        end = self.data.index[-1].to_pydatetime()\n        timeElapsed = end - start\n        timeElapsed = timeElapsed.days\n        if(cumulativeReturn > -1):\n            self.annualizedReturn = 100 * (((1 + cumulativeReturn) ** (365/timeElapsed)) - 1)\n        else:\n            self.annualizedReturn = -100\n        return self.annualizedReturn\n    def computeAnnualizedVolatility(self):\n        self.annualizedVolatily = 100 * np.sqrt(252) * self.data['Returns'].std()\n        return self.annualizedVolatily\n    def computeSharpeRatio(self, riskFreeRate=0):\n        expectedReturn = self.data['Returns'].mean()\n        volatility = self.data['Returns'].std()\n        if expectedReturn != 0 and volatility != 0:\n            self.sharpeRatio = np.sqrt(252) * (expectedReturn - riskFreeRate)/volatility\n        else:\n            self.sharpeRatio = 0\n        return self.sharpeRatio\n    def computeSortinoRatio(self, riskFreeRate=0):\n        expectedReturn = np.mean(self.data['Returns'])\n        negativeReturns = [returns for returns in self.data['Returns'] if returns < 0]\n        volatility = np.std(negativeReturns)\n        if expectedReturn != 0 and volatility != 0:\n            self.sortinoRatio = np.sqrt(252) * (expectedReturn - riskFreeRate)/volatility\n        else:\n            self.sortinoRatio = 0\n        return self.sortinoRatio\n    def computeMaxDrawdown(self, plotting=False, savePlots=False):\n        capital = self.data['Money'].values\n        through = np.argmax(np.maximum.accumulate(capital) - capital)\n        if through != 0:\n            peak = np.argmax(capital[:through])\n            self.maxDD = 100 * (capital[peak] - capital[through])/capital[peak]\n            self.maxDDD = through - peak\n        else:\n            self.maxDD = 0\n            self.maxDDD = 0\n            return self.maxDD, self.maxDDD\n        if plotting:\n            plt.figure(figsize=(10, 4))\n            plt.plot(self.data['Money'], lw=2, color='Blue')\n            plt.title(\"Plot Title\")\n            plt.xlabel(\"X-axis\")\n            plt.ylabel(\"Y-axis\")\n            plt.plot([self.data.iloc[[peak]].index, self.data.iloc[[through]].index],\n                     [capital[peak], capital[through]], 'o', color='Red', markersize=5)\n            plt.xlabel('Time')\n            plt.ylabel('Price')\n            if savePlots:\n                plt.savefig(''.join(['images/', 'MaximumDrawDown', '.png']))\n        return self.maxDD, self.maxDDD\n    def computeProfitability(self):\n        good = 0\n        bad = 0\n        profit = 0\n        loss = 0\n        index = next((i for i in range(len(self.data.index)) if self.data['Action'][i] != 0), None)\n        if index == None:\n            self.profitability = 0\n            self.averageProfitLossRatio = 0\n            return self.profitability, self.averageProfitLossRatio\n        money = self.data['Money'][index]\n        for i in range(index+1, len(self.data.index)):\n            if(self.data['Action'][i] != 0):\n                delta = self.data['Money'][i] - money\n                money = self.data['Money'][i]\n                if(delta >= 0):\n                    good += 1\n                    profit += delta\n                else:\n                    bad += 1\n                    loss -= delta\n        delta = self.data['Money'][-1] - money\n        if(delta >= 0):\n            good += 1\n            profit += delta\n        else:\n            bad += 1\n            loss -= delta\n        self.profitability = 100 * good/(good + bad)\n        if(good != 0):\n            profit /= good\n        if(bad != 0):\n            loss /= bad\n        if(loss != 0):\n            self.averageProfitLossRatio = profit/loss\n        else:\n            self.averageProfitLossRatio = float('Inf')\n        return self.profitability, self.averageProfitLossRatio\n    def computeSkewness(self):\n        self.skewness = self.data[\"Returns\"].skew()\n        return self.skewness\n    def computePerformance(self):\n        self.computePnL()\n        self.computeAnnualizedReturn()\n        self.computeAnnualizedVolatility()\n        self.computeProfitability()\n        self.computeSharpeRatio()\n        self.computeSortinoRatio()\n        self.computeMaxDrawdown()\n        self.computeSkewness()\n        self.performanceTable = [[\"Profit & Loss (P&L)\", \"{0:.0f}\".format(self.PnL)],\n                                 [\"Annualized Return\", \"{0:.2f}\".format(self.annualizedReturn) + '%'],\n                                 [\"Annualized Volatility\", \"{0:.2f}\".format(self.annualizedVolatily) + '%'],\n                                 [\"Sharpe Ratio\", \"{0:.3f}\".format(self.sharpeRatio)],\n                                 [\"Sortino Ratio\", \"{0:.3f}\".format(self.sortinoRatio)],\n                                 [\"Maximum Drawdown\", \"{0:.2f}\".format(self.maxDD) + '%'],\n                                 [\"Maximum Drawdown Duration\", \"{0:.0f}\".format(self.maxDDD) + ' days'],\n                                 [\"Profitability\", \"{0:.2f}\".format(self.profitability) + '%'],\n                                 [\"Ratio Average Profit/Loss\", \"{0:.3f}\".format(self.averageProfitLossRatio)],\n                                 [\"Skewness\", \"{0:.3f}\".format(self.skewness)]]\n        return self.performanceTable\n    def displayPerformance(self, name):\n        self.computePerformance()\n        headers = [\"Performance Indicator\", name]\n        tabulation = tabulate(self.performanceTable, headers, tablefmt=\"fancy_grid\", stralign=\"center\")\n        logging.info('\\n' + tabulation)","metadata":{"_uuid":"7adee86b-7e62-4b5c-9805-866897cf48ce","_cell_guid":"c7aa1260-54b4-4495-b3d7-b7772d2edf04","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}