{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Applied to Algorithmic Trading\n",
    "\n",
    "<a href=\"https://www.kaggle.com/addarm/unsupervised-learning-as-signals-for-pairs-trading\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRO\n",
    "\n",
    "\n",
    "This deep learning network was inspired by the paper:\n",
    "```BibTeX\n",
    "@article{theate2021application,\n",
    "  title={An application of deep reinforcement learning to algorithmic trading},\n",
    "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
    "  journal={Expert Systems with Applications},\n",
    "  volume={173},\n",
    "  pages={114632},\n",
    "  year={2021},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Local...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/adamd/workspace/deep-reinforced-learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install scikit-learn\n",
    "    %pip install tensorflow\n",
    "    %pip install tqdm\n",
    "    %pip install matplotlib\n",
    "    %pip install python-dotenv\n",
    "    %pip install yfinance\n",
    "    %pip install pyarrow\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
    "else:\n",
    "    DATA_DIR = \"./data/\"\n",
    "    print('Running Local...')\n",
    "\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import BDay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2017-01-01\"\n",
    "SPLIT_DATE = '2018-1-1' # Turning point from train to tst\n",
    "END_DATE = \"2019-12-31\" # pd.Timestamp(datetime.now() - BDay(1)).strftime('%Y-%m-%d')\n",
    "DATA_DIR = \"./data\"\n",
    "INDEX = \"Date\"\n",
    "TICKER_SYMBOLS = [\n",
    "    'DIA',  # Dow Jones\n",
    "    'SPY',  # S&P 500\n",
    "    'QQQ',  # NASDAQ 100\n",
    "    'EZU',  # FTSE 100\n",
    "    'EWJ',  # Nikkei 225\n",
    "    'GOOGL',  # Google\n",
    "    'AAPL',  # Apple\n",
    "    'META',  # Facebook\n",
    "    'AMZN',  # Amazon\n",
    "    'MSFT',  # Microsoft\n",
    "    'NOK',  # Nokia\n",
    "    'PHIA.AS',  # Philips\n",
    "    'SIE.DE',  # Siemens\n",
    "    'BIDU',  # Baidu\n",
    "    'BABA',  # Alibaba\n",
    "    '0700.HK',  # Tencent\n",
    "    '6758.T',  # Sony\n",
    "    'JPM',  # JPMorgan Chase\n",
    "    'HSBC',  # HSBC\n",
    "    '0939.HK',  # CCB\n",
    "    'XOM',  # ExxonMobil\n",
    "    'TSLA',  # Tesla\n",
    "    'VOW3.DE',  # Volkswagen\n",
    "    '7203.T',  # Toyota\n",
    "    'KO',  # Coca Cola\n",
    "    'ABI.BR',  # AB InBev\n",
    "    '2503.T',  # Kirin\n",
    "]\n",
    "TICKER_SYMBOLS = ['TSLA']\n",
    "TARGET = 'TSLA'\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "CAPITAL = 1000\n",
    "STATE_LEN = 30\n",
    "FEES = 0.1 / 100\n",
    "FEATURES = 4 # 4 dims: HLOC\n",
    "OBS_SPACE = (STATE_LEN)*FEATURES\n",
    "ACT_SPACE = 2\n",
    "EPISODES = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA => min_date: 2017-01-03 00:00:00, max_date: 2019-12-30 00:00:00, kurt:-0.56, skewness:-0.28, outliers_count:0,  nan_count: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>14.324000</td>\n",
       "      <td>14.688667</td>\n",
       "      <td>14.064000</td>\n",
       "      <td>14.466000</td>\n",
       "      <td>14.466000</td>\n",
       "      <td>88849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>14.316667</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>14.287333</td>\n",
       "      <td>15.132667</td>\n",
       "      <td>15.132667</td>\n",
       "      <td>168202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>15.094667</td>\n",
       "      <td>15.165333</td>\n",
       "      <td>14.796667</td>\n",
       "      <td>15.116667</td>\n",
       "      <td>15.116667</td>\n",
       "      <td>88675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>15.128667</td>\n",
       "      <td>15.354000</td>\n",
       "      <td>15.030000</td>\n",
       "      <td>15.267333</td>\n",
       "      <td>15.267333</td>\n",
       "      <td>82918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <td>15.264667</td>\n",
       "      <td>15.461333</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>15.418667</td>\n",
       "      <td>15.418667</td>\n",
       "      <td>59692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>27.452000</td>\n",
       "      <td>28.134001</td>\n",
       "      <td>27.333332</td>\n",
       "      <td>27.948000</td>\n",
       "      <td>27.948000</td>\n",
       "      <td>199794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>27.890667</td>\n",
       "      <td>28.364668</td>\n",
       "      <td>27.512667</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>120820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>28.527332</td>\n",
       "      <td>28.898666</td>\n",
       "      <td>28.423332</td>\n",
       "      <td>28.729334</td>\n",
       "      <td>28.729334</td>\n",
       "      <td>159508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.020666</td>\n",
       "      <td>28.407333</td>\n",
       "      <td>28.691999</td>\n",
       "      <td>28.691999</td>\n",
       "      <td>149185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>28.586000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>27.284000</td>\n",
       "      <td>27.646667</td>\n",
       "      <td>27.646667</td>\n",
       "      <td>188796000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume\n",
       "Date                                                                        \n",
       "2017-01-03  14.324000  14.688667  14.064000  14.466000  14.466000   88849500\n",
       "2017-01-04  14.316667  15.200000  14.287333  15.132667  15.132667  168202500\n",
       "2017-01-05  15.094667  15.165333  14.796667  15.116667  15.116667   88675500\n",
       "2017-01-06  15.128667  15.354000  15.030000  15.267333  15.267333   82918500\n",
       "2017-01-09  15.264667  15.461333  15.200000  15.418667  15.418667   59692500\n",
       "...               ...        ...        ...        ...        ...        ...\n",
       "2019-12-23  27.452000  28.134001  27.333332  27.948000  27.948000  199794000\n",
       "2019-12-24  27.890667  28.364668  27.512667  28.350000  28.350000  120820500\n",
       "2019-12-26  28.527332  28.898666  28.423332  28.729334  28.729334  159508500\n",
       "2019-12-27  29.000000  29.020666  28.407333  28.691999  28.691999  149185500\n",
       "2019-12-30  28.586000  28.600000  27.284000  27.646667  27.646667  188796000\n",
       "\n",
       "[753 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, datadir=DATA_DIR):\n",
    "    tickers = {}\n",
    "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
    "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    for symbol in tickers_symbols:\n",
    "        cached_file_path = f\"{datadir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(cached_file_path):\n",
    "                df = pd.read_parquet(cached_file_path)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                assert len(df) > 0\n",
    "            else:\n",
    "                df = yf.download(\n",
    "                    symbol,\n",
    "                    start=START_DATE,\n",
    "                    end=END_DATE,\n",
    "                    progress=False,\n",
    "                    interval=INTERVAL,\n",
    "                )\n",
    "                assert len(df) > 0\n",
    "                df.to_parquet(cached_file_path, index=True, compression=\"snappy\")\n",
    "            min_date = df.index.min()\n",
    "            max_date = df.index.max()\n",
    "            nan_count = df[\"Close\"].isnull().sum()\n",
    "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
    "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
    "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
    "            print(\n",
    "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
    "            )\n",
    "            tickers[symbol] = df\n",
    "\n",
    "            if min_date > latest_start:\n",
    "                latest_start = min_date\n",
    "            if max_date < earliest_end:\n",
    "                earliest_end = max_date\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "    return tickers, latest_start, earliest_end\n",
    "\n",
    "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
    "tickers[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 17:39:51.011466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-08 17:39:51.011524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-08 17:39:51.012463: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-08 17:39:51.019976: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 17:39:52.088158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import py_environment, utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "import numpy as np\n",
    "\n",
    "ACT_LONG = 1\n",
    "ACT_SHORT = -1\n",
    "ACT_HOLD = 0\n",
    "\n",
    "\n",
    "class TradingEnv(py_environment.PyEnvironment):\n",
    "    \"\"\"\n",
    "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
    "\n",
    "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
    "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the stock market data.\n",
    "    - data_dim: Dimension of the data to be used for each observation.\n",
    "    - money: Initial capital to start trading.\n",
    "    - stateLength: Number of past observations to consider for the state.\n",
    "    - transactionCosts: Costs associated with trading actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, data_dim, money, stateLength, transactionCosts):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = self.preprocess_data(data)\n",
    "        self.data_dim = data_dim\n",
    "\n",
    "        self.initial_balance = money\n",
    "        self.state_length = stateLength\n",
    "        self.transaction_cost = transactionCosts\n",
    "        self._episode_ended = False\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(self.state_length, self.data_dim), dtype=np.float32, name='observation')\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        log_returns = np.log(df / df.shift(1))\n",
    "        normalized_data = (log_returns - log_returns.mean()) / log_returns.std()\n",
    "        normalized_data.dropna(inplace=True)\n",
    "        return normalized_data\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Provides the specification of the action space.\"\"\"\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.total_shares = 0\n",
    "        self.current_step = self.state_length\n",
    "        self._episode_ended = False\n",
    "        initial_observation = self._next_observation()\n",
    "        return ts.restart(initial_observation)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \"\"\"Generates the next observation based on the current step.\"\"\"\n",
    "        if self.current_step + self.state_length > len(self.data):\n",
    "            padding_rows_needed = max(len(self.data) - (self.current_step + self.state_length), 0)\n",
    "            frame = self.data.iloc[-self.state_length:] if padding_rows_needed == 0 else self.data.iloc[-padding_rows_needed:]\n",
    "            padding = np.zeros((padding_rows_needed, self.data_dim))\n",
    "            obs = np.vstack((padding, frame[['Close', 'Low', 'High', 'Volume']].values))\n",
    "        else:\n",
    "            frame = self.data.iloc[self.current_step-self.state_length:self.current_step]\n",
    "            obs = frame[['Close', 'Low', 'High', 'Volume']].values\n",
    "\n",
    "        obs = np.array(obs, dtype=np.float32)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Executes a trading action and returns the new state of the environment.\"\"\"\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if action == ACT_SHORT or action == ACT_LONG:\n",
    "            if self.total_shares > 0:\n",
    "                self.balance += self.total_shares * current_price * (1 - self.transaction_cost)\n",
    "                reward = self.balance - self.initial_balance\n",
    "                self.total_shares = 0\n",
    "\n",
    "        if action == ACT_LONG:\n",
    "            self.position = 1\n",
    "            self.total_shares = self.balance // (current_price * (1 + self.transaction_cost))\n",
    "            self.balance -= self.total_shares * current_price * (1 + self.transaction_cost)\n",
    "        elif action == ACT_SHORT:\n",
    "            self.position = -1\n",
    "\n",
    "        done = self.current_step >= len(self.data)\n",
    "        if done:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self._next_observation(), reward)\n",
    "        else:\n",
    "            return ts.transition(self._next_observation(), reward, discount=1.0)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Outputs the current state of the environment for visualization.\"\"\"\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}')\n",
    "\n",
    "environment = TradingEnv(tickers[TARGET], FEATURES, CAPITAL, STATE_LEN, FEES)\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(\n",
      "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(30, 4), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))})\n",
      "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(-1, dtype=int32), maximum=array(1, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(environment)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Architecure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "2 models:\n",
    "- Policy Model: This is the primary model that the agent uses to make decisions or select actions based on the current state of the environment. The policy model is actively trained and updated throughout the training process based on the agent's experiences. In real-life applications, after the training phase is complete, the policy model is what gets deployed to make decisions or take actions in the given environment.\n",
    "- Target Model: The target model is used exclusively during the training phase to provide a stable target for the temporal difference (TD) error calculation, which is crucial for the stability of the Q-learning updates. The target model's weights are periodically synchronized with the policy model's weights but at a much slower rate. This delayed update helps to stabilize the learning process by making the target for the policy updates more consistent across training batches. The target model itself is not used for decision-making or action selection outside of the training context.\n",
    "\n",
    "Some notes on this 2 model arch:\n",
    "- Stability/Reducing Temporal Correlations: The agent learns a policy that maps states to actions by using a Q-function. This Q-function estimates the rewards by taking a certain action in a given state. The learning process continuously updates the Q-values based on new experiences. If the Q-function is constantly changing—as it would be when updates are made based on estimates from the same function—it can lead to unstable training dynamics. The estimates can become overly optimistic, and the learning process can diverge.\n",
    "- Target: The target network is a stable baseline for the policy network to compare against. While the policy network is frequently updated to reflect the latest learning, the target network's weights are updated less frequently. This slower update rate provides a fixed target for the policy network to aim for over multiple iterations, making the learning process more stable.\n",
    "\n",
    "In practice, the policy network is responsible for selecting actions during training and gameplay. Its weights are regularly updated to reflect the agent's learning. The target network, on the other hand, is used to generate the Q-value targets for the updates of the policy network. Every few steps, the weights from the policy network are copied to the target network, ensuring the target for the updates remains relatively stable but still gradually adapts to the improved policy. The policy model is used both during training (for learning and decision-making) and after training (for decision-making in the deployment environment).\n",
    "\n",
    "The target model is used during the training process only, to calculate stable target values for updating the policy model.\n",
    "After training is complete and the model is deployed in a real-world application, only the policy model is used to make decisions or take actions based on the learned policy. The target model's role ends with the completion of the training phase, as its primary purpose is to aid in the convergence and stability of the training process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRL Flow\n",
    "\n",
    "- Initialization: init policy network and the target network with the same architecture but separate parameters.\n",
    "- Data Preparation: Normalize input data using calculated coefficients to ensure consistency in scale.\n",
    "- Learning Process:\n",
    "    -At each training step, observe the current market state and process it through normalization.\n",
    "    - Select an action using the epsilon-greedy policy (a balance between exploration and exploitation) based on the current state.\n",
    "    - Execute the selected action in the simulated trading environment, observe the next state, and receive a reward based on the action's outcome.\n",
    "    - Store the experience (current state, action, reward, next state) in the replay memory.\n",
    "    - Sample a random batch of past experiences from the replay memory for learning to reduce correlation between consecutive learning steps.\n",
    "    - Use the policy network to predict Q-values for the current states and the target network to calculate the target Q-values for the next states.\n",
    "    - Update the policy network by minimizing the difference between its Q-value predictions and the target Q-values using backpropagation.\n",
    "    - Every few steps, update the target network's parameters with the policy network's parameters to gradually adapt the learning target.\n",
    "- Evaluation and Adjustment: Periodically test the trained policy network on a separate validation set or environment to evaluate performance.\n",
    "    -Repeat the learning and evaluation process for many episodes until the policy network stabilizes and performs satisfactorily.\n",
    "- Application Phase\n",
    "    - Model Deployment: Deploy the trained policy network in a real-world environment or a simulation that closely mimics real trading conditions.\n",
    "    - Real-time Operation: Observe the current market state and process it (normalization, etc.) as done during training.\n",
    "    - Use the trained policy network to select the action that maximizes expected rewards based on the current market state, leaning towards exploitation of the learned policy over exploration. Execute the selected action in the market (buy, sell, hold).\n",
    "- Continuous Learning:\n",
    "    - Repeat the learning process with new market data and experiences, possibly in a less frequent, offline manner.\n",
    "    - Update the policy and target networks as new data becomes available and as the market evolves to maintain or improve performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 17:39:57.809931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:57.861154: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:57.861235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQNModel(Sequential):\n",
    "    def __init__(self, observation_space, action_space, learning_rate=0.0001):\n",
    "        super().__init__()\n",
    "        self.add(Dense(512, activation='relu', input_shape=(observation_space,)))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Dropout(0.2))\n",
    "        self.add(Dense(512, activation='relu'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Dropout(0.2))\n",
    "        self.add(Dense(action_space, activation='linear'))\n",
    "        self.compile(loss='mean_squared_error', optimizer=Adam(learning_rate))\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, observation_space, action_space, replay_memory_size=10000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, target_update_iter=1000):\n",
    "        self.action_space = action_space\n",
    "        self.memory = ReplayMemory(replay_memory_size)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_iter = target_update_iter\n",
    "        self.iteration = 0\n",
    "\n",
    "        self.policy_model = DQNModel(observation_space, action_space, learning_rate)\n",
    "        self.target_model = DQNModel(observation_space, action_space, learning_rate)\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())\n",
    "\n",
    "        self.callbacks = [\n",
    "            EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
    "        ]\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        state = np.array(state).flatten()[None, :]\n",
    "\n",
    "        q_values = self.policy_model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array([s.flatten() for s in states])\n",
    "        next_states = np.array([ns.flatten() for ns in next_states])\n",
    "\n",
    "        not_dones = np.array([1 - done for done in dones])\n",
    "        target_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        targets = rewards + self.gamma * np.max(target_q_values, axis=1) * not_dones\n",
    "        q_values = self.policy_model.predict(states, verbose=0)\n",
    "        for i, action in enumerate(actions):\n",
    "            q_values[i][action] = targets[i]\n",
    "\n",
    "        history = self.policy_model.fit(states, q_values, epochs=1, verbose=0, callbacks=self.callbacks)\n",
    "        loss = history.history['loss'][0]\n",
    "\n",
    "        self.adapt_epsilon()\n",
    "        self.iteration += 1\n",
    "\n",
    "        if self.iteration % self.target_update_iter == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def reset_episode_stats(self):\n",
    "        self.total_reward = 0\n",
    "        self.total_loss = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "\n",
    "    def adapt_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        self.policy_model.save(file_name)\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        self.policy_model = load_model(file_name)\n",
    "        self.target_model.set_weights(self.policy_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingSimulator:\n",
    "    def __init__(self, env, agent, episodes, checkpoint_interval):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.episodes = episodes\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "\n",
    "    def train(self):\n",
    "        episode_rewards = []\n",
    "        episode_losses = []\n",
    "        for episode in tqdm(range(0, self.episodes + 1), desc=\"Training\"):\n",
    "            self.agent.reset_episode_stats()\n",
    "            time_step = self.env.reset()\n",
    "            state = time_step.observation\n",
    "            done = time_step.is_last()\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.agent.act(state)\n",
    "                time_step = self.env.step(action)\n",
    "                next_state = time_step.observation\n",
    "                reward = time_step.reward\n",
    "                done = time_step.is_last()\n",
    "\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                loss = self.agent.replay()\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                total_loss += loss\n",
    "\n",
    "            avg_loss = total_loss / episode if episode > 0 else 0\n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_losses.append(avg_loss)\n",
    "\n",
    "            if episode > 0 and episode % self.checkpoint_interval == 0:\n",
    "                self.agent.save_model(f'dqn_model_checkpoint_{episode}.h5')\n",
    "                print(f\"\\nCheckpoint saved at episode {episode}\")\n",
    "\n",
    "            print(f'\\nEpisode {episode}: Total Reward: {total_reward}, Avg Loss: {avg_loss:.4f}, Epsilon: {self.agent.epsilon:.4f}')\n",
    "\n",
    "            if episode > 0 and episode % self.agent.target_update_iter == 0:\n",
    "                self.agent.update_target_model()\n",
    "\n",
    "        print(f'\\nTraining completed. Mean Reward: {np.mean(episode_rewards):.4f}, Mean Loss: {np.mean(episode_losses):.4f}')\n",
    "\n",
    "    def validate(self):\n",
    "        total_rewards = []\n",
    "        for episode in tqdm(range(1, self.episodes + 1), desc=\"Validation\"):\n",
    "            self.agent.reset_episode_stats()\n",
    "            time_step = self.env.reset()\n",
    "            state = time_step.observation\n",
    "            done = time_step.is_last()\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.agent.act(state)\n",
    "                time_step = self.env.step(action)\n",
    "                next_state = time_step.observation\n",
    "                reward = time_step.reward\n",
    "                done = time_step.is_last()\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f'\\nValidation Episode: {episode}, Total Reward: {total_reward}')\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f'\\nAverage Reward Over Validation Episodes: {avg_reward:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 17:39:57.943907: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:57.944396: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:57.944770: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:58.183425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:58.183665: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:58.183690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-08 17:39:58.183874: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 17:39:58.183914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2246 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "Training:   0%|          | 0/3 [00:00<?, ?it/s]2024-03-08 17:39:58.885043: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-08 17:40:00.934826: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ff7b9917710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-08 17:40:00.934876: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-03-08 17:40:00.940652: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-08 17:40:00.957159: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709916001.041144 3242519 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Training:  33%|███▎      | 1/3 [00:35<01:11, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 0: Total Reward: -41815.60546875, Avg Loss: 0.0000, Epsilon: 0.4552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [01:29<00:46, 46.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1: Total Reward: -66971.28991699219, Avg Loss: 11435772.0371, Epsilon: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [02:24<00:00, 48.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 2: Total Reward: -39966.41552734375, Avg Loss: 5507418.9102, Epsilon: 0.0502\n",
      "\n",
      "Training completed. Mean Reward: -49584.4370, Mean Loss: 5647730.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:24<00:00, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Episode: 1, Total Reward: -8006.8603515625\n",
      "\n",
      "Average Reward Over Validation Episodes: -8006.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stock= tickers[TARGET]\n",
    "train_data = stock[stock.index < pd.to_datetime(SPLIT_DATE)].copy()\n",
    "test_data = stock[stock.index >= pd.to_datetime(SPLIT_DATE)].copy()\n",
    "\n",
    "agent = DQNAgent(observation_space=STATE_LEN * FEATURES, action_space=ACT_SPACE)\n",
    "\n",
    "env = TradingEnv(data=train_data, data_dim=FEATURES, money=CAPITAL, stateLength=STATE_LEN, transactionCosts=FEES)\n",
    "simulator = TradingSimulator(env, agent, episodes=2, checkpoint_interval=100)\n",
    "simulator.train()\n",
    "\n",
    "env = TradingEnv(data=test_data, data_dim=FEATURES, money=CAPITAL, stateLength=STATE_LEN, transactionCosts=FEES)\n",
    "simulator = TradingSimulator(env, agent, episodes=1, checkpoint_interval=100)\n",
    "simulator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "CONCLUDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
    "- [Open Gym AI Github](https://github.com/openai/gym)\n",
    "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
    "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)\n",
    "- [Remote development in WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)\n",
    "- [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\n",
    "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "- [TensorRT for CUDA](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github\n",
    "\n",
    "Article here is also available on [Github](https://github.com/adamd1985/pairs_trading_unsupervised_learning)\n",
    "\n",
    "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/unsupervised-learning-as-signals-for-pairs-trading)\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
