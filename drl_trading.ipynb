{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Applied to Algorithmic Trading\n",
    "\n",
    "<a href=\"https://www.kaggle.com/addarm/unsupervised-learning-as-signals-for-pairs-trading\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRO\n",
    "\n",
    "\n",
    "This deep learning network was inspired by the paper:\n",
    "```BibTeX\n",
    "@article{theate2021application,\n",
    "  title={An application of deep reinforcement learning to algorithmic trading},\n",
    "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
    "  journal={Expert Systems with Applications},\n",
    "  volume={173},\n",
    "  pages={114632},\n",
    "  year={2021},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Local...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/adamd/workspace/deep-reinforced-learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle confgs\n",
    "    print('Running in Kaggle...')\n",
    "    %pip install scikit-learn\n",
    "    %pip install tensorflow\n",
    "    %pip install tqdm\n",
    "    %pip install matplotlib\n",
    "    %pip install python-dotenv\n",
    "    %pip install yfinance\n",
    "    %pip install pyarrow\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
    "else:\n",
    "    DATA_DIR = \"./data/\"\n",
    "    print('Running Local...')\n",
    "\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import BDay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2017-01-01\"\n",
    "SPLIT_DATE = '2018-1-1' # Turning point from train to tst\n",
    "END_DATE = \"2019-12-31\" # pd.Timestamp(datetime.now() - BDay(1)).strftime('%Y-%m-%d')\n",
    "DATA_DIR = \"./data\"\n",
    "INDEX = \"Date\"\n",
    "TICKER_SYMBOLS = [\n",
    "    'DIA',  # Dow Jones\n",
    "    'SPY',  # S&P 500\n",
    "    'QQQ',  # NASDAQ 100\n",
    "    'EZU',  # FTSE 100\n",
    "    'EWJ',  # Nikkei 225\n",
    "    'GOOGL',  # Google\n",
    "    'AAPL',  # Apple\n",
    "    'META',  # Facebook\n",
    "    'AMZN',  # Amazon\n",
    "    'MSFT',  # Microsoft\n",
    "    'NOK',  # Nokia\n",
    "    'PHIA.AS',  # Philips\n",
    "    'SIE.DE',  # Siemens\n",
    "    'BIDU',  # Baidu\n",
    "    'BABA',  # Alibaba\n",
    "    '0700.HK',  # Tencent\n",
    "    '6758.T',  # Sony\n",
    "    'JPM',  # JPMorgan Chase\n",
    "    'HSBC',  # HSBC\n",
    "    '0939.HK',  # CCB\n",
    "    'XOM',  # ExxonMobil\n",
    "    'TSLA',  # Tesla\n",
    "    'VOW3.DE',  # Volkswagen\n",
    "    '7203.T',  # Toyota\n",
    "    'KO',  # Coca Cola\n",
    "    'ABI.BR',  # AB InBev\n",
    "    '2503.T',  # Kirin\n",
    "]\n",
    "TICKER_SYMBOLS = ['TSLA']\n",
    "TARGET = 'TSLA'\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "CAPITAL = 100000\n",
    "STATE_LEN = 30\n",
    "FEES = 0.1 / 100\n",
    "FEATURES = 4 # 4 dims: HLOC\n",
    "OBS_SPACE = (STATE_LEN)*FEATURES\n",
    "ACT_SPACE = 2\n",
    "EPISODES = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA => min_date: 1970-01-01 00:00:00, max_date: 1970-01-01 00:00:00.000000752, kurt:-0.56, skewness:-0.28, outliers_count:0,  nan_count: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000000</th>\n",
       "      <td>14.324000</td>\n",
       "      <td>14.688667</td>\n",
       "      <td>14.064000</td>\n",
       "      <td>14.466000</td>\n",
       "      <td>14.466000</td>\n",
       "      <td>88849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000001</th>\n",
       "      <td>14.316667</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>14.287333</td>\n",
       "      <td>15.132667</td>\n",
       "      <td>15.132667</td>\n",
       "      <td>168202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000002</th>\n",
       "      <td>15.094667</td>\n",
       "      <td>15.165333</td>\n",
       "      <td>14.796667</td>\n",
       "      <td>15.116667</td>\n",
       "      <td>15.116667</td>\n",
       "      <td>88675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000003</th>\n",
       "      <td>15.128667</td>\n",
       "      <td>15.354000</td>\n",
       "      <td>15.030000</td>\n",
       "      <td>15.267333</td>\n",
       "      <td>15.267333</td>\n",
       "      <td>82918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000004</th>\n",
       "      <td>15.264667</td>\n",
       "      <td>15.461333</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>15.418667</td>\n",
       "      <td>15.418667</td>\n",
       "      <td>59692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000748</th>\n",
       "      <td>27.452000</td>\n",
       "      <td>28.134001</td>\n",
       "      <td>27.333332</td>\n",
       "      <td>27.948000</td>\n",
       "      <td>27.948000</td>\n",
       "      <td>199794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000749</th>\n",
       "      <td>27.890667</td>\n",
       "      <td>28.364668</td>\n",
       "      <td>27.512667</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>28.350000</td>\n",
       "      <td>120820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000750</th>\n",
       "      <td>28.527332</td>\n",
       "      <td>28.898666</td>\n",
       "      <td>28.423332</td>\n",
       "      <td>28.729334</td>\n",
       "      <td>28.729334</td>\n",
       "      <td>159508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000751</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.020666</td>\n",
       "      <td>28.407333</td>\n",
       "      <td>28.691999</td>\n",
       "      <td>28.691999</td>\n",
       "      <td>149185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000752</th>\n",
       "      <td>28.586000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>27.284000</td>\n",
       "      <td>27.646667</td>\n",
       "      <td>27.646667</td>\n",
       "      <td>188796000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Open       High        Low      Close  \\\n",
       "1970-01-01 00:00:00.000000000  14.324000  14.688667  14.064000  14.466000   \n",
       "1970-01-01 00:00:00.000000001  14.316667  15.200000  14.287333  15.132667   \n",
       "1970-01-01 00:00:00.000000002  15.094667  15.165333  14.796667  15.116667   \n",
       "1970-01-01 00:00:00.000000003  15.128667  15.354000  15.030000  15.267333   \n",
       "1970-01-01 00:00:00.000000004  15.264667  15.461333  15.200000  15.418667   \n",
       "...                                  ...        ...        ...        ...   \n",
       "1970-01-01 00:00:00.000000748  27.452000  28.134001  27.333332  27.948000   \n",
       "1970-01-01 00:00:00.000000749  27.890667  28.364668  27.512667  28.350000   \n",
       "1970-01-01 00:00:00.000000750  28.527332  28.898666  28.423332  28.729334   \n",
       "1970-01-01 00:00:00.000000751  29.000000  29.020666  28.407333  28.691999   \n",
       "1970-01-01 00:00:00.000000752  28.586000  28.600000  27.284000  27.646667   \n",
       "\n",
       "                               Adj Close     Volume  \n",
       "1970-01-01 00:00:00.000000000  14.466000   88849500  \n",
       "1970-01-01 00:00:00.000000001  15.132667  168202500  \n",
       "1970-01-01 00:00:00.000000002  15.116667   88675500  \n",
       "1970-01-01 00:00:00.000000003  15.267333   82918500  \n",
       "1970-01-01 00:00:00.000000004  15.418667   59692500  \n",
       "...                                  ...        ...  \n",
       "1970-01-01 00:00:00.000000748  27.948000  199794000  \n",
       "1970-01-01 00:00:00.000000749  28.350000  120820500  \n",
       "1970-01-01 00:00:00.000000750  28.729334  159508500  \n",
       "1970-01-01 00:00:00.000000751  28.691999  149185500  \n",
       "1970-01-01 00:00:00.000000752  27.646667  188796000  \n",
       "\n",
       "[753 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, datadir=DATA_DIR):\n",
    "    tickers = {}\n",
    "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
    "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    for symbol in tickers_symbols:\n",
    "        cached_file_path = f\"{datadir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(cached_file_path):\n",
    "                df = pd.read_parquet(cached_file_path)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                assert len(df) > 0\n",
    "            else:\n",
    "                df = yf.download(\n",
    "                    symbol,\n",
    "                    start=START_DATE,\n",
    "                    end=END_DATE,\n",
    "                    progress=False,\n",
    "                    interval=INTERVAL,\n",
    "                )\n",
    "                assert len(df) > 0\n",
    "                df.to_parquet(cached_file_path, index=False, compression=\"snappy\")\n",
    "            min_date = df.index.min()\n",
    "            max_date = df.index.max()\n",
    "            nan_count = df[\"Close\"].isnull().sum()\n",
    "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
    "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
    "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
    "            print(\n",
    "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
    "            )\n",
    "            tickers[symbol] = df\n",
    "\n",
    "            if min_date > latest_start:\n",
    "                latest_start = min_date\n",
    "            if max_date < earliest_end:\n",
    "                earliest_end = max_date\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "    return tickers, latest_start, earliest_end\n",
    "\n",
    "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
    "tickers[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 03:10:54.561099: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-08 03:10:54.595139: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-08 03:10:54.595173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-08 03:10:54.596211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-08 03:10:54.601707: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-08 03:10:54.602224: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 03:10:55.452353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import py_environment, utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "import numpy as np\n",
    "\n",
    "ACT_LONG = 1\n",
    "ACT_SHORT = -1\n",
    "ACT_HOLD = 0\n",
    "\n",
    "\n",
    "class TradingEnv(py_environment.PyEnvironment):\n",
    "    \"\"\"\n",
    "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
    "\n",
    "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
    "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the stock market data.\n",
    "    - data_dim: Dimension of the data to be used for each observation.\n",
    "    - startingDate: The starting date of the trading period.\n",
    "    - money: Initial capital to start trading.\n",
    "    - stateLength: Number of past observations to consider for the state.\n",
    "    - transactionCosts: Costs associated with trading actions.\n",
    "    - endingDate: The ending date of the trading period.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, data_dim, startingDate, money, stateLength, transactionCosts, endingDate):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.data_dim = data_dim\n",
    "        self.startingDate = startingDate\n",
    "        self.initial_balance = money\n",
    "        self.state_length = stateLength\n",
    "        self.transaction_cost = transactionCosts\n",
    "        self.endingDate = endingDate\n",
    "        self._episode_ended = False\n",
    "\n",
    "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(shape=(self.state_length, self.data_dim), dtype=np.float32, name='observation')\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Provides the specification of the action space.\"\"\"\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.total_shares = 0\n",
    "        self.current_step = self.state_length\n",
    "        self._episode_ended = False\n",
    "        initial_observation = self._next_observation()\n",
    "        return ts.restart(initial_observation)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \"\"\"Generates the next observation based on the current step.\"\"\"\n",
    "        if self.current_step + self.state_length > len(self.data):\n",
    "            padding_rows_needed = max(len(self.data) - (self.current_step + self.state_length), 0)\n",
    "            frame = self.data.iloc[-self.state_length:] if padding_rows_needed == 0 else self.data.iloc[-padding_rows_needed:]\n",
    "            padding = np.zeros((padding_rows_needed, self.data_dim))\n",
    "            obs = np.vstack((padding, frame[['Close', 'Low', 'High', 'Volume']].values))\n",
    "        else:\n",
    "            frame = self.data.iloc[self.current_step-self.state_length:self.current_step]\n",
    "            obs = frame[['Close', 'Low', 'High', 'Volume']].values\n",
    "\n",
    "        obs = np.array(obs, dtype=np.float32)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Executes a trading action and returns the new state of the environment.\"\"\"\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        if action == ACT_SHORT or action == ACT_LONG:\n",
    "            if self.total_shares > 0:\n",
    "                self.balance += self.total_shares * current_price * (1 - self.transaction_cost)\n",
    "                reward = self.balance - self.initial_balance\n",
    "                self.total_shares = 0\n",
    "\n",
    "        if action == ACT_LONG:\n",
    "            self.position = 1\n",
    "            self.total_shares = self.balance // (current_price * (1 + self.transaction_cost))\n",
    "            self.balance -= self.total_shares * current_price * (1 + self.transaction_cost)\n",
    "        elif action == ACT_SHORT:\n",
    "            self.position = -1\n",
    "\n",
    "        done = self.current_step >= len(self.data)\n",
    "        if done:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self._next_observation(), reward)\n",
    "        else:\n",
    "            return ts.transition(self._next_observation(), reward, discount=1.0)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Outputs the current state of the environment for visualization.\"\"\"\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}')\n",
    "\n",
    "environment = TradingEnv(tickers[TARGET], FEATURES, START_DATE, CAPITAL, STATE_LEN, FEES, END_DATE)\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "TimeStep Specs: TimeStep(\n",
      "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': BoundedTensorSpec(shape=(30, 4), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))})\n",
      "Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(-1, dtype=int32), maximum=array(1, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(environment)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network Architecure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# Replay Memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# DQN Model\n",
    "class DQNModel(Sequential):\n",
    "    def __init__(self, flattened_observation_space, action_space, learning_rate=0.0001):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.add(Dense(512, activation='relu', input_shape=(flattened_observation_space,)))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Dropout(0.2))\n",
    "        self.add(Dense(512, activation='relu'))\n",
    "        self.add(BatchNormalization())\n",
    "        self.add(Dropout(0.2))\n",
    "        self.add(Dense(action_space, activation='linear'))\n",
    "        self.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, observation_space, action_space, replay_memory_size=10000, batch_size=64, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995, learning_rate=0.001, target_update_iter=100):\n",
    "        self.action_space = action_space\n",
    "        self.memory = ReplayMemory(replay_memory_size)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_iter = target_update_iter\n",
    "        self.model = DQNModel(observation_space, action_space, learning_rate)\n",
    "        self.target_model = DQNModel(observation_space, action_space, learning_rate)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.iteration = 0\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(np.array([state.flatten()]), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states = np.array([x[0].flatten() for x in batch])\n",
    "        actions = np.array([x[1] for x in batch])\n",
    "        rewards = np.array([x[2] for x in batch])\n",
    "        next_states = np.array([x[3].flatten() for x in batch])\n",
    "        dones = np.array([x[4] for x in batch])\n",
    "\n",
    "        not_dones = 1 - dones\n",
    "        ns_values = self.target_model.predict(next_states, verbose=0)\n",
    "        q_update = rewards + self.gamma * np.amax(ns_values, axis=1) * not_dones\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "\n",
    "        for i, action in enumerate(actions):\n",
    "            q_values[i][action] = q_update[i]\n",
    "\n",
    "        self.model.fit(states, q_values, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        self.iteration += 1\n",
    "        if self.iteration % self.target_update_iter == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        self.model.save_weights(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingSimulator:\n",
    "    def __init__(self, env, agent, training_episodes, validation_episodes, checkpoint_interval):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.training_episodes = training_episodes\n",
    "        self.validation_episodes = validation_episodes\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "\n",
    "    def train(self):\n",
    "        for episode in tqdm(range(1, self.training_episodes + 1), desc=\"train\"):\n",
    "            time_step = self.env.reset()\n",
    "            state = time_step.observation\n",
    "            done = time_step.is_last()\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.agent.act(state)\n",
    "                time_step = self.env.step(action)\n",
    "                next_state = time_step.observation\n",
    "                reward = time_step.reward\n",
    "                done = time_step.is_last()\n",
    "\n",
    "                self.agent.remember(state, action, reward, next_state, done)\n",
    "                self.agent.replay()\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            if episode % self.checkpoint_interval == 0:\n",
    "                self.agent.save_model(f'dqn_model_checkpoint_{episode}.h5')\n",
    "                print(f\"Checkpoint saved at episode {episode}\")\n",
    "\n",
    "            print(f'Training Episode: {episode}, Total Reward: {total_reward}')\n",
    "\n",
    "            if episode % self.agent.target_update_iter == 0:\n",
    "                self.agent.update_target_model()\n",
    "\n",
    "    def validate(self):\n",
    "        total_rewards = []\n",
    "        for episode in tqdm(range(1, self.training_episodes + 1), desc=\"validate\"):\n",
    "            time_step = self.env.reset()\n",
    "            state = time_step.observation\n",
    "            done = time_step.is_last()\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.agent.act(state)\n",
    "                time_step = self.env.step(action)\n",
    "                next_state = time_step.observation\n",
    "                reward = time_step.reward\n",
    "                done = time_step.is_last()\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f'Validation Episode: {episode}, Total Reward: {total_reward}')\n",
    "\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        print(f'Average Reward Over Validation Episodes: {avg_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 03:11:00.724451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-08 03:11:00.725631: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "train:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "env = TradingEnv(data=tickers[TARGET], data_dim=FEATURES, startingDate=START_DATE, money=CAPITAL, stateLength=STATE_LEN, transactionCosts=FEES, endingDate=END_DATE)\n",
    "agent = DQNAgent(observation_space=STATE_LEN * FEATURES, action_space=ACT_SPACE)\n",
    "\n",
    "simulator = TradingSimulator(env, agent, training_episodes=500, validation_episodes=50, checkpoint_interval=100)\n",
    "simulator.train()\n",
    "simulator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "CONCLUDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
    "- [Open Gym AI Github](https://github.com/openai/gym)\n",
    "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
    "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github\n",
    "\n",
    "Article here is also available on [Github](https://github.com/adamd1985/pairs_trading_unsupervised_learning)\n",
    "\n",
    "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/unsupervised-learning-as-signals-for-pairs-trading)\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
