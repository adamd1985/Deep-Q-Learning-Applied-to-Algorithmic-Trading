{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee65a3b0",
   "metadata": {
    "id": "oaDoHbxVH0CW",
    "papermill": {
     "duration": 0.012245,
     "end_time": "2024-10-02T20:56:26.358604",
     "exception": false,
     "start_time": "2024-10-02T20:56:26.346359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deep Q-Learning Applied to Algorithmic Trading\n",
    "\n",
    "<a href=\"https://www.kaggle.com/code/addarm/deep-q-rl-with-algorithmic-trading-policy\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/blob/main/drl_trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb7454",
   "metadata": {
    "id": "fopVWzH_sxr-",
    "papermill": {
     "duration": 0.013201,
     "end_time": "2024-10-02T20:56:26.383676",
     "exception": false,
     "start_time": "2024-10-02T20:56:26.370475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![\"Pavlov's trader dog, DALEE 2024\"](https://github.com/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/blob/main/images/rl_banner.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763aa1b",
   "metadata": {
    "id": "xqvaSLkfsxr-",
    "papermill": {
     "duration": 0.011034,
     "end_time": "2024-10-02T20:56:26.405952",
     "exception": false,
     "start_time": "2024-10-02T20:56:26.394918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the book *\"A Random Walk Down Wall Street\"*, the author Burton G. Malkiel claimed that: “a blindfolded monkey throwing darts at a newspaper's financial pages could select a portfolio that would do just as well as one carefully selected by experts.”\n",
    "\n",
    "But what if instead of a monkey, it was Pavlov's dog trained with reinforcement learning to select the optimal portfolio strategy? In this article, Reinforcement Learning (RL) refers to the use of machine learning, where an agent learns actions in an environment to maximize its value. The agent learns from the outcomes of its actions, without being explicitly programmed with task-specific rules.\n",
    "\n",
    "The goal of any RL algorithm is to find a value-maximizing policy (*π*):\n",
    "$$\n",
    "\\pi^* = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "Where *γ (0 ≤ γ ≤ 1)* is the discount factor controlling the agent's future rewards, *t* is the timestep, and *R* is the return at each step. The policy in RL represents the probability of taking **action *a*** in **state *s***.\n",
    "\n",
    "The algorithm we will adopt is **Q-Learning**, a model-free RL algorithm that aims to indirectly learn the policy through the **VALUE** of an action for a discrete state, rather than the policy itself. It's useful in our case as it doesn’t require modeling the environment—in our case, the random capital markets.\n",
    "\n",
    "Estimating the **Q-Value** is done through Bellman's equation:\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} Q^*(s', a') \\mid s, a]\n",
    "$$\n",
    "\n",
    "These Q-values are stored in Q-Tables and used by the agent as a lookup to find all possible actions' Q-values from the current state, selecting the action with the highest Q-value (exploitation). This approach works well in finite spaces but struggles in stochastic environments with limitless combinations, a problem we will solve using a neural network.\n",
    "\n",
    "The agent designed in this article has been inspired by the paper of *Théate, Thibaut and Ernst, Damien (2021)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8e193",
   "metadata": {
    "id": "aM59cTClH0CZ",
    "papermill": {
     "duration": 0.011215,
     "end_time": "2024-10-02T20:56:26.428525",
     "exception": false,
     "start_time": "2024-10-02T20:56:26.417310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```BibTeX\n",
    "@article{theate2021application,\n",
    "  title={An application of deep reinforcement learning to algorithmic trading},\n",
    "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
    "  journal={Expert Systems with Applications},\n",
    "  volume={173},\n",
    "  pages={114632},\n",
    "  year={2021},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92362443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T20:56:26.452640Z",
     "iopub.status.busy": "2024-10-02T20:56:26.452268Z",
     "iopub.status.idle": "2024-10-02T21:01:22.549764Z",
     "shell.execute_reply": "2024-10-02T21:01:22.548462Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1710410570017,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "C39UfWDmzYvL",
    "papermill": {
     "duration": 296.112006,
     "end_time": "2024-10-02T21:01:22.551841",
     "exception": false,
     "start_time": "2024-10-02T20:56:26.439835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "    print('Running in Kaggle')\n",
    "    IN_KAGGLE = True\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
    "\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "elif 'COLAB_GPU' in os.environ:\n",
    "    print('Running in Google Colab')\n",
    "    IN_KAGGLE = False\n",
    "    IN_COLAB = True\n",
    "    DATA_DIR = \"/content/drive/MyDrive/DATASET\"\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print('Running Local')\n",
    "    IN_KAGGLE = False\n",
    "    IN_COLAB = False\n",
    "    DATA_DIR = \"./data/\"\n",
    "\n",
    "if IN_KAGGLE or IN_COLAB:\n",
    "    %pip install scikit-learn\n",
    "    %pip install statsmodels\n",
    "    %pip install matplotlib\n",
    "    %pip install yfinance\n",
    "    %pip install pyarrow\n",
    "    %pip install ta\n",
    "    %pip install tqdm\n",
    "    %pip install shap==0.46.0\n",
    "    # TensorFlow and TF-Agents version compatibility\n",
    "    %pip install tensorflow==2.15.1\n",
    "    %pip install tf-agents==0.19.0\n",
    "    %pip install dm-reverb==0.14.0\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e09bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:22.638984Z",
     "iopub.status.busy": "2024-10-02T21:01:22.638662Z",
     "iopub.status.idle": "2024-10-02T21:01:34.145371Z",
     "shell.execute_reply": "2024-10-02T21:01:34.144370Z"
    },
    "papermill": {
     "duration": 11.552617,
     "end_time": "2024-10-02T21:01:34.147671",
     "exception": false,
     "start_time": "2024-10-02T21:01:22.595054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ta.trend import MACD, EMAIndicator\n",
    "from ta.volatility import AverageTrueRange\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import py_environment, tf_py_environment, utils\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy, policy_saver, random_tf_policy\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "import reverb\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3dfa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:34.235641Z",
     "iopub.status.busy": "2024-10-02T21:01:34.234571Z",
     "iopub.status.idle": "2024-10-02T21:01:34.728954Z",
     "shell.execute_reply": "2024-10-02T21:01:34.727853Z"
    },
    "executionInfo": {
     "elapsed": 6369,
     "status": "ok",
     "timestamp": 1710410576375,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "GJiIs_h-H0Ca",
    "outputId": "e639a0a2-0f32-4741-916b-afb9f8f8202d",
    "papermill": {
     "duration": 0.540348,
     "end_time": "2024-10-02T21:01:34.730880",
     "exception": false,
     "start_time": "2024-10-02T21:01:34.190532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_strategy():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        # Enable memory growth for each detected GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])  # Adjust memory limit if needed\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f\"GPU devices found: {gpus}\")\n",
    "    else:\n",
    "        # Default to a single-device strategy (CPU) if no GPUs are available\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print(\"No GPU found. Using CPU strategy.\")\n",
    "    return strategy\n",
    "\n",
    "strategy = initialize_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba5412",
   "metadata": {},
   "source": [
    "# Parameters and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9fdf43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:34.817938Z",
     "iopub.status.busy": "2024-10-02T21:01:34.817578Z",
     "iopub.status.idle": "2024-10-02T21:01:34.828732Z",
     "shell.execute_reply": "2024-10-02T21:01:34.828004Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1710410576375,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "Mh5TWk3l1CJ-",
    "papermill": {
     "duration": 0.056818,
     "end_time": "2024-10-02T21:01:34.830834",
     "exception": false,
     "start_time": "2024-10-02T21:01:34.774016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "MODELS_PATH = './models'\n",
    "LOGS_PATH = './logs'\n",
    "AGENT_HISTORY = f\"{LOGS_PATH}/history\"\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_PATH, exist_ok=True)\n",
    "os.makedirs(AGENT_HISTORY, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Indices and tickers\n",
    "START_DATE = \"2017-01-01\"   # Training dates\n",
    "SPLIT_DATE = '2018-01-01'   # Testing cutoff\n",
    "END_DATE = \"2019-12-31\"     # Testing dates\n",
    "INDEX = \"Date\"\n",
    "TARGET = 'TSLA'\n",
    "RATES_INDEX = \"^FVX\"        # 5 Year Treasury Note Yield\n",
    "VOLATILITY_INDEX = \"^VIX\"   # CBOE Volatility Index\n",
    "SMALLCAP_INDEX = \"^RUT\"     # Russell 2000 Index\n",
    "GOLD_FUTURES = \"GC=F\"       # Gold futures\n",
    "OIL_FUTURES = \"CL=F\"        # Crude Oil Futures\n",
    "MARKET = \"^SPX\"             # S&P 500 Index\n",
    "TICKER_SYMBOLS = [TARGET, RATES_INDEX, VOLATILITY_INDEX, SMALLCAP_INDEX, GOLD_FUTURES, MARKET, OIL_FUTURES]\n",
    "INTERVAL = \"1d\"\n",
    "TRADING_DAYS_YEAR = 252     # Trading days in a year\n",
    "RISK_FREE_RATE = 0.021      # Average riskfree from 2012 to 2020\n",
    "\n",
    "# Trading Params\n",
    "ACT_SHORT = 0\n",
    "ACT_LONG = 1\n",
    "ACT_HOLD = 2\n",
    "ACTIONS = [ACT_SHORT, ACT_LONG]\n",
    "CAPITAL = 100000\n",
    "TRADE_COSTS_PERCENT = 10 / 100 / 100  # 10 basis points costs\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
    "LEARN_RATE = 1e-3           # The networks learning rate (min 1e-5)\n",
    "TRAIN_EPISODES = 500        # Number of episodes to train the networks (max 1000)\n",
    "COLLECT_SIZE = 1000         # Default memory buffer, should default to observation state size\n",
    "STATE_LEN = 30              # How much historic timesteps to return from the environment\n",
    "LOG_INTERVALS = 20          # Log every X iterations\n",
    "TEST_INTERVALS = 100        # Every X iterations, validate\n",
    "TARGET_UPDATE_ITERS = 20    # Update the target networks\n",
    "VALIDATION_ITERS = 10       # Repeat the experiment N times for significant metrics\n",
    "DISCOUNT = 0.4              # The gamma in the bellman, how much to discount past rewards\n",
    "EPSILON_START = 1.0         # For epsilon greedy algos, how much to explore VS exploit\n",
    "EPSILON_END = 0.01          # At the end there should be more exploitation\n",
    "EPSILON_DECAY = 10000       # Used for anealing the epsilon\n",
    "GRAD_CLIP = 1.0             # Stop exploding/disappearing gradients\n",
    "REWARD_CLIP = 1.0           # Stop exploding/disappearing rewards\n",
    "DROPOUT = 0.2               # Chance of tensor dropout\n",
    "L2FACTOR = 1e-5             # Regularization wieghts\n",
    "ADAM_WEIGHTS = 1e-5         # Weighted Optimizer\n",
    "NEURONS = 512               # Network capacity\n",
    "LAYERS = (NEURONS, NEURONS, NEURONS, NEURONS, NEURONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404783a",
   "metadata": {
    "id": "H4DpIfIPH0Ch",
    "papermill": {
     "duration": 0.042595,
     "end_time": "2024-10-02T21:01:34.916293",
     "exception": false,
     "start_time": "2024-10-02T21:01:34.873698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Financial data\n",
    "\n",
    "We download some financial data, this is now the standard in our articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d2f1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:35.003430Z",
     "iopub.status.busy": "2024-10-02T21:01:35.003035Z",
     "iopub.status.idle": "2024-10-02T21:01:37.436434Z",
     "shell.execute_reply": "2024-10-02T21:01:37.435403Z"
    },
    "executionInfo": {
     "elapsed": 1225,
     "status": "ok",
     "timestamp": 1710410577593,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "s64pmt9mH0Cj",
    "outputId": "ffdcc9c6-322f-4598-c4d0-92ffb7f60885",
    "papermill": {
     "duration": 2.479323,
     "end_time": "2024-10-02T21:01:37.438366",
     "exception": false,
     "start_time": "2024-10-02T21:01:34.959043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, data_dir=DATA_DIR):\n",
    "    tickers = {}\n",
    "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
    "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    for symbol in tickers_symbols:\n",
    "        cached_file_path = f\"{data_dir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(cached_file_path):\n",
    "                df = pd.read_parquet(cached_file_path)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                assert len(df) > 0\n",
    "            else:\n",
    "                df = yf.download(\n",
    "                    symbol,\n",
    "                    start=START_DATE,\n",
    "                    end=END_DATE,\n",
    "                    progress=False,\n",
    "                    interval=INTERVAL,\n",
    "                )\n",
    "                assert len(df) > 0\n",
    "                df.to_parquet(cached_file_path, index=True, compression=\"snappy\")\n",
    "            min_date = df.index.min()\n",
    "            max_date = df.index.max()\n",
    "            nan_count = df[\"Close\"].isnull().sum()\n",
    "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
    "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
    "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
    "            print(\n",
    "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
    "            )\n",
    "            tickers[symbol] = df\n",
    "\n",
    "            if min_date > latest_start:\n",
    "                latest_start = min_date\n",
    "            if max_date < earliest_end:\n",
    "                earliest_end = max_date\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {symbol}: {e}\")\n",
    "\n",
    "    return tickers, latest_start, earliest_end\n",
    "\n",
    "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
    "stock_df = tickers[TARGET].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974af1d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67b17ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:37.527969Z",
     "iopub.status.busy": "2024-10-02T21:01:37.527647Z",
     "iopub.status.idle": "2024-10-02T21:01:37.533103Z",
     "shell.execute_reply": "2024-10-02T21:01:37.532211Z"
    },
    "papermill": {
     "duration": 0.051392,
     "end_time": "2024-10-02T21:01:37.534982",
     "exception": false,
     "start_time": "2024-10-02T21:01:37.483590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MACRO_FEATURES = [RATES_INDEX, VOLATILITY_INDEX, MARKET, GOLD_FUTURES, OIL_FUTURES]\n",
    "TA_FEATURES = ['MACD', 'MACD_HIST', 'MACD_SIG', 'ATR', 'EMA_SHORT', 'EMA_MID', 'EMA_LONG']\n",
    "HLOC_FEATURES = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "FEATURES = ['Price Returns', 'Price Delta', 'Close Position', 'Volume']\n",
    "TARGET_FEATURE = \"Price Raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721d60b",
   "metadata": {},
   "source": [
    "## Technicals\n",
    "\n",
    "These are technical analysis features to be leveraged by our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3933142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:37.622697Z",
     "iopub.status.busy": "2024-10-02T21:01:37.622387Z",
     "iopub.status.idle": "2024-10-02T21:01:37.669393Z",
     "shell.execute_reply": "2024-10-02T21:01:37.668465Z"
    },
    "papermill": {
     "duration": 0.093238,
     "end_time": "2024-10-02T21:01:37.671394",
     "exception": false,
     "start_time": "2024-10-02T21:01:37.578156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "macd = MACD(close=stock_df[\"Close\"], window_slow=26, window_fast=12, window_sign=9, fillna=True)\n",
    "stock_df['MACD'] = macd.macd()\n",
    "stock_df['MACD_HIST'] = macd.macd_diff()\n",
    "stock_df['MACD_SIG'] = macd.macd_signal()\n",
    "\n",
    "atr = AverageTrueRange(stock_df[\"High\"], stock_df[\"Low\"], stock_df[\"Close\"], window = 14, fillna = True)\n",
    "stock_df['ATR'] = atr.average_true_range()\n",
    "\n",
    "ema = EMAIndicator(stock_df[\"Close\"], window = 12, fillna = True)\n",
    "stock_df['EMA_SHORT'] = ema.ema_indicator()\n",
    "ema = EMAIndicator(stock_df[\"Close\"], window = 26, fillna = True)\n",
    "stock_df['EMA_MID'] = ema.ema_indicator()\n",
    "ema = EMAIndicator(stock_df[\"Close\"], window = 200, fillna = True)\n",
    "stock_df['EMA_LONG'] = ema.ema_indicator()\n",
    "\n",
    "stock_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948efca",
   "metadata": {},
   "source": [
    "## Macro\n",
    "\n",
    "These are macro features, whose change will be observed by our networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1c257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:37.760726Z",
     "iopub.status.busy": "2024-10-02T21:01:37.760400Z",
     "iopub.status.idle": "2024-10-02T21:01:37.791739Z",
     "shell.execute_reply": "2024-10-02T21:01:37.790892Z"
    },
    "papermill": {
     "duration": 0.077992,
     "end_time": "2024-10-02T21:01:37.793755",
     "exception": false,
     "start_time": "2024-10-02T21:01:37.715763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_df[VOLATILITY_INDEX] = tickers[VOLATILITY_INDEX][\"Close\"].pct_change().fillna(0)\n",
    "stock_df[RATES_INDEX] = tickers[RATES_INDEX][\"Close\"].pct_change().fillna(0)\n",
    "stock_df[SMALLCAP_INDEX] = tickers[SMALLCAP_INDEX][\"Close\"].pct_change().fillna(0)\n",
    "stock_df[GOLD_FUTURES] = tickers[GOLD_FUTURES][\"Close\"].pct_change().fillna(0)\n",
    "stock_df[OIL_FUTURES] = tickers[OIL_FUTURES][\"Close\"].pct_change().fillna(0)\n",
    "stock_df[MARKET] = tickers[MARKET][\"Close\"].pct_change().fillna(0)\n",
    "\n",
    "stock_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b106f7a",
   "metadata": {
    "id": "ZDY9OBeVsxsA",
    "papermill": {
     "duration": 0.044046,
     "end_time": "2024-10-02T21:01:37.881496",
     "exception": false,
     "start_time": "2024-10-02T21:01:37.837450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Problem Definition\n",
    "\n",
    "With Q-Training, we shall teach a pavlovian-agent to trade. Our objective is to make sequential interaction that lead to the highest sharpe ratio, formalized by this policy (remember Q-Learning is off-policy, and we won't learn this directly):\n",
    "\n",
    "$$\n",
    "\\pi^*(a_t | s_t) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\middle| s_t = f(o_1, a_1, r_1, \\ldots, o_t), a_t \\right]\n",
    "$$\n",
    "\n",
    "At each timestep *t*:\n",
    "\n",
    "1. Observe the environments state *st* and map history with *f(.)*\n",
    "2. Observations *ot* from history *ht*, have previous actions *a_t-1*, previous observations *o_t-1* and their returns *r_t-1*. For our experiment, we'll encode these into features for a network.\n",
    "3. Execute action *a_t*, which can be: hold, long, short\n",
    "4. Get returns *r_t* discounted at *γt*. *γ* is the discounting factor to prevent the agent from doing only tactical choices for returns in the present (missing better future returns).\n",
    "\n",
    "\n",
    "The *π(at|ht)* creates an action on a Quantity Q *at = Qt*. Where a positive *Q* is the long, the negative *Q* signals a short and when its 0 no action is taken. For this article we will use the definition of policy *π(at|ht)* and Q-Value *Q(at,st)* interchangeably, as Q will define quantities bought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c659b",
   "metadata": {
    "id": "I7yPL6IJtuuz",
    "papermill": {
     "duration": 0.043544,
     "end_time": "2024-10-02T21:01:37.968611",
     "exception": false,
     "start_time": "2024-10-02T21:01:37.925067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Observations and State Space\n",
    "\n",
    "The paper uses only High, Low, Open, Close and Volume as observations from the agent's environment state.\n",
    "\n",
    "We will augment this space with 2 technical indicators and 2 macroeconomic indicators:\n",
    "- 20day slow and 7 day fast exponential moving average, from our article: \"Momentum and Reversion Trading Signals Analysis\"\n",
    "- The daily VIX as proxy for market volatility & fear, and the 2 year T-note as proxy for inflation & rates, from our article: \"Temporal Convolutional Neural Network with Conditioning for Broad Market Signals\"\n",
    "\n",
    "$$\n",
    "o_t = s_t \\in \\{{\\text{High}_t}, {\\text{Low}_t}, {\\text{Open}_t}, {\\text{Close}_t}, {\\text{Volume}_t}, {\\text{FastEMA}_t}, {\\text{SlowEMA}_t}, {\\text{VIX}_t}, {\\text{T2YR}_t}, {\\text{Gold}_t}\\, {\\text{Market}_t}\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75225007",
   "metadata": {
    "id": "IUoFMk_AsxsA",
    "papermill": {
     "duration": 0.043282,
     "end_time": "2024-10-02T21:01:38.055621",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.012339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Actions and Rewards\n",
    "\n",
    "A core concept in RL is rewards engineering. Let's look at our action space *A* at time *t*:\n",
    "\n",
    "$$\n",
    "a_t = Q_t \\in \\{Q_{\\text{Long}, t}, Q_{\\text{Short}, t}\\}\n",
    "$$\n",
    "\n",
    "The action *Q_Long,t* is set to maximize returns on a buy, given our liquidity *vc_t* (the value *v* of our portfolio with cash remainng *c*) and purchasing *Q_long* at price *p* shares (transaction costs *C*) if we are not already long:\n",
    "\n",
    "$$\n",
    "Q_{\\text{Long}, t} =\n",
    "\\begin{cases}\n",
    "\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Long}, t-1}, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The action *Q_Short,t* aims to convert a **negative** number of shares to returns (shorting is the borrowing of shares, therefore our *v_c* will be initially negative).\n",
    "\n",
    "$$\n",
    "\\hat{Q}_{\\text{Short}, t} =\n",
    "\\begin{cases}\n",
    "-2n_t - \\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Short}, t-1}, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Note the *-2n* is an indication to sell twice, meaning not only close the long position but open a short position for the *Qn* shares, since shorting is a negative trajectory, we need to negate the amount we can buy to get the correct representation in our holdings. If we had no shares to start, then *-2(0)* will not have an effect save for the short amount:\n",
    "\n",
    "$$\n",
    "\\hat{Q}_{\\text{Short}, t} = -\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "Shorts are risky, and we need to give boundaries to the agent, as a short can incur infinite loss:\n",
    "\n",
    "$$\n",
    "Q_{\\text{Short}, t} = \\max\\{\\hat{Q}_{\\text{Short}, t}, Q_t\\}\n",
    "$$\n",
    "\n",
    "Given that our portfolio cannot fall into negative amounts, we need to model constraints.\n",
    "1. Cash value *vc_t* needs to be large enough to return to neutral *n_t=0*.\n",
    "2. To return to 0, we need to adjust for costs *C* which are caused by market volatility epsiloc *ϵ* (think slippages, spreads, etc..).\n",
    "3. We redefine the action space permissable to ensure we can always return to neutral.\n",
    "\n",
    "$$\n",
    "v_{c,t+1} \\geq -n_{t+1} p_t (1 + \\varepsilon)(1 + C)\n",
    "$$\n",
    "\n",
    "The action space *A* is redefined as a set of acceptable values for *Q_t* between boundaries *Q-* and *Q+*:\n",
    "\n",
    "$$\n",
    "A = \\left\\{ Q_t \\in \\mathbb{Z} \\cap \\left[Q_t^-, Q_t^+\\right] \\right\\}\n",
    "$$\n",
    "\n",
    "Where the top boundary *Q+* is:\n",
    "$$\n",
    "Q_t^+ = \\frac{v_{c,t}}{p_t (1+C)}\n",
    "$$\n",
    "\n",
    "And the lower boundary *Q-* is (for both coming out of a long where delta *t* is positive, or reversing a short and incurring twice the costs with delta *t* in the negative):\n",
    "\n",
    "$$\n",
    "Q_t^- = \\begin{cases}\n",
    "    \\frac{\\Delta t}{p_t \\varepsilon (1 + C)} & \\text{if } \\Delta t \\geq 0, \\\\\n",
    "    \\frac{\\Delta t}{p_t (2C + \\varepsilon(1 + C))} & \\text{if } \\Delta t < 0,\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "with *delta t* being the in change of portfolio value in time:\n",
    "\n",
    "$$\n",
    " t_Δ = -v_{c,t} - n_t p_t (1 + \\varepsilon)(1 + C)\n",
    "$$\n",
    "\n",
    "In the above boundaries, the cost of trading is defined as:\n",
    "\n",
    "$$\n",
    "v_{c,t+1} = v_{c,t} - Q_t p_t - C |Q_t| p_t\n",
    "$$\n",
    "\n",
    "Where *C* is the percentage cost of the transaction given the absolute quantity *|Q_t|* of shares and their price *p_t*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06e1e1",
   "metadata": {
    "id": "_A2EBYR9sxsA",
    "papermill": {
     "duration": 0.043501,
     "end_time": "2024-10-02T21:01:38.142680",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.099179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Agent's Objective\n",
    "\n",
    "In the paper, they utilize the percentage returns as a rewards signal, clipped between -1 and 1, and adjusted by a discount factor γ:\n",
    "$$\n",
    "\\text{Signal} = \\left( \\frac{vc_{t} - vc_{t-1}}{vc_{t-1}} \\right) \\gamma^t\n",
    "$$\n",
    "\n",
    "In the article, we will use an annualized Sharpe (from *N* time window, up to 252 trading days), and teach the agent's to generate an optimal ratio, clipped no discount factor:\n",
    "\n",
    "$$\n",
    "\\text{Signal} = \\frac{E\\left[\\sum_{t=0}^{T} r_t - R_f\\times \\right]\\sqrt{N} }{\\sqrt{\\mathrm{Var}\\left[\\sum_{t=0}^{T} r_t\\right]}}\n",
    "$$\n",
    "\n",
    "which is just the maximization of:\n",
    "\n",
    "$$\n",
    "\\text{sharpe}= \\left( \\frac{\\bar{R} - R_f}{\\sigma} \\right)\n",
    "$$\n",
    "\n",
    "or the returns of the portfolio (*R* average), minus the risk free rate (*Rf*, at the time of writing, 4.5%) divided by the volatility (*σ*) of the portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cf3b1",
   "metadata": {
    "id": "lke4koO5H0Cl",
    "papermill": {
     "duration": 0.043252,
     "end_time": "2024-10-02T21:01:38.229683",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.186431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trading Environment\n",
    "\n",
    "Using TensorFlow's PyEnvironment, we will give the agent the environment that implements the above rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6deefa44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:38.348362Z",
     "iopub.status.busy": "2024-10-02T21:01:38.347914Z",
     "iopub.status.idle": "2024-10-02T21:01:38.422145Z",
     "shell.execute_reply": "2024-10-02T21:01:38.420879Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1710410577594,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "dXLhk-CKH0Co",
    "papermill": {
     "duration": 0.144642,
     "end_time": "2024-10-02T21:01:38.425379",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.280737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TradingEnv(py_environment.PyEnvironment):\n",
    "    \"\"\"\n",
    "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
    "\n",
    "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
    "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the stock market data.\n",
    "    - data_dim: Dimension of the data to be used for each observation.\n",
    "    - money: Initial capital to start trading.\n",
    "    - state_length: Number of past observations to consider for the state.\n",
    "    - transaction_cost: Costs associated with trading actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, features = FEATURES, money=CAPITAL, state_length=STATE_LEN, transaction_cost=0, market_costs=TRADE_COSTS_PERCENT, reward_discount=DISCOUNT):\n",
    "        super(TradingEnv, self).__init__()\n",
    "\n",
    "        assert data is not None\n",
    "\n",
    "        self.features = features\n",
    "        self.data_dim = len(self.features)\n",
    "        self.state_length = state_length\n",
    "        self.current_step = self.state_length\n",
    "        self.reward_discount = reward_discount\n",
    "\n",
    "        self.balance = money\n",
    "        self.initial_balance = money\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.epsilon = max(market_costs, np.finfo(float).eps) # there is always volatility costs\n",
    "        self.total_shares = 0\n",
    "\n",
    "        self._episode_ended = False\n",
    "        self._batch_size = 1\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(self.state_length * self.data_dim, ), dtype=np.float32, name='observation')\n",
    "\n",
    "        self.data = self.preprocess_data(data.copy())\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def batched(self):\n",
    "        return False #True\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return None #self._batch_size\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, size):\n",
    "        self._batch_size = size\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        price_raw = df['Close'].copy()\n",
    "\n",
    "        # Engineer features from HLOC\n",
    "        df['Price Returns'] = df['Close'].pct_change().fillna(0)\n",
    "        df['Price Delta'] = (df['High'] - df['Low'])\n",
    "        df['Close Position'] = abs(df['Close'] - df['Low']) / df['Price Delta'].replace(0, 0.5)\n",
    "        for col in [col for col in self.features]:\n",
    "            col_min, col_max = df[col].min(), df[col].max()\n",
    "            if col_min != col_max:\n",
    "                df[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "            else:\n",
    "                df[col] = 0.\n",
    "        df = df.ffill().bfill()\n",
    "\n",
    "        df[TARGET_FEATURE] = price_raw\n",
    "        df['Sharpe'] = 0\n",
    "        df['Position'] = 0\n",
    "        df['Action'] = ACT_HOLD\n",
    "        df['Holdings'] = 0.0\n",
    "        df['Cash'] = float(self.balance)\n",
    "        df['Money'] = df['Holdings'] + df['Cash']\n",
    "        df['Reward'] = 0.0\n",
    "\n",
    "        assert not df.isna().any().any()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Provides the specification of the action space.\"\"\"\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.current_step = self.state_length\n",
    "        self._episode_ended = False\n",
    "        self.total_shares = 0\n",
    "\n",
    "        self.data['Reward'] = 0.\n",
    "        self.data['Sharpe'] = 0.\n",
    "        self.data['Position'] = 0\n",
    "        self.data['Action'] = ACT_HOLD\n",
    "        self.data['Holdings'] = 0.\n",
    "        self.data['Cash']  = float(self.balance)\n",
    "        self.data['Money'] = self.data.iloc[0]['Holdings'] + self.data.iloc[0]['Cash']\n",
    "        self.data['Returns'] = 0.\n",
    "\n",
    "        return ts.restart(self._next_observation())\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \"\"\"Generates the next observation based on the current step and history length.\"\"\"\n",
    "        start_idx = max(0, self.current_step - self.state_length + 1)\n",
    "        end_idx = self.current_step + 1\n",
    "        obs = self.data[self.features].iloc[start_idx:end_idx]\n",
    "\n",
    "        # flatten because: https://stackoverflow.com/questions/67921084/dqn-agent-issue-with-custom-environment\n",
    "        obs_values = obs.values.flatten().astype(np.float32)\n",
    "        return obs_values\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Executes a trading action and updates the environment's state.\"\"\"\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        self.current_step += 1\n",
    "        current_price = self.data.iloc[self.current_step][TARGET_FEATURE]\n",
    "\n",
    "        assert not self.data.iloc[self.current_step].isna().any().any()\n",
    "\n",
    "        if action == ACT_LONG:\n",
    "            self._process_long_position(current_price)\n",
    "        elif action == ACT_SHORT:\n",
    "            prev_current_price = self.data.iloc[self.current_step - 1][TARGET_FEATURE]\n",
    "            self._process_short_position(current_price, prev_current_price)\n",
    "        elif action == ACT_HOLD:\n",
    "            self._process_hold_position()\n",
    "        else:\n",
    "          raise Exception(f\"Invalid Actions: {action}\")\n",
    "\n",
    "        self._update_financials()\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        # reward = self._calculate_sharpe_reward_signal()\n",
    "        reward = self.data['Returns'][self.current_step]\n",
    "        self.data.at[self.data.index[self.current_step], \"Reward\"] = reward\n",
    "        if done:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(self._next_observation(), reward)\n",
    "        else:\n",
    "            return ts.transition(self._next_observation(), reward, discount=self.reward_discount)\n",
    "\n",
    "    def _get_lower_bound(self, cash, total_shares, price):\n",
    "        \"\"\"\n",
    "        Compute the lower bound of the action space, particularly for short selling,\n",
    "        based on current cash, the number of shares, and the current price.\n",
    "        \"\"\"\n",
    "        delta = -cash - total_shares * price * (1 + self.epsilon) * (1 + self.transaction_cost)\n",
    "\n",
    "        if delta < 0:\n",
    "            lowerBound = delta / (price * (2 * self.transaction_cost + self.epsilon * (1 + self.transaction_cost)))\n",
    "        else:\n",
    "            lowerBound = delta / (price * self.epsilon * (1 + self.transaction_cost))\n",
    "\n",
    "        if np.isinf(lowerBound):\n",
    "            assert False\n",
    "        return lowerBound\n",
    "\n",
    "    def _process_hold_position(self):\n",
    "        step_idx = self.data.index[self.current_step]\n",
    "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
    "        self.data.at[step_idx, \"Holdings\"] = self.data.iloc[self.current_step - 1][\"Holdings\"]\n",
    "        self.data.at[step_idx, \"Position\"] = self.data.iloc[self.current_step - 1][\"Position\"]\n",
    "        self.data.at[step_idx, \"Action\"] = ACT_HOLD\n",
    "\n",
    "    def _process_long_position(self, current_price):\n",
    "        step_idx = self.data.index[self.current_step]\n",
    "        self.data.at[step_idx, 'Position'] = 1\n",
    "        self.data.at[step_idx, 'Action'] = ACT_LONG\n",
    "\n",
    "        if self.data.iloc[self.current_step - 1]['Position'] == 1:\n",
    "            # more long\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash']\n",
    "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
    "            self.data.at[step_idx, \"Action\"] = ACT_HOLD\n",
    "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
    "            # new long\n",
    "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
    "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
    "        else:\n",
    "            # short to long\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
    "            self.total_shares = math.floor(self.data.iloc[self.current_step]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
    "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
    "\n",
    "    def _process_short_position(self, current_price, prev_price):\n",
    "        \"\"\"\n",
    "        Adjusts the logic for processing short positions to include lower bound calculations.\n",
    "        \"\"\"\n",
    "        step_idx = self.data.index[self.current_step]\n",
    "        self.data.at[step_idx, 'Position'] = -1\n",
    "        self.data.at[step_idx, \"Action\"] = ACT_SHORT\n",
    "        if self.data.iloc[self.current_step - 1]['Position'] == -1:\n",
    "            # Short more\n",
    "            low = self._get_lower_bound(self.data.iloc[self.current_step - 1]['Cash'], -self.total_shares, prev_price)\n",
    "            if low <= 0:\n",
    "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
    "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
    "                self.data.at[step_idx, \"Action\"] = ACT_HOLD\n",
    "            else:\n",
    "                total_sharesToBuy = min(math.floor(low), self.total_shares)\n",
    "                self.total_shares -= total_sharesToBuy\n",
    "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] - total_sharesToBuy * current_price * (1 + self.transaction_cost)\n",
    "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
    "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
    "            # new short\n",
    "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
    "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
    "        else:\n",
    "            # long to short\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
    "            self.total_shares = math.floor(self.data.iloc[self.current_step][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
    "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
    "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
    "\n",
    "    def _update_financials(self):\n",
    "        \"\"\"Updates the financial metrics including cash, money, and returns.\"\"\"\n",
    "        step_idx = self.data.index[self.current_step]\n",
    "        self.balance = self.data.iloc[self.current_step]['Cash']\n",
    "\n",
    "        self.data.at[step_idx,'Money'] = self.data.iloc[self.current_step]['Holdings'] + self.data.iloc[self.current_step]['Cash']\n",
    "        self.data.at[step_idx,'Returns'] = ((self.data.iloc[self.current_step]['Money'] - self.data.iloc[self.current_step - 1]['Money'])) / self.data.iloc[self.current_step - 1]['Money']\n",
    "\n",
    "    def _calculate_reward_signal(self, reward_clip=REWARD_CLIP):\n",
    "        \"\"\"\n",
    "        Calculates the reward for the current step. In the paper they use the %returns.\n",
    "        \"\"\"\n",
    "        reward = self.data.iloc[self.current_step]['Returns']\n",
    "        return np.clip(reward, -reward_clip, reward_clip)\n",
    "\n",
    "    def _calculate_sharpe_reward_signal(self, risk_free_rate=RISK_FREE_RATE, periods_per_year=TRADING_DAYS_YEAR, reward_clip=REWARD_CLIP):\n",
    "        \"\"\"\n",
    "        Calculates the annualized Sharpe ratio up to the CURRENT STEP.\n",
    "\n",
    "        Parameters:\n",
    "        - risk_free_rate (float): The annual risk-free rate. It will be adjusted to match the period of the returns.\n",
    "        - periods_per_year (int): Number of periods in a year (e.g., 252 for daily, 12 for monthly).\n",
    "\n",
    "        Returns:\n",
    "        - float: The annualized Sharpe ratio as reward.\n",
    "        \"\"\"\n",
    "        period_risk_free_rate = (1 + risk_free_rate) ** (1 / periods_per_year) - 1\n",
    "        observed_returns = self.data['Returns'].iloc[:self.current_step + 1]\n",
    "\n",
    "        excess_returns = observed_returns - period_risk_free_rate\n",
    "\n",
    "        mean_excess_return = np.mean(excess_returns)\n",
    "        std_dev_returns = np.std(observed_returns)\n",
    "\n",
    "        sharpe_ratio = mean_excess_return / std_dev_returns if std_dev_returns > 0 else 0\n",
    "        annual_sr = sharpe_ratio * np.sqrt(periods_per_year)\n",
    "\n",
    "        self.data.at[self.data.index[self.current_step], 'Sharpe'] = annual_sr\n",
    "\n",
    "        return np.clip(annual_sr, -reward_clip, reward_clip)\n",
    "\n",
    "    def get_trade_data(self):\n",
    "        self.data['cReturns'] = np.cumprod(1 + self.data['Returns']) - 1\n",
    "        return self.data.iloc[:self.current_step + 1]\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}, Holdings: {self.total_shares}')\n",
    "        print(f\"trade stats: {self.get_trade_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17a4f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:38.523053Z",
     "iopub.status.busy": "2024-10-02T21:01:38.522204Z",
     "iopub.status.idle": "2024-10-02T21:01:38.561102Z",
     "shell.execute_reply": "2024-10-02T21:01:38.559953Z"
    },
    "executionInfo": {
     "elapsed": 18080,
     "status": "ok",
     "timestamp": 1710410595668,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "_2lK8WKhtuu4",
    "outputId": "76ff0f33-2642-4bdd-bf85-e113b7556db0",
    "papermill": {
     "duration": 0.086733,
     "end_time": "2024-10-02T21:01:38.563122",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.476389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = stock_df[stock_df.index < pd.to_datetime(SPLIT_DATE)].copy()\n",
    "test_data = stock_df[stock_df.index >= pd.to_datetime(SPLIT_DATE)].copy()\n",
    "\n",
    "train_env = TradingEnv(train_data)\n",
    "# utils.validate_py_environment(train_env, episodes=TRAIN_EPISODES // 5)\n",
    "test_env = TradingEnv(test_data)\n",
    "# utils.validate_py_environment(train_env, episodes=TRAIN_EPISODES // 5)\n",
    "\n",
    "print(f\"TimeStep Specs: {train_env.time_step_spec()}\")\n",
    "print(f\"Action Specs: {train_env.action_spec()}\")\n",
    "print(f\"Reward Specs: {train_env.time_step_spec().reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682aea75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:38.653160Z",
     "iopub.status.busy": "2024-10-02T21:01:38.652817Z",
     "iopub.status.idle": "2024-10-02T21:01:38.678020Z",
     "shell.execute_reply": "2024-10-02T21:01:38.677095Z"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1710410595668,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "6yXdVTRctuu5",
    "outputId": "9dac562a-7dba-446a-e971-b505c5ca4c11",
    "papermill": {
     "duration": 0.073133,
     "end_time": "2024-10-02T21:01:38.680797",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.607664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_action_and_print_state(env, action):\n",
    "    next_time_step = env.step(np.array(action, dtype=np.int32))\n",
    "    print(f'Action taken: {action} at step: {env.current_step}')\n",
    "    # print(f'New state: {next_time_step.observation}')\n",
    "    print(f'New balance: {env.balance}')\n",
    "    print(f'Total shares: {env.total_shares}')\n",
    "    print(f'Reward: {next_time_step.reward}\\n')\n",
    "\n",
    "time_step = train_env.reset()\n",
    "\n",
    "# Some dryruns to validate our env logic: Buy, Sell, we should have a positive balance with TSLA\n",
    "execute_action_and_print_state(train_env, ACT_HOLD)\n",
    "execute_action_and_print_state(train_env, ACT_LONG)\n",
    "execute_action_and_print_state(train_env, ACT_SHORT)\n",
    "execute_action_and_print_state(train_env, ACT_HOLD)\n",
    "execute_action_and_print_state(train_env, ACT_LONG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936d5ad",
   "metadata": {
    "id": "xcb7hq6yH0Cq",
    "papermill": {
     "duration": 0.044211,
     "end_time": "2024-10-02T21:01:38.770776",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.726565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deep Q-Network Architecure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104779f9",
   "metadata": {
    "id": "_AIE_vPrH0Cr",
    "papermill": {
     "duration": 0.043843,
     "end_time": "2024-10-02T21:01:38.858737",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.814894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Architecture\n",
    "\n",
    "Deep Q-neural-network architecture (DQN) approximates the the Q-tables algorithm as its approximating the action-value function π∗(at|st). Its an approximation because the number of combinations you can have with your Q-Tables is gargantuan and impossible to process.\n",
    "\n",
    "The Q-network is also referred to as the policy model. We will also leverage a target Q-network in our architecture. Tha Target model is updated more seldomly than the Q-Network, and helps stabilize the training process as the Q-Network is trained to reduced its output and the target network (a more stable value).\n",
    "\n",
    "Finally, we will add a Replay Memory to sample data for our models. The memory is a circular memory of fixed size (therefore it 'forgets' old memories), and at every fixed frequency the models use the memory to calculate the loss between their predicted Q values and the ones performed in the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac3895",
   "metadata": {
    "id": "Q49V1zseH0Cs",
    "papermill": {
     "duration": 0.043666,
     "end_time": "2024-10-02T21:01:38.946870",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.903204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Reinforce Learning Flow\n",
    "\n",
    "A picture says a thousand words; the flow chart below will guide us on the whole training and updating the target model:\n",
    "\n",
    "![\"Flowchart of training\"](https://raw.githubusercontent.com/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/main/images/Q-net.png)\n",
    "\n",
    "First we initinialize the environment *St* and the action state *Qt*.\n",
    "\n",
    "We then run multiple episodes of n iterations to train the model and remember the state, actions and the Q value predicted. On each iteration, these events will occur:\n",
    "1. Fetch the state.\n",
    "1. Take either a random action (ε greedy) or predict a Q value given an action in the current state, the former is called exploration, and the latter exploitation. The ε decays with time, as the model learns, it should explore less.\n",
    "1. When predicting the Q value, it will use the policy model. Regardles of exploring or explointing, it saves the memory of the states, actions and the given Q value.\n",
    "1. Calculate the target Q-Value by taking the max predictions from the target-network. From the previous formula *rt + γt * Qtarget(s_t+1, a_t+1)* where gamma *γ* is the discounting factor.\n",
    "1. Re-traing the policy model to minimize the Q-values from the different models. Training uses sampled states from our replay memory.\n",
    "1. At the end of the episode, or any interval of our chosing, we copy the wieghts of the policy model to the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9507233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:39.037279Z",
     "iopub.status.busy": "2024-10-02T21:01:39.036381Z",
     "iopub.status.idle": "2024-10-02T21:01:40.343787Z",
     "shell.execute_reply": "2024-10-02T21:01:40.342703Z"
    },
    "executionInfo": {
     "elapsed": 4191,
     "status": "ok",
     "timestamp": 1710410599829,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "2y5vgYpxH0Cs",
    "outputId": "4855dd85-dee9-4dcf-9104-d4dfde71470b",
    "papermill": {
     "duration": 1.355075,
     "end_time": "2024-10-02T21:01:40.346054",
     "exception": false,
     "start_time": "2024-10-02T21:01:38.990979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_q_network(env, fc_layer_params=LAYERS, dropout_rate=DROPOUT, l2_reg=L2FACTOR):\n",
    "    \"\"\"\n",
    "    Creates a Q-Network with dropout and batch normalization.\n",
    "    Parameters:\n",
    "    - env: The environment instance.\n",
    "    - fc_layer_params: Tuple of integers representing the number of units in each dense layer.\n",
    "    - dropout_rate: Dropout rate for dropout layers.\n",
    "    - l2_reg: L2 regularization factor.\n",
    "\n",
    "    Returns:\n",
    "    - q_net: The Q-Network model.\n",
    "    \"\"\"\n",
    "    env = tf_py_environment.TFPyEnvironment(env)\n",
    "    action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "    num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "    layers = []\n",
    "    for num_units in fc_layer_params:\n",
    "        layers.append(tf.keras.layers.Dense(\n",
    "                                num_units,\n",
    "                                activation=None,\n",
    "                                kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'),\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n",
    "        # Internal Covariate Shift Reductio by normalizing layer inputs, this improves gradient flow.\n",
    "        layers.append(tf.keras.layers.BatchNormalization())\n",
    "        layers.append(tf.keras.layers.LeakyReLU())\n",
    "        layers.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    q_values_layer = tf.keras.layers.Dense(\n",
    "        num_actions,\n",
    "        activation=None,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "        bias_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
    "\n",
    "    q_net = sequential.Sequential(layers + [q_values_layer])\n",
    "\n",
    "    return q_net\n",
    "\n",
    "def create_agent(q_net, env,\n",
    "                 t_q_net=None,\n",
    "                 optimizer=None,\n",
    "                 eps=EPSILON_START,\n",
    "                 learning_rate=LEARN_RATE,\n",
    "                 gradient_clipping=GRAD_CLIP,\n",
    "                 weight_decay = ADAM_WEIGHTS,\n",
    "                 discount=DISCOUNT):\n",
    "    \"\"\"\n",
    "    Creates a DDQN agent for a given environment with specified configurations.\n",
    "\n",
    "    Parameters:\n",
    "    - q_net (tf_agents.networks.Network): The primary Q-network for the agent.\n",
    "    - env (tf_agents.environments.PyEnvironment or tf_agents.environments.TFPyEnvironment):\n",
    "      The environment the agent will interact with. A TFPyEnvironment wrapper is applied\n",
    "      if not already wrapped.\n",
    "    - t_q_net (tf_agents.networks.Network, optional): The target Q-network for the agent.\n",
    "      If None, no target network is used.\n",
    "    - optimizer (tf.keras.optimizers.Optimizer, optional): The optimizer to use for training the agent.\n",
    "      If None, an Adam optimizer with exponential decay learning rate is used.\n",
    "    - eps (float): The epsilon value for epsilon-greedy exploration.\n",
    "    - learning_rate (float): The initial learning rate for the exponential decay learning rate schedule.\n",
    "      Ignored if an optimizer is provided.\n",
    "    - gradient_clipping (float): The value for gradient clipping. If 1., no clipping is applied.\n",
    "\n",
    "    Returns:\n",
    "    - agent (tf_agents.agents.DqnAgent): The initialized and configured DDQN agent.\n",
    "    \"\"\"\n",
    "    if optimizer is None:\n",
    "      optimizer = tf.keras.optimizers.AdamW(\n",
    "          learning_rate=learning_rate,\n",
    "          weight_decay=weight_decay\n",
    "      )\n",
    "    env = tf_py_environment.TFPyEnvironment(env)\n",
    "    agent = dqn_agent.DqnAgent(\n",
    "        env.time_step_spec(),\n",
    "        env.action_spec(),\n",
    "        gamma=discount,\n",
    "        q_network=q_net,\n",
    "        target_q_network=t_q_net,\n",
    "        target_update_period=TARGET_UPDATE_ITERS,\n",
    "        optimizer=optimizer,\n",
    "        epsilon_greedy=eps,\n",
    "        reward_scale_factor=1,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        td_errors_loss_fn=common.element_wise_huber_loss,\n",
    "        train_step_counter=tf.compat.v1.train.get_or_create_global_step(),\n",
    "        name=\"Trader\"\n",
    "    )\n",
    "\n",
    "    agent.initialize()\n",
    "    print(agent.policy)\n",
    "    print(agent.collect_policy)\n",
    "    return agent\n",
    "\n",
    "with strategy.scope():\n",
    "  q_net = create_q_network(train_env)\n",
    "  t_q_net = create_q_network(train_env)\n",
    "  agent = create_agent(q_net, train_env, t_q_net=t_q_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee99246",
   "metadata": {
    "id": "WXZ_bo04H0Ct",
    "papermill": {
     "duration": 0.043937,
     "end_time": "2024-10-02T21:01:40.435321",
     "exception": false,
     "start_time": "2024-10-02T21:01:40.391384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trading Operations\n",
    "\n",
    "Using TensorFlow agents' framework, training our pavlovian trader should be easier than building the architecture ourselves.\n",
    "\n",
    "The trading simulator class will prepare all the variables required. In this case it will initialize the reply memory using DeepMind's Reverb, and create a collector policy for the agent. Unlike the evaluation policy (*π(at|ht)*) which is use to predict the target Q value, the collector will explore and collect data with actions and their resulting value for the memory, memories are saved as trajectories (*τ*) in tensorflow which is a collection of the current observed state (*ot*), the action taken (*at*), the reward received (*r_t+1*) and the following observed state (*o_t+1*) formalized as *r=(o_t-1, a_t-1, rt, ot, dt)*, where dt is a flag for the end state if this was the last observation.\n",
    "\n",
    "To give learning opportunity to our agent, we will use a high epsilon to have it explore a lot, and slowly decay it using the formula below:\n",
    "\n",
    "$$\n",
    "\\epsilon_{\\text{decayed}} = \\epsilon_{\\text{final}} + (\\epsilon_{\\text{initial}} - \\epsilon_{\\text{final}}) \\times e^{-\\frac{\\text{step}}{\\text{decay\\_steps}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- *ϵ_decayed* is the decayed epsilon value at the current step,\n",
    "- *ϵ_initial* is the initial epsilon value at the start of training, we set it to 1, meaning it only explores at start.\n",
    "- *ϵ_final* is the end value we want that the agent exploits is environment, preferably when deployed.\n",
    "- *step* is the current step or iteration in the training process, and decay_steps is a parameter that controls the rate, in our case 1000. As the steps approach the end, the decay will get smaller and smaller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100e21c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:40.527525Z",
     "iopub.status.busy": "2024-10-02T21:01:40.527141Z",
     "iopub.status.idle": "2024-10-02T21:01:40.572942Z",
     "shell.execute_reply": "2024-10-02T21:01:40.572172Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1710410599830,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "7DmbtWugH0Ct",
    "papermill": {
     "duration": 0.095401,
     "end_time": "2024-10-02T21:01:40.574981",
     "exception": false,
     "start_time": "2024-10-02T21:01:40.479580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TradingSimulator:\n",
    "    \"\"\"\n",
    "    A simulator class to train and evaluate a DDQN agent on a trading environment using TF-Agents and Reverb.\n",
    "\n",
    "    Args:\n",
    "        env: A Python environment for training the agent.\n",
    "        eval_env: A Python environment for evaluating the agent.\n",
    "        agent: The DDQN agent to be trained.\n",
    "        episodes (int): Total number of episodes for training.\n",
    "        batch_size (int): Size of the batches used in replay buffer.\n",
    "        collect_steps (int): Number of steps to collect in each training step.\n",
    "        log_interval (int): Frequency of logging during training.\n",
    "        eval_interval (int): Frequency of evaluation during training.\n",
    "        global_step (int): A global step tracker.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 eval_env,\n",
    "                 agent,\n",
    "                 episodes=TRAIN_EPISODES, batch_size=BATCH_SIZE,\n",
    "                 collect_steps=COLLECT_SIZE,\n",
    "                 log_interval=LOG_INTERVALS,\n",
    "                 eval_interval=TEST_INTERVALS,\n",
    "                 global_step=None):\n",
    "        assert env is not None and eval_env is not None and agent is not None\n",
    "\n",
    "        self.py_env = env\n",
    "        self.env = tf_py_environment.TFPyEnvironment(self.py_env)\n",
    "        self.py_eval_env = eval_env\n",
    "        self.eval_env = tf_py_environment.TFPyEnvironment(self.py_eval_env)\n",
    "        self.agent = agent\n",
    "        self.episodes = episodes\n",
    "        self.log_interval = log_interval\n",
    "        self.eval_interval = eval_interval\n",
    "        self.global_step = global_step if global_step is not None else tf.compat.v1.train.get_or_create_global_step()\n",
    "        self.batch_size = batch_size\n",
    "        self.collect_steps = collect_steps\n",
    "        self.replay_max = int(collect_steps * 1.5)\n",
    "        self.policy = self.agent.policy\n",
    "        self.collect_policy = self.agent.collect_policy\n",
    "\n",
    "        self.replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "            tensor_spec.from_spec(self.agent.collect_data_spec)\n",
    "        )\n",
    "\n",
    "    def init_memory(self, table_name='uniform_table'):\n",
    "        \"\"\"\n",
    "        Initialize the replay memory using Reverb, setting up a replay table and dataset.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the replay table for Reverb.\n",
    "\n",
    "        Returns:\n",
    "            A dataset and an iterator for sampling replay data.\n",
    "        \"\"\"\n",
    "        self.table = reverb.Table(\n",
    "            table_name,\n",
    "            max_size=self.replay_max,\n",
    "            sampler=reverb.selectors.Uniform(),\n",
    "            remover=reverb.selectors.Fifo(),\n",
    "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "            signature=self.replay_buffer_signature\n",
    "        )\n",
    "        self.reverb_server = reverb.Server([self.table])\n",
    "        self.replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "            self.agent.collect_data_spec,\n",
    "            table_name=table_name,\n",
    "            sequence_length=None,\n",
    "            local_server=self.reverb_server\n",
    "        )\n",
    "\n",
    "        self.rb_observer = reverb_utils.ReverbAddTrajectoryObserver(self.replay_buffer.py_client, table_name, sequence_length=2)\n",
    "        self.dataset = self.replay_buffer.as_dataset(\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            sample_batch_size=self.batch_size,\n",
    "            num_steps=2\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "        return self.dataset, iter(self.dataset)\n",
    "\n",
    "    def clear_model_directories(self, directories=[MODELS_PATH, LOGS_PATH]):\n",
    "        \"\"\"\n",
    "        Clear model directories by deleting all files and directories except zipped files.\n",
    "\n",
    "        Args:\n",
    "            directories (list): List of directories to be cleared.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for root, dirs, files in os.walk(directories, topdown=False):\n",
    "                for name in files:\n",
    "                    file_path = os.path.join(root, name)\n",
    "                    if not file_path.endswith('.zip'):\n",
    "                        os.remove(file_path)\n",
    "                for name in dirs:\n",
    "                    dir_path = os.path.join(root, name)\n",
    "                    shutil.rmtree(dir_path)\n",
    "            print(f\"Cleared all temporary files and directories in {directories}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing directories: {e}\")\n",
    "\n",
    "    def get_q_values(self, time_step):\n",
    "        \"\"\"\n",
    "        Get the Q-values and target Q-values from the DDQN networks at the given time step.\n",
    "\n",
    "        Args:\n",
    "            time_step: The time step for which Q-values need to be calculated.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of numpy arrays representing the online and target Q-values.\n",
    "        \"\"\"\n",
    "        batched_time_step = tf.nest.map_structure(lambda t: tf.expand_dims(t, 0), time_step)\n",
    "        q_values, _ = self.agent._q_network(batched_time_step.observation, batched_time_step.step_type)\n",
    "        best_action = tf.argmax(q_values, axis=-1)\n",
    "        target_q_values, _ = self.agent._target_q_network(batched_time_step.observation, batched_time_step.step_type)\n",
    "        target_q_values = tf.gather(target_q_values, best_action, axis=-1, batch_dims=1)\n",
    "        return q_values.numpy(), target_q_values.numpy()\n",
    "\n",
    "    def train(self,\n",
    "              checkpoint_path=MODELS_PATH,\n",
    "              initial_epsilon=EPSILON_START,\n",
    "              final_epsilon=EPSILON_END,\n",
    "              decay_steps=EPSILON_DECAY,\n",
    "              strategy=None,\n",
    "              only_purge_buffer=False):\n",
    "        \"\"\"\n",
    "        Train the DDQN agent with the specified hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_path (str): Path for saving checkpoints.\n",
    "            initial_epsilon (float): Initial epsilon value for epsilon-greedy policy.\n",
    "            final_epsilon (float): Final epsilon value for epsilon-greedy policy.\n",
    "            decay_steps (int): Steps for epsilon decay.\n",
    "            strategy: Distributed strategy for parallel execution.\n",
    "            only_purge_buffer (bool): Flag to only purge buffer and not train.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of rewards, losses, Q-values, and target Q-values collected during training.\n",
    "        \"\"\"\n",
    "        self.init_memory()\n",
    "        train_checkpointer = None\n",
    "        if checkpoint_path is not None:\n",
    "            checkpoint_dir = os.path.join(checkpoint_path, 'checkpoint')\n",
    "            train_checkpointer = common.Checkpointer(\n",
    "                ckpt_dir=checkpoint_dir,\n",
    "                max_to_keep=1,\n",
    "                agent=self.agent,\n",
    "                policy=self.agent.policy,\n",
    "                replay_buffer=self.replay_buffer,\n",
    "                global_step=self.global_step\n",
    "            )\n",
    "            train_checkpointer.initialize_or_restore()\n",
    "            print(f'Checkpoint restored: Step {self.global_step.numpy()}')\n",
    "            root_dir = os.path.join(checkpoint_path, 'learner')\n",
    "        else:\n",
    "            temp_dir = tempfile.TemporaryDirectory()\n",
    "            root_dir = temp_dir.name\n",
    "        agent_learner = learner.Learner(\n",
    "            root_dir=root_dir,\n",
    "            train_step=self.global_step,\n",
    "            agent=self.agent,\n",
    "            experience_dataset_fn=lambda: self.dataset,\n",
    "            checkpoint_interval=self.eval_interval if self.eval_interval is not None else 1000,\n",
    "            use_reverb_v2=False,\n",
    "            summary_interval=self.log_interval if self.log_interval is not None else 1000,\n",
    "            strategy=strategy,\n",
    "            summary_root_dir=LOGS_PATH if self.log_interval is not None else None\n",
    "        )\n",
    "        collect_driver = py_driver.PyDriver(\n",
    "            self.py_env,\n",
    "            py_tf_eager_policy.PyTFEagerPolicy(self.collect_policy, use_tf_function=True),\n",
    "            [self.rb_observer],\n",
    "            max_steps=self.collect_steps\n",
    "        )\n",
    "        losses, rewards, q_values_list, target_q_values_list = [], [], [], []\n",
    "        time_step = self.py_env.reset()\n",
    "        print(f\"Training from step: {self.global_step.numpy()} to step: {self.episodes}\")\n",
    "\n",
    "        while self.global_step.numpy() < self.episodes:\n",
    "            time_step = self.py_env.reset() if time_step.is_last() else time_step\n",
    "            time_step, _ = collect_driver.run(time_step)\n",
    "            agent_learner.run()\n",
    "            # Anneal the agents epsilon.\n",
    "            self.collect_policy._epsilon = (\n",
    "                final_epsilon + (initial_epsilon - final_epsilon) * tf.math.exp(-1.0 * tf.cast(self.global_step.numpy(), tf.float32) / decay_steps)\n",
    "            )\n",
    "\n",
    "            if self.log_interval and self.global_step.numpy() % self.log_interval == 0:\n",
    "                print(f'Step = {self.global_step.numpy()} of {self.episodes}: Loss = {agent_learner.loss().loss.numpy()}')\n",
    "                q_values, t_q_values = self.get_q_values(time_step)\n",
    "                q_values_list.append(q_values)\n",
    "                target_q_values_list.append(t_q_values)\n",
    "\n",
    "            if (self.eval_interval and\n",
    "                    (self.global_step.numpy() % self.eval_interval == 0\n",
    "                        or self.global_step.numpy() == self.episodes - 1)):\n",
    "                total_rewards, avg_rewards = self.eval_metrics(strategy)\n",
    "                rewards.append(np.mean(avg_rewards))\n",
    "                losses.append(agent_learner.run().loss.numpy())\n",
    "                print(f'Step = {self.global_step.numpy()} | Avg Reward = {np.mean(avg_rewards)} | Total = {total_rewards[-1]}')\n",
    "                if train_checkpointer is not None:\n",
    "                    train_checkpointer.save(self.global_step)\n",
    "        self.rb_observer.close()\n",
    "        self.reverb_server.stop()\n",
    "        self.global_step.assign(0)\n",
    "        if checkpoint_path is not None:\n",
    "            policy_dir = os.path.join(checkpoint_path, 'policy')\n",
    "            try:\n",
    "                policy_saver.PolicySaver(self.agent.policy).save(policy_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving policy, using checkpoint instead: {e}\")\n",
    "                train_checkpointer.save(self.global_step)\n",
    "            self.zip_directories(checkpoint_path)\n",
    "            print(\"Training complete and policy saved.\")\n",
    "            self.clear_model_directories(checkpoint_path)\n",
    "            print(\"Temp Training files cleared.\")\n",
    "        else:\n",
    "            temp_dir.cleanup()\n",
    "\n",
    "        return rewards, losses, q_values_list, target_q_values_list\n",
    "\n",
    "    def eval_metrics(self, strategy):\n",
    "        \"\"\"\n",
    "        Evaluate the trained agent using the evaluation environment.\n",
    "\n",
    "        Args:\n",
    "            strategy: Distributed strategy for parallel execution.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of total episode returns and average episode rewards.\n",
    "        \"\"\"\n",
    "        assert self.policy is not None, f\"No policy, you need to train first.\"\n",
    "        total_returns_list, episode_avg_rewards_list = [], []\n",
    "\n",
    "        with strategy.scope():\n",
    "            time_step = self.eval_env.reset()\n",
    "            episode_rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            i = 0\n",
    "            while not time_step.is_last():\n",
    "                action_step = self.policy.action(time_step)\n",
    "                time_step = self.eval_env.step(action_step.action)\n",
    "                episode_rewards = episode_rewards.write(i, time_step.reward)\n",
    "                i += 1\n",
    "\n",
    "            episode_rewards = episode_rewards.stack()\n",
    "            total_episode_return = tf.reduce_sum(episode_rewards)\n",
    "            episode_avg_return = tf.reduce_mean(episode_rewards)\n",
    "\n",
    "            total_returns_list.append(total_episode_return.numpy())\n",
    "            episode_avg_rewards_list.append(episode_avg_return.numpy())\n",
    "\n",
    "        return np.array(total_returns_list), np.array(episode_avg_rewards_list)\n",
    "\n",
    "    def zip_directories(self, directories=MODELS_PATH, output_filename=f'{MODELS_PATH}/model_files'):\n",
    "        \"\"\"\n",
    "        Archive the specified directories into a zip file.\n",
    "\n",
    "        Args:\n",
    "            directories (str): Directory to be archived.\n",
    "            output_filename (str): Name of the output archive file.\n",
    "        \"\"\"\n",
    "        archive_path = shutil.make_archive(output_filename, 'zip', root_dir='.', base_dir=directories)\n",
    "        print(f\"Archived {directories} into {archive_path}\")\n",
    "\n",
    "    def load_and_eval_policy(self, policy_path):\n",
    "        \"\"\"\n",
    "        Load and evaluate a saved policy from the specified path.\n",
    "\n",
    "        Args:\n",
    "            policy_path (str): Directory path of the saved policy.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of the loaded policy, total rewards, and average return.\n",
    "        \"\"\"\n",
    "        policy_dir = os.path.join(policy_path, 'policy')\n",
    "        try:\n",
    "            self.policy = tf.saved_model.load(policy_dir)\n",
    "        except Exception as e:\n",
    "            checkpoint_dir = os.path.join(policy_path, 'checkpoint')\n",
    "            train_checkpointer = common.Checkpointer(\n",
    "                ckpt_dir=checkpoint_dir,\n",
    "                agent=self.agent,\n",
    "                policy=self.agent.policy,\n",
    "                replay_buffer=self.replay_buffer,\n",
    "                global_step=self.global_step\n",
    "            )\n",
    "            status = train_checkpointer.initialize_or_restore()\n",
    "            print(f'Checkpoint restored: {status}')\n",
    "        total_rewards, avg_return = self.eval_metrics(strategy)\n",
    "        print(f'Average Return = {np.mean(avg_return)}, Total Return = {np.mean(total_rewards)}')\n",
    "\n",
    "        return self.policy, total_rewards, avg_return\n",
    "\n",
    "    def plot_performance(self, average_rewards, losses, q_values, target_q_values):\n",
    "        \"\"\"\n",
    "        Plot the performance metrics including rewards, losses, and Q-values over training iterations.\n",
    "\n",
    "        Args:\n",
    "            average_rewards (list): Average rewards collected during training.\n",
    "            losses (list): Loss values collected during training.\n",
    "            q_values (list): Q-values collected during training.\n",
    "            target_q_values (list): Target Q-values collected during training.\n",
    "        \"\"\"\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(24, 6))\n",
    "        episodes_index = np.arange(len(average_rewards)) * self.eval_interval\n",
    "        q_values = np.array([np.mean(q_val) if q_val.ndim > 1 else q_val for q_val in q_values]).flatten()\n",
    "        target_q_values = np.array([np.mean(tq_val) if tq_val.ndim > 1 else tq_val for tq_val in target_q_values]).flatten()\n",
    "\n",
    "        axs[0].set_xlabel('Episodes')\n",
    "        axs[0].set_ylabel('Rewards')\n",
    "        axs[0].plot(episodes_index, average_rewards, label='Average Rewards', color=\"yellow\")\n",
    "        axs[0].tick_params(axis='y')\n",
    "        axs[0].legend(loc=\"upper right\")\n",
    "        axs[0].set_title('Average Rewards over Iterations')\n",
    "        axs[1].set_xlabel('Episodes')\n",
    "        axs[1].set_ylabel('Loss')\n",
    "        axs[1].plot(episodes_index, losses, label='Loss', color=\"red\")\n",
    "        axs[1].tick_params(axis='y')\n",
    "        axs[1].legend(loc=\"upper right\")\n",
    "        axs[1].set_title('Loss over Iterations')\n",
    "\n",
    "        min_q_len = min(len(q_values), len(target_q_values))\n",
    "        q_episodes_index = np.arange(min_q_len) * self.log_interval\n",
    "        axs[2].set_xlabel('Episodes')\n",
    "        axs[2].set_ylabel('Q-Values')\n",
    "        axs[2].plot(q_episodes_index, q_values[:min_q_len], label='Online Q-Values', color=\"green\")\n",
    "        axs[2].plot(q_episodes_index, target_q_values[:min_q_len], label='Target Q-Values', color=\"blue\")\n",
    "        axs[2].tick_params(axis='y')\n",
    "        axs[2].legend(loc=\"upper right\")\n",
    "        axs[2].set_title('Networks Q-Values over Iterations')\n",
    "        fig.suptitle('DDQN Performance', fontsize=16)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_eval_trades(self, storage_dir=LOGS_PATH, file_name='backtest'):\n",
    "        \"\"\"\n",
    "        Plot the backtest results of the agent's trading performance.\n",
    "\n",
    "        Args:\n",
    "            storage_dir (str): Directory path to save the backtest plot.\n",
    "            file_name (str): Name of the file for the saved plot.\n",
    "\n",
    "        Displays:\n",
    "            The backtest plot, showing Buy/Sell signals, cumulative returns, and rewards.\n",
    "        \"\"\"\n",
    "        trades_df = self.py_eval_env.get_trade_data()\n",
    "        assert len(trades_df) > 1, \"No trades in evaluation environment. You need to call eval_metrics.\"\n",
    "\n",
    "        print(f\"Cumulative Ret from the strategy: {trades_df['cReturns'].iloc[-1]*100.:.02f}%\")\n",
    "        buy_signals = trades_df[trades_df['Action'] == ACT_LONG]\n",
    "        sell_signals = trades_df[trades_df['Action'] == ACT_SHORT]\n",
    "\n",
    "        _, axes = plt.subplots(3, 1, figsize=(18, 11), gridspec_kw={'height_ratios': [4, 2, 2]})\n",
    "\n",
    "        axes[0].plot(trades_df['Close'], label=f'Close', color='blue', alpha=0.6, linestyle='--')\n",
    "        axes[0].scatter(buy_signals.index, buy_signals['Close'], color='green', marker='^', label='Buy')\n",
    "        axes[0].scatter(sell_signals.index, sell_signals['Close'], color='red', marker='v', label='Sell')\n",
    "        axes[0].set_title(f'Close')\n",
    "        axes[0].set_ylabel('Price')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        axes[1].plot(trades_df['cReturns'], label='Cumulative rets', color='purple')\n",
    "        axes[1].set_title('Cumulative rets')\n",
    "        axes[1].set_ylabel('Cumulative rets')\n",
    "        axes[1].grid(True)\n",
    "        axes[1].legend()\n",
    "        axes[2].plot(trades_df['Reward'], label='Rewards', color='green')\n",
    "        axes[2].set_title('Rewards or Penalties')\n",
    "        axes[2].set_ylabel('Rewards or Penalties')\n",
    "        axes[2].grid(True)\n",
    "        axes[2].legend()\n",
    "        plt.tight_layout()\n",
    "        try:\n",
    "            if not os.path.exists(storage_dir):\n",
    "                os.makedirs(storage_dir)\n",
    "            file_path = os.path.join(storage_dir, f'{file_name}.png')\n",
    "            plt.savefig(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't save plot {e}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b0211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:01:40.665086Z",
     "iopub.status.busy": "2024-10-02T21:01:40.664478Z",
     "iopub.status.idle": "2024-10-02T21:18:17.176205Z",
     "shell.execute_reply": "2024-10-02T21:18:17.175156Z"
    },
    "papermill": {
     "duration": 996.559491,
     "end_time": "2024-10-02T21:18:17.179004",
     "exception": false,
     "start_time": "2024-10-02T21:01:40.619513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim = TradingSimulator(\n",
    "    env=train_env,\n",
    "    eval_env=test_env,\n",
    "    agent=agent,\n",
    "    collect_steps=len(train_data)\n",
    ")\n",
    "rewards, losses, q_values, target_q_values = sim.train(strategy=strategy)\n",
    "\n",
    "sim.plot_performance(rewards, losses, q_values, target_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14660ad8",
   "metadata": {},
   "source": [
    "A not on the paper's code, the researchers use a 'trick' to aid the networks to converge faster - this is they return both the action and the states, and, the inverse of the action and the states. They can do this because the agent we both are building really cares about long and short positions. \n",
    "\n",
    "Given we are using the TF-Agents framework to allow us to focus more on the network, algo, and features - we miss this opportunity as the **Learner** class abstracts the access to the networks, therefore only the returned state (and not inverse states) are use to train the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be007fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:18:17.277459Z",
     "iopub.status.busy": "2024-10-02T21:18:17.276797Z",
     "iopub.status.idle": "2024-10-02T21:18:29.700481Z",
     "shell.execute_reply": "2024-10-02T21:18:29.699513Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1710411543205,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "SM6Lz_mSylZw",
    "papermill": {
     "duration": 12.475507,
     "end_time": "2024-10-02T21:18:29.702740",
     "exception": false,
     "start_time": "2024-10-02T21:18:17.227233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy, total_rewards, avg_return = sim.load_and_eval_policy(policy_path=MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262bfc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:18:29.799941Z",
     "iopub.status.busy": "2024-10-02T21:18:29.799574Z",
     "iopub.status.idle": "2024-10-02T21:18:29.900421Z",
     "shell.execute_reply": "2024-10-02T21:18:29.899406Z"
    },
    "papermill": {
     "duration": 0.151497,
     "end_time": "2024-10-02T21:18:29.902384",
     "exception": false,
     "start_time": "2024-10-02T21:18:29.750887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trade_metrics(df, risk_free_rate=RISK_FREE_RATE, market_index=None):\n",
    "    \"\"\"\n",
    "    Calculate various performance metrics for the trading strategy based on the given trade data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing trading data with at least 'Returns' and 'Position' columns.\n",
    "        risk_free_rate (float): Risk-free rate used for Sharpe and Sortino ratios.\n",
    "        market_index (pd.DataFrame, optional): Market index DataFrame to calculate Beta, with a 'Close' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with calculated metrics including:\n",
    "            - Cumulative Returns\n",
    "            - Annualized Returns\n",
    "            - Maximum Return\n",
    "            - Maximum Loss\n",
    "            - Variance\n",
    "            - Standard Deviation\n",
    "            - Maximum Drawdown\n",
    "            - Drawdown Length\n",
    "            - Sharpe Ratio\n",
    "            - Sortino Ratio\n",
    "            - Number of Trades\n",
    "            - Trades per Interval\n",
    "            - Number of Intervals\n",
    "            - Returns Skewness\n",
    "            - Returns Kurtosis\n",
    "            - Beta\n",
    "            - Information Ratio\n",
    "            - Trade Churn\n",
    "            - Profitability Ratio [%]\n",
    "    \"\"\"\n",
    "    def calc_annualized_sharpe(rets, risk_free_rate=RISK_FREE_RATE):\n",
    "        mean_rets = rets.mean()\n",
    "        std_rets = rets.std()\n",
    "        sharpe_ratio = 0.\n",
    "        if std_rets != 0:\n",
    "            sharpe_ratio = (mean_rets - (risk_free_rate / TRADING_DAYS_YEAR)) / std_rets\n",
    "            sharpe_ratio *= np.sqrt(TRADING_DAYS_YEAR)\n",
    "        return sharpe_ratio\n",
    "\n",
    "    def calc_annualized_sortino(returns, risk_free_rate):\n",
    "        downside_risk = np.sqrt(((returns[returns < 0])**2).mean()) * np.sqrt(TRADING_DAYS_YEAR)\n",
    "        return (returns.mean() * TRADING_DAYS_YEAR - risk_free_rate) / downside_risk\n",
    "\n",
    "    variance = df['Returns'].var()\n",
    "    sharpe = calc_annualized_sharpe(df['Returns'],  risk_free_rate=risk_free_rate)\n",
    "    sortino = calc_annualized_sortino(df['Returns'],  risk_free_rate=risk_free_rate)\n",
    "\n",
    "    df['Drawdown'] = (1 + df['Returns']).cumprod().div((1 + df['Returns']).cumprod().cummax()) - 1\n",
    "    max_drawdown = df['Drawdown'].min()\n",
    "    drawdown_length = (df['Drawdown'] < 0).astype(int).groupby(df['Drawdown'].eq(0).cumsum()).cumsum().max()\n",
    "\n",
    "    trades = (df['Position'].diff().ne(0) & df['Position'].ne(0)).sum()\n",
    "\n",
    "    beta = None\n",
    "    if market_index is not None:\n",
    "        market_index['Returns'] = pd.to_numeric(market_index['Close'].pct_change().fillna(0), errors='coerce').fillna(0)\n",
    "        y = pd.to_numeric(df['Returns'], errors='coerce').fillna(0)\n",
    "        X = add_constant(market_index['Returns'].reset_index(drop=True))\n",
    "        y = y.iloc[:len(X)].reset_index(drop=True)\n",
    "        X = X.iloc[:len(y)].reset_index(drop=True)\n",
    "        model = OLS(y, X).fit()\n",
    "        beta = model.params[1]\n",
    "\n",
    "    active_return = df['Returns'] - (risk_free_rate / TRADING_DAYS_YEAR)\n",
    "    tracking_error = active_return.std()\n",
    "    information_ratio = (active_return.mean() / tracking_error) * np.sqrt(TRADING_DAYS_YEAR)\n",
    "    trade_churn = trades / len(df)\n",
    "    cumulative_return = (np.cumprod(1 + df['Returns']) - 1).iloc[-1] if not df['Returns'].empty else 0\n",
    "    annualized_return = (1 + cumulative_return)**(TRADING_DAYS_YEAR / len(df)) - 1 if len(df) > 0 else 0\n",
    "    winning_trades = df[df['Returns'] > 0]['Returns']\n",
    "    profitability_ratio = (winning_trades.sum() / len(df)) * 100\n",
    "\n",
    "    stats_df = pd.DataFrame({\n",
    "        \"Cumulative Returns\": [cumulative_return],\n",
    "        \"Annualized Returns\": [annualized_return],\n",
    "        \"Maximum Return\": [df['Returns'].max()],\n",
    "        \"Maximum Loss\": [df['Returns'].min()],\n",
    "        \"Variance\": [variance],\n",
    "        \"Standard Deviation\": [np.sqrt(variance)],\n",
    "        \"Maximum Drawdown\": [max_drawdown],\n",
    "        \"Drawdown Length\": [drawdown_length],\n",
    "        \"Sharpe Ratio\": [sharpe],\n",
    "        \"Sortino Ratio\": [sortino],\n",
    "        \"Number of Trades\": [trades],\n",
    "        \"Trades per Interval\": [trades / len(df)],\n",
    "        \"Number of Intervals\": [len(df)],\n",
    "        \"Returns\": [df['Returns'].to_numpy()],\n",
    "        \"Returns Skewness\": [skew(df['Returns'].to_numpy())],\n",
    "        \"Returns Kurtosis\": [kurtosis(df['Returns'].to_numpy())],\n",
    "        \"Beta\": [beta],\n",
    "        \"Information Ratio\": [information_ratio],\n",
    "        \"Trade Churn\": [trade_churn],\n",
    "        \"Profitability Ratio [%]\": [profitability_ratio],\n",
    "    })\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "metrics = get_trade_metrics(test_env.get_trade_data(), market_index=tickers[MARKET])\n",
    "metrics.drop(columns=[\"Returns\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e18d80",
   "metadata": {
    "papermill": {
     "duration": 0.047803,
     "end_time": "2024-10-02T21:18:29.998643",
     "exception": false,
     "start_time": "2024-10-02T21:18:29.950840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Our Baseline Metrics\n",
    "\n",
    "Compared to the paper, our modernized DDQN agent using TF-Agents achieved 7 times the sharpe.\n",
    "\n",
    "| Metric                     | Article Baseline       | B&H               | S&H              | TF                | MR               | TDQN Benchmark   |  \n",
    "|----------------------------|------------------------|-------------------|------------------|-------------------|------------------|------------------|  \n",
    "| **Annualized Return**      | 87.25%                 | 24.11%            | -7.38%           | -100.00%          | 19.02%           | 12.80%           |  \n",
    "| **Annualized Vol**         | 3.28%                  | 53.14%            | 46.11%           | 52.70%            | 58.05%           | 52.09%           |  \n",
    "| **Sharpe Ratio**           | 1.421                  | 0.508             | -0.154           | -0.987            | 0.358            | 0.261            |  \n",
    "| **Sortino Ratio**          | 1.594                  | 0.741             | -0.205           | -1.229            | 0.539            | 0.359            |  \n",
    "| **Max Drawdown**           | -25.25%                | 52.83%            | 54.09%           | 79.91%            | 65.31%           | 58.95%           |  \n",
    "| **Max Drawdown Days**      | 141                    | 205               | 144              | 229               | 159              | 331              |  \n",
    "| **Cumulative Returns**     | 2.49                   | -                 | -                | -                 | -                | -                |  \n",
    "| **Maximum Return**         | 17.67%                 | -                 | -                | -                 | -                | -                |  \n",
    "| **Maximum Loss**           | -13.61%                | -                 | -                | -                 | -                | -                |  \n",
    "| **Variance**               | 0.1076%                | -                 | -                | -                 | -                | -                |  \n",
    "| **Standard Deviation**     | 3.28%                  | -                 | -                | -                 | -                | -                |  \n",
    "| **Drawdown Length**        | 141                    | -                 | -                | -                 | -                | -                |  \n",
    "| **Number of Trades**       | 195                    | -                 | -                | -                 | -                | -                |  \n",
    "| **Trades per Interval**    | 0.388                  | -                 | -                | -                 | -                | -                |  \n",
    "| **Number of Intervals**    | 502                    | -                 | -                | -                 | -                | -                |  \n",
    "| **Returns Skewness**       | 0.759                  | -                 | -                | -                 | -                | -                |  \n",
    "| **Returns Kurtosis**       | 4.76                   | -                 | -                | -                 | -                | -                |  \n",
    "| **Beta**                   | -0.387                 | -                 | -                | -                 | -                | -                |  \n",
    "| **Information Ratio**      | 1.421                  | -                 | -                | -                 | -                | -                |  \n",
    "| **Trade Churn**            | 0.388                  | -                 | -                | -                 | -                | -                |  \n",
    "| **Profitability Ratio [%]**| 1.26%                  | -                 | -                | -                 | -                | -                |  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5359f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T21:18:30.096737Z",
     "iopub.status.busy": "2024-10-02T21:18:30.096358Z",
     "iopub.status.idle": "2024-10-02T21:18:31.905879Z",
     "shell.execute_reply": "2024-10-02T21:18:31.904904Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1710411543206,
     "user": {
      "displayName": "Adam Darmanin",
      "userId": "00262451996831505471"
     },
     "user_tz": -60
    },
    "id": "fYL1ny9Etuu8",
    "papermill": {
     "duration": 1.863661,
     "end_time": "2024-10-02T21:18:31.910008",
     "exception": false,
     "start_time": "2024-10-02T21:18:30.046347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim.plot_eval_trades()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2d6e1",
   "metadata": {},
   "source": [
    "## Manual Policy Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cb89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = test_env.get_trade_data()\n",
    "trade_data.reset_index(inplace=True)\n",
    "print(trade_data[trade_data['Action'].isin([ACT_LONG, ACT_SHORT])]['Action'].tail(5))\n",
    "\n",
    "last_trade_step = trade_data[trade_data['Action'].isin([ACT_LONG, ACT_SHORT])].iloc[-1].name\n",
    "start_idx = max(0, last_trade_step - test_env.state_length + 1)\n",
    "end_idx = last_trade_step + 1\n",
    "\n",
    "last_trade_state = trade_data.iloc[start_idx:end_idx][test_env.features].values.flatten().astype(np.float32)\n",
    "batched_observation = np.expand_dims(last_trade_state, axis=0)\n",
    "q_values, _ = agent._q_network(batched_observation)\n",
    "print(q_values)\n",
    "\n",
    "predicted_action = np.argmax(q_values, axis=1)[0]\n",
    "print(f\"Predicted Action: {predicted_action} => Returns: {trade_data.loc[last_trade_step, 'Returns']} and Rewards: {trade_data.loc[last_trade_step, 'Reward']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6033b",
   "metadata": {},
   "source": [
    "# Validation Runs\n",
    "\n",
    "Given an RL algorithm is inherintly stochastic, we want to get an approximation with variance of its performance across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a696c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_agent(trading_simulator, test_env, strategy=None, num_runs=VALIDATION_ITERS, market_index=None):\n",
    "    \"\"\"\n",
    "    Validate the RL algorithm by running training and evaluation multiple times, and calculating trade metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - trading_simulator: Instance of the TradingSimulator class.\n",
    "    - test_env: The environment used for evaluation after training.\n",
    "    - strategy: TensorFlow strategy for distributed training, if applicable.\n",
    "    - num_runs: Number of times to run the DDQN training and evaluation.\n",
    "    - market_index: Market index data (e.g., S&P 500) to calculate beta and other metrics.\n",
    "\n",
    "    Returns:\n",
    "    - metrics_df: A DataFrame containing the mean and standard deviation of aggregated metrics and trading statistics.\n",
    "    \"\"\"\n",
    "    all_eval_rewards, all_eval_returns, all_trade_metrics = [], [], []\n",
    "\n",
    "    for run in tqdm(range(num_runs), desc=\"Validating Algo...\"):\n",
    "        trading_simulator.train(checkpoint_path=None,\n",
    "                                strategy=strategy)\n",
    "        total_returns, avg_rewards = trading_simulator.eval_metrics(strategy)\n",
    "        all_eval_rewards.append(np.mean(avg_rewards))\n",
    "        all_eval_returns.append(np.sum(total_returns))\n",
    "        trade_data = test_env.get_trade_data()\n",
    "        run_trade_metrics = get_trade_metrics(trade_data, market_index=market_index)\n",
    "        all_trade_metrics.append(run_trade_metrics.drop(columns=[\"Returns\"]).T)\n",
    "    core_metrics_summary = pd.DataFrame({\n",
    "        'Metric': ['Eval Rewards', 'Eval Total Return'],\n",
    "        'Mean': [np.mean(all_eval_rewards), np.mean(all_eval_returns)],\n",
    "        '-/+ Std': [np.std(all_eval_rewards), np.std(all_eval_returns)]\n",
    "    })\n",
    "\n",
    "    # Aggregate trading metrics across all runs\n",
    "    trade_metrics_mean = pd.concat(all_trade_metrics, axis=1).mean(axis=1).to_frame('Mean')\n",
    "    trade_metrics_std = pd.concat(all_trade_metrics, axis=1).std(axis=1).to_frame('-/+ Std')\n",
    "    trade_metrics_summary = trade_metrics_mean.join(trade_metrics_std).reset_index().rename(columns={'index': 'Metric'})\n",
    "\n",
    "    # Combine core metrics and trading metrics into final DataFrame\n",
    "    combined_metrics = pd.concat([core_metrics_summary, trade_metrics_summary], ignore_index=True)\n",
    "\n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trading_simulator = TradingSimulator(train_env,\n",
    "                                     test_env,\n",
    "                                     agent,\n",
    "                                     collect_steps=len(train_data),\n",
    "                                     log_interval=None,  # Silence these for validation.\n",
    "                                     eval_interval=None, # We validate after the full training.\n",
    "                                )\n",
    "metrics_df = validate_agent(val_trading_simulator,\n",
    "                            test_env,\n",
    "                            num_runs=VALIDATION_ITERS,\n",
    "                            strategy=strategy,\n",
    "                            market_index=tickers[MARKET])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba210142",
   "metadata": {
    "papermill": {
     "duration": 0.052738,
     "end_time": "2024-10-02T21:18:32.016123",
     "exception": false,
     "start_time": "2024-10-02T21:18:31.963385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be190b3",
   "metadata": {
    "papermill": {
     "duration": 0.052883,
     "end_time": "2024-10-02T21:18:32.122387",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.069504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Experiment 1 - Sharpe Ratio as the Reward\n",
    "\n",
    "Since we want to reduce the portolio's volatility while attaining higher returns, we should test having a rolling window Sharpe ration as the reward function with the assumption that our agent can minimize its portfolio's volatility. In our case, we need to square root the number of past trading sessions (max 252, the trading days in a year) to give an annualized Sharpe as a reward, The results are promising, better than the baseline:\n",
    "\n",
    "```json\n",
    "{'Annualized Return': 0.2483099753903375,\n",
    " 'Annualized Vol': 0.5315927401982844,\n",
    " 'Sharpe Ratio': 0.3753156743014152,\n",
    " 'Downside Deviation': 0.3649215721069904,\n",
    " 'Sortino Ratio': 0.0021695799842264578,\n",
    " 'Max Drawdown': -0.5284848329394528,\n",
    " 'Max Drawdown Days': 493,\n",
    " 'Trade Churn': 0.0,\n",
    " 'Skewness': 0.547255666186771,\n",
    " 'Kurtosis': 5.424081523143858}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d86a3",
   "metadata": {
    "papermill": {
     "duration": 0.054355,
     "end_time": "2024-10-02T21:18:32.230193",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.175838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Experiment 2: Technical Analysis (TA) Signals\n",
    "\n",
    "Using the Pandas-TA library, we augment our timeseries with the following signals:\n",
    "\n",
    "- Moving Average Convergence Divergence (MACD) is useful to confirm the presence of a trend. In addition, it can spot divergences from price, which can signal potential reversals. The MACD is created with a 12 day fast moving average (MA), a 26 day slow moving average (MA), and the signal which is a 9 day exponential moving average (EMA) of their differences.\n",
    "\n",
    "- Average true range (ATR) will signal the agent price swings and their magnitude, this would hint at the environment's volatility. It's built by decomposing a 14 day moving average of price extremes.\n",
    "\n",
    "Results:\n",
    "\n",
    "```json\n",
    "{'Annualized Return': 0.08037565358057806,\n",
    " 'Annualized Vol': 0.5327752235074609,\n",
    " 'Sharpe Ratio': 0.05927596580709699,\n",
    " 'Downside Deviation': 0.36637039877343286,\n",
    " 'Sortino Ratio': 0.00034205956635066734,\n",
    " 'Max Drawdown': 0.0,\n",
    " 'Max Drawdown Days': 672,\n",
    " 'Trade Churn': 0.0,\n",
    " 'Skewness': 0.5699760228074306,\n",
    " 'Kurtosis': 5.441197183719924}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac43f8",
   "metadata": {
    "papermill": {
     "duration": 0.052832,
     "end_time": "2024-10-02T21:18:32.336616",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.283784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Experiment 3: Macro Signals\n",
    "\n",
    "\n",
    "In this experiment, we will give the agent insight on its macro environment through the following timeseries:\n",
    "\n",
    "- VIX - the volatility and fear index for the current period.\n",
    "- 10 Year Treasury Note Yield - as a proxy to inflation.\n",
    "- S&P 500 - For the market risk factor.\n",
    "\n",
    "Gives these results:\n",
    "```json\n",
    "{'Annualized Return': 0.7375854975646395,\n",
    " 'Annualized Vol': 0.48576500545216406,\n",
    " 'Sharpe Ratio': 1.417950247927827,\n",
    " 'Downside Deviation': 0.30906051769218884,\n",
    " 'Sortino Ratio': 0.008843886276718567,\n",
    " 'Max Drawdown': -0.38977234510237335,\n",
    " 'Max Drawdown Days': 142,\n",
    " 'Trade Churn': 0.0,\n",
    " 'Skewness': 0.7135103541646352,\n",
    " 'Kurtosis': 4.722124713372126}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42952305",
   "metadata": {
    "papermill": {
     "duration": 0.052689,
     "end_time": "2024-10-02T21:18:32.442074",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.389385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Side-by-Side Experiment Results\n",
    "\n",
    "| Metric                 | Article Baseline       | Exp 1: Sharpe        | Exp 2: TA            | Exp 3: Macro         | B&H               | S&H              | TF                | MR               | TQDN Benchmark   | \n",
    "|------------------------|------------------------|----------------------|----------------------|----------------------|-------------------|------------------|-------------------|------------------|------------------|\n",
    "| Annualized Return      | 14.08%                 | 24.83%               | 8.04%                | 73.76%               | 24.11%            | -7.38%           | -100.00%          | 19.02%           | 12.80%           | \n",
    "| Annualized Vol         | 49.57%                 | 53.16%               | 53.28%               | 48.58%               | 53.14%            | 46.11%           | 52.70%            | 58.05%           | 52.09%           | \n",
    "| Sharpe Ratio           | 0.186                  | 0.375                | 0.059                | 1.418                | 0.508             | -0.154           | -0.987            | 0.358            | 0.261            | \n",
    "| Sortino Ratio          | 0.001                  | 0.0022               | 0.0003               | 0.0088               | 0.741             | -0.205           | -1.229            | 0.539            | 0.359            | \n",
    "| Max Drawdown           | -46.39%                | -52.85%              | 0.00%                | -38.98%              | 52.83%            | 54.09%           | 79.91%            | 65.31%           | 58.95%           | \n",
    "| Max Drawdown Days      | 394                    | 493                  | 672                  | 142                  | 205               | 144              | 229               | 159              | 331              | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa89a48",
   "metadata": {
    "id": "pTeb9dL6H0Cu",
    "papermill": {
     "duration": 0.106104,
     "end_time": "2024-10-02T21:18:32.601316",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.495212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this article we have adapted Deep Q-Network (TDQN) algorithm from *Théate, Thibaut and Ernst, Damien (2021)*, using our signals and Tensorflow's Agent framework. Our agent can now determine optimal trading positions (buy, sell, or hold) to maximize our portfolio returns in a simulated environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff30449",
   "metadata": {
    "id": "bBrMDUgZH0Cv",
    "papermill": {
     "duration": 0.053092,
     "end_time": "2024-10-02T21:18:32.707739",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.654647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- [A Random Walk Down Wall Street](https://www.amazon.co.uk/Random-Walk-Down-Wall-Street/dp/0393330338)\n",
    "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
    "- [Open Gym AI Github](https://github.com/openai/gym)\n",
    "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
    "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)\n",
    "- [Remote development in WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)\n",
    "- [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\n",
    "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "- [TensorRT for CUDA](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)\n",
    "- [Momentum and Reversion Trading Signals Analysis](https://medium.com/call-for-atlas/momentum-and-reversion-the-poor-mans-trading-models-9b8e1e6d3496)\n",
    "- [Temporal Convolutional Neural Network with Conditioning for Broad Market Signals](https://medium.com/call-for-atlas/temporal-convolutional-neural-network-with-conditioning-for-broad-market-signals-9f0b0426b2b9)\n",
    "- [Pandas TA - A Technical Analysis Library in Python 3](https://github.com/twopirllc/pandas-ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b15611",
   "metadata": {
    "id": "kIgjl92lH0Cv",
    "papermill": {
     "duration": 0.052026,
     "end_time": "2024-10-02T21:18:32.813263",
     "exception": false,
     "start_time": "2024-10-02T21:18:32.761237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Github\n",
    "\n",
    "Article and code available on [Github](https://github.com/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading)\n",
    "\n",
    "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/deep-q-rl-with-algorithmic-trading-policy)\n",
    "\n",
    "Google Collab available [here](https://colab.research.google.com/github/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/blob/main/drl_trading.ipynb)\n",
    "\n",
    "## Media\n",
    "\n",
    "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
    "\n",
    "## CC Licensing and Use\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "drl_ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1333.695052,
   "end_time": "2024-10-02T21:18:36.279110",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-02T20:56:22.584058",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
